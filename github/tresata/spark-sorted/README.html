<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <p><a href="https://travis-ci.org/tresata/spark-sorted" target="_blank"><img src="https://camo.githubusercontent.com/d97bddc491cdbf21b864461c4bff98d5c4f4e965/68747470733a2f2f7472617669732d63692e6f72672f747265736174612f737061726b2d736f727465642e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/tresata/spark-sorted.svg?branch=master" style="max-width:100%;"></a></p> 
  <h1><a href="https://github.com/tresata/spark-sorted#spark-sorted" aria-hidden="true" class="anchor" id="user-content-spark-sorted" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>spark-sorted</h1> 
  <p>Spark-sorted is a library that aims to make non-reduce type operations on very large groups in spark possible, including support for processing ordered values. To do so it relies on Spark's new sort-based shuffle and on never materializing the group for a given key but instead representing it by consecutive rows within a partition that get processed with a map-like (iterator based streaming) operation.</p> 
  <h2><a href="https://github.com/tresata/spark-sorted#groupsorted" aria-hidden="true" class="anchor" id="user-content-groupsorted" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>GroupSorted</h2> 
  <p>GroupSorted is a partitioned key-value RDDs that also satisfy the following criteria:</p> 
  <ul> 
   <li>all rows (key, value pairs) for a given key are consecutive and in the same partition</li> 
   <li>the values can optionally be ordered per key</li> 
  </ul> 
  <p>GroupSorted can be created from a RDD[(K, V)] using the rdd.groupSort operator. To enable the groupSort operator add the following import:</p> 
  <pre><code>import com.tresata.spark.sorted.PairRDDFunctions._
</code></pre> 
  <p>GroupSorted adds methods to a key-value RDD to process all values records for a given key: mapStreamByKey, foldLeftByKey, reduceLeftByKey and scanLeftByKey.</p> 
  <p>For example say you have a data-set of stock prices, represented as follows:</p> 
  <pre><code>type Ticker = String
case class Quote(time: Int, price: Double)
val prices: RDD[(Ticker, Quote)] = ...
</code></pre> 
  <p>Assuming you have a function calculates exponential moving averages (EMAs), you could produce time series of EMAs for all tickers as follows:</p> 
  <pre><code>val emas: Iterator[Double] =&gt; Iterator[Double] = ...
prices.groupSort(Ordering.by[Quote, Int](_.time)).mapStreamByKey{ iter =&gt; emas(iter.map(_.price)) }
</code></pre> 
  <p>A Java Api is available in package com.tresata.spark.sorted.api.java. Please see the unit tests for usage examples.</p> 
  <p>Have fun! Team @ Tresata</p> 
  <h2><a href="https://github.com/tresata/spark-sorted#update-apr-2016" aria-hidden="true" class="anchor" id="user-content-update-apr-2016" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Update Apr 2016</h2> 
  <p>Starting with release 0.7.0 GroupSorted operations will return another GroupSorted where possible, to retain the information on RDD layout (partitioning, ordering of keys, and ordering of values), so that operations can be chained efficiently.</p> 
  <p>Also, we are introducing the mergeJoin operation on GroupSorted as an experimental feature. A mergeJoin takes advantage of the sorting of keys within partitions to join using a sort-merge join. This can be much faster than the default join for key-value RDDs. It also allows one side to stream through the join (without any buffering), while the other side is buffered in memory, making it more scalable for certain applications. Please note this limitation: since values for one side are fully buffered in memory per key, mergeJoin is probably unsuitable for many-to-many joins.</p> 
  <h2><a href="https://github.com/tresata/spark-sorted#update-nov-2016" aria-hidden="true" class="anchor" id="user-content-update-nov-2016" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Update Nov 2016</h2> 
  <p>Starting with release 1.0.0 Scala 2.10 is no longer supported.</p> 
  <p>Release 1.0.0 will also add initial support for the groupSort operation on Spark SQL Dataset. For Dataset the design of groupSort is kept very simple: it only works for a key-value Dataset (<code>Dataset[(K, V)]</code>) with the grouping by key and sorting by value per key. To sort by anything other than the natural ordering of the values simply do a transformation on the values (see unit tests for examples) to get back to a natural ordering. Unfortunately for Dataset it is not (yet) possible to push the secondary sort into the shuffle but the good news is that the sort is implemented efficiently using the Dataset.sortWithinPartitions operation.</p> 
  <h2><a href="https://github.com/tresata/spark-sorted#update-jan-2017" aria-hidden="true" class="anchor" id="user-content-update-jan-2017" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Update Jan 2017</h2> 
  <p>Starting with release 1.2.0 Java 7 is no longer supported.</p> 
 </article>
</div>