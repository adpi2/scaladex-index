<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <p><a href="https://travis-ci.org/getryft/spark-ryft-connector" target="_blank"><img src="https://camo.githubusercontent.com/ab06469165df4bce4efc8f547fdc405bfc258f13/68747470733a2f2f7472617669732d63692e6f72672f676574727966742f737061726b2d727966742d636f6e6e6563746f722e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://travis-ci.org/getryft/spark-ryft-connector.svg?style=flat-square" style="max-width:100%;"></a> <a href="https://maven-badges.herokuapp.com/maven-central/com.ryft/spark-ryft-connector_2.10" target="_blank"><img src="https://camo.githubusercontent.com/ab8d752c64de1cce032444663bcbf3dc952fbc5e/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f636f6d2e727966742f737061726b2d727966742d636f6e6e6563746f725f322e31302f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/com.ryft/spark-ryft-connector_2.10/badge.svg" style="max-width:100%;"></a> <a href="https://maven-badges.herokuapp.com/maven-central/com.ryft/spark-ryft-connector_2.11" target="_blank"><img src="https://camo.githubusercontent.com/188bd383decb24d0ea36aeb3ca4425c113809bfd/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f636f6d2e727966742f737061726b2d727966742d636f6e6e6563746f725f322e31312f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/com.ryft/spark-ryft-connector_2.11/badge.svg" style="max-width:100%;"></a></p> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#spark-ryft-connector" aria-hidden="true" class="anchor" id="user-content-spark-ryft-connector" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark Ryft Connector</h1> 
  <p>This library lets you expose Ryft as Spark RDDs or Data Frames by consuming Ryft REST api.</p> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#installation" aria-hidden="true" class="anchor" id="user-content-installation" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Installation</h2> 
  <div class="highlight highlight-source-shell">
   <pre>sbt clean compile</pre>
  </div> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#build-executable-jar" aria-hidden="true" class="anchor" id="user-content-build-executable-jar" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Build executable jar</h2> 
  <div class="highlight highlight-source-shell">
   <pre>sbt clean assembly</pre>
  </div> 
  <p>You can find jar at: <code>../spark-ryft-connector/target/scala-2.10/</code></p> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#ryft-query-mechanism" aria-hidden="true" class="anchor" id="user-content-ryft-query-mechanism" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Ryft Query mechanism</h2> 
  <p>There are two main types of RyftQuery:</p> 
  <ol> 
   <li>SimpleQuery</li> 
   <li>RecordQuery</li> 
  </ol> 
  <p><code>SimpleQuery</code> represents RAW_TEXT search for one or more search queries. This type of query only uses the CONTAINS relational operator. For two or more search queries, use the OR logical operator. For example, to conduct a free text search for 3 words ‘query0’, ‘query1’, ‘query2’ use this statement:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-en">SimpleQuery</span>(<span class="pl-en">List</span>(“query0”,”query1”,”query2”))</pre>
  </div> 
  <p>It's transformed into this command:</p> 
  <pre><code>((RAW_TEXT CONTAINS “query0”)OR(RAW_TEXT CONTAINS ”query1”)OR(RAW_TEXT CONTAINS “query2"))
</code></pre> 
  <p><code>RecordQuery</code> provides more complexity. It allows you to use Ryft RECORD and RECORD.field queries to search on an entire record set or specific fields within record sets. You can use method chaining mechanism to create nested queries.</p> 
  <p>Here are two examples:</p> 
  <ul> 
   <li>search for all records where field desc contains <code>VEHICLE</code></li> 
  </ul> 
  <pre><code>RecordQuery(recordField("desc") contains "VEHICLE") -&gt; (RECORD.desc CONTAINS “VEHICLE”) 
</code></pre> 
  <ul> 
   <li>search for all records which contains ‘VEHICLE’ in any field</li> 
  </ul> 
  <pre><code>RecordQuery(record contains "VEHICLE") -&gt; (RECORD CONTAINS “VEHICLE”) 
</code></pre> 
  <p>RecordQuery allows you to combine two or more queries by chaining the commands. If the original query is this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-en">RecordQuery</span>(recordField(<span class="pl-s"><span class="pl-pds">"</span>desc<span class="pl-pds">"</span></span>) contains <span class="pl-s"><span class="pl-pds">"</span>VEHICLE<span class="pl-pds">"</span></span>)
      .or(recordField(<span class="pl-s"><span class="pl-pds">"</span>desc<span class="pl-pds">"</span></span>) contains <span class="pl-s"><span class="pl-pds">"</span>BIKE<span class="pl-pds">"</span></span>)  </pre>
  </div> 
  <p>Then RecordQuery will convert it to produce this query:</p> 
  <pre><code>((RECORD.desc CONTAINS “VEHICLE”)OR(RECORD.desc CONTAINS “BIKE”))
</code></pre> 
  <p>You can also create complex nested queries, like this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-en">RecordQuery</span>(
      <span class="pl-en">RecordQuery</span>(recordField(<span class="pl-s"><span class="pl-pds">"</span>desc<span class="pl-pds">"</span></span>) contains <span class="pl-s"><span class="pl-pds">"</span>VEHICLE<span class="pl-pds">"</span></span>)
      .or(recordField(<span class="pl-s"><span class="pl-pds">"</span>desc<span class="pl-pds">"</span></span>) contains <span class="pl-s"><span class="pl-pds">"</span>BIKE<span class="pl-pds">"</span></span>)
      .or(recordField(<span class="pl-s"><span class="pl-pds">"</span>desc<span class="pl-pds">"</span></span>) contains <span class="pl-s"><span class="pl-pds">"</span>MOTO<span class="pl-pds">"</span></span>))</pre>
  </div> 
  <p>Producing the following query:</p> 
  <pre><code>(((RECORD.desc CONTAINS “VEHICLE”)OR(RECORD.desc CONTAINS “BIKE”)OR(RECORD.desc CONTAINS “MOTO”)))
</code></pre> 
  <p>Note that you should include <code>import com.ryft.spark.connector._</code> to allow using of provided API.</p> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#date-expression" aria-hidden="true" class="anchor" id="user-content-date-expression" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Date expression</h1> 
  <p>Record queries provide a mechanism to search on "dates." Different date ranges can be searched for by modifying the expression provided. There are two supported general expression types: DATE(DateFormat operator ValueB) DATE(ValueA operator DateFormat operator ValueB) Where DateFormat is a file-specific date (e.g. 'MM/DD/YYYY' for '02/04/2015') and operator is a math comparison operator (e.g. &lt;, &gt;=, === (equality) or =/= (inequality)). You should use simple DSL to specify date field:</p> 
  <pre><code>RecordQuery(recordField("Date") contains DateValue(Format("MM/DD/YYYY") === Date("04/05/2015")))
RecordQuery(recordField("Date") contains DateValue(Date("01/05/2015") &lt; Format("MM/DD/YYYY") &lt; Date("04/05/2015")))
</code></pre> 
  <p>It produces the following queries:</p> 
  <pre><code>(RECORD.Date CONTAINS DATE(MM/DD/YYYY = 04/05/2015))
(RECORD.Date CONTAINS DATE(01/05/2015 &lt; MM/DD/YYYY &lt; 04/05/2015))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#time-expression" aria-hidden="true" class="anchor" id="user-content-time-expression" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Time expression</h1> 
  <p>Different time ranges can be searched for by modifying the expression provided. There are two general supported expression types: TIME(TimeFormat operator ValueB) TIME(ValueA operator TimeFormat operator ValueB) Where TimeFormat is a file-specific time (e.g. 'HH:MM:SS' for '09:15:00') and operator is a math comparison operator (e.g. &lt;, &gt;=, === (equality) or =/= (inequality). Here is an example:</p> 
  <pre><code>RecordQuery(recordField("Date") contains TimeValue(Format("HH:MM:SS") =/= Time("12:00:00")))
RecordQuery(recordField("Date") contains TimeValue(Time("12:00:00") &lt; Format("HH:MM:SS") &lt;= Time("12:30:00")))
</code></pre> 
  <p>Producing following queries:</p> 
  <pre><code>(RECORD.Date CONTAINS TIME(HH:MM:SS != 12:00:00))
(RECORD.Date CONTAINS TIME(12:00:00 &lt; HH:MM:SS &lt;= 12:30:00))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#numeric-expression" aria-hidden="true" class="anchor" id="user-content-numeric-expression" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Numeric expression</h1> 
  <p>Different numeric ranges can be searched for by modifying the expression provided. There are two general supported expression types: NUMBER(NUM operator1 ValueA, "subitizer", "decimal") NUMBER(ValueA operator1 NUM operator2 ValueB, "subitizer", "decimal")</p> 
  <p>The "subitizer" is defined as the separating character to use. For example, for standard US numbers, a comma would be specified. The "decimal" is defined as the decimal specifier to use. For example, for standard US numbers, a period would be specified.</p> 
  <p>NOTE: The character specified for subitizer and decimal must be different.</p> 
  <p>Here is the code example:</p> 
  <pre><code>SimpleQuery(NumericValue(NUM &lt;= Number(50), ",", "."))
SimpleQuery(NumericValue(Number(35) &lt; NUM &lt;= Number(50), ",", "."))
</code></pre> 
  <p>It produces the following queries:</p> 
  <pre><code>(RAW_TEXT CONTAINS NUMBER(NUM &lt;= "50", ",", "."))
(RAW_TEXT CONTAINS NUMBER("35" &lt; NUM &lt;= "50", ",", "."))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#currency-expression" aria-hidden="true" class="anchor" id="user-content-currency-expression" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Currency expression</h1> 
  <p>Currency searches are a type of number searches and extend the general relational expression defined previously. Different currency ranges can be searched for by modifying the expression provided. There are two general supported expression types:</p> 
  <pre><code>CURRENCY(CUR operator1 ValueA, "currency","subitizer","decimal")
CURRENCY(ValueA operator1 CUR operator2 ValueB, "currency","subitizer", "decimal")
</code></pre> 
  <p>"Subitizer" is an optional parameter for currency expression. In the US, a comma is used as the subitizer, and a period is used for the decimal, like this: $20,450.36. In Europe and elsewhere, it is the opposite (period is subitizer and comma is decimal), like this: €20.460,36.</p> 
  <p>Here is the code example:</p> 
  <pre><code>SimpleQuery(CurrencyValue(CUR &gt;= Currency("$500"), "$", "."))
SimpleQuery(CurrencyValue(Currency("$300") &lt; CUR &lt;= Currency("$500"), "$", ",", "."))
</code></pre> 
  <p>It produces the following queries:</p> 
  <pre><code>(RAW_TEXT CONTAINS CURRENCY(CUR &gt;= "$500", "$", "", "."))
(RAW_TEXT CONTAINS CURRENCY("$300" &lt; CUR &lt;= "$500", "$", ",", "."))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#regex-expression" aria-hidden="true" class="anchor" id="user-content-regex-expression" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Regex expression</h1> 
  <p>Different regex can be searched for by modifying the expression provided. REGEX(expression, "PCRE_OPTION_DEFAULT") PCRE_OPTION_DEFAULT is mandatory parameter.</p> 
  <pre><code>SimpleQuery(RegexValue("$[3-5]00", "PCRE_OPTION_DEFAULT"))
</code></pre> 
  <p>producing following query:</p> 
  <pre><code>(RAW_TEXT CONTAINS REGEX("$[3-5]00", PCRE_OPTION_DEFAULT))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#hamming--edit-search-expressions" aria-hidden="true" class="anchor" id="user-content-hamming--edit-search-expressions" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Hamming &amp; Edit search expressions</h1> 
  <p>There are two ways the Fuzzy Hamming and Fuzzy Edit Distance searches are expressed, respectively: FHS(Value,CS,DIST,WIDTH) FEDS(Value,CS,DIST,WIDTH) These are the parameters:</p> 
  <ul> 
   <li>Value: search expression</li> 
   <li>CS: denotes the case sensitive selection, true|false (optional false by default)</li> 
   <li>DIST: search distance for the primitive</li> 
   <li>WIDTH: surrounding width Here is the code example:</li> 
  </ul> 
  <pre><code>SimpleQuery(HammingValue("Jones", 1, 10))
SimpleQuery(EditValue("Jones", 2, 10, true))
</code></pre> 
  <p>It produces the following query:</p> 
  <pre><code>(RAW_TEXT CONTAINS FHS("Jones", CS=false, DIST=1, WIDTH=10))
(RAW_TEXT CONTAINS FEDS("Jones", CS=true, DIST=2, WIDTH=10))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#ipv4-expression" aria-hidden="true" class="anchor" id="user-content-ipv4-expression" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>IPv4 expression</h1> 
  <p>The IPv4 Search operation can be used to search for exact IPv4 addresses or IPv4 addresses in a particular range in both structured and unstructured text using the standard “a.b.c.d” format for IPv4 addresses.</p> 
  <pre><code>SimpleQuery(IPv4Value(IP &gt; IPv4("10.11.12.13")))
SimpleQuery(IPv4Value(IPv4("10.10.0.0") &lt;= IP &lt;= IPv4("10.10.255.255")))
</code></pre> 
  <p>producing following query:</p> 
  <pre><code>(RAW_TEXT CONTAINS IPV4(IP &gt; "10.11.12.13"))
(RAW_TEXT CONTAINS IPV4("10.10.0.0" &lt;= IP &lt;= "10.10.255.255"))
</code></pre> 
  <h1><a href="https://github.com/getryft/spark-ryft-connector#expansion-of-search-types" aria-hidden="true" class="anchor" id="user-content-expansion-of-search-types" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Expansion Of Search Types</h1> 
  <p>Query builder DSL allows users to mix different types of query into one. It would be decomposed and processed on the Ryft REST side, like this:</p> 
  <pre><code>RecordQuery(recordField("Description") contains HammingValue("vehycle", 1, 0))
    .and(recordField("ID") contains EditValue("10029", 3, 0))
    .and(recordField("Date") contains TimeValue(Time("11:50:00") &lt; Format("HH:MM:SS") &lt;= Time("11:55:00")))
</code></pre> 
  <p>It produces the following query:</p> 
  <pre><code>((RECORD.Date CONTAINS TIME(11:50:00 &lt; HH:MM:SS &lt;= 11:55:00))
    AND(RECORD.ID CONTAINS FEDS("10029", CS=false, DIST=3, WIDTH=0))
    AND(RECORD.Description CONTAINS FHS("vehycle", CS=false, DIST=1, WIDTH=0)))
</code></pre> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#query-options" aria-hidden="true" class="anchor" id="user-content-query-options" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Query options</h2> 
  <p>To provide specific settings for Ryft query, use <code>RyftQueryOptions</code></p> 
  <ul> 
   <li>files: Files to search.</li> 
   <li>surrounding: Width when generating results. For example, a value of 2 means that 2 characters before and after a search match will be included with data result. Also 'line' value means that will be included the whole line.</li> 
   <li>fuzziness: Specify the fuzzy search distance [0..255].</li> 
   <li>mode: Specify the fuzzy search mode. User fhs for Hamming search and feds for Edit distance search.</li> 
   <li>fields: Specify list of fields that should be returned (Words For structured data only).</li> 
   <li>ryftNodes: Specify number of ryft nodes to use for query.</li> 
   <li>format: Specify the format of file (xml or json).</li> 
   <li>caseSensitive: Case sensitive flag.</li> 
  </ul> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#dataframe-support" aria-hidden="true" class="anchor" id="user-content-dataframe-support" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>DataFrame support</h2> 
  <p>Current connector supports registering as data frames in Spark SQL context so that they can be queried using SparkSQL. You need to do the following:</p> 
  <ul> 
   <li>Describe a data schema.</li> 
   <li>Register the list of files to be searched by the query and the format of the file structure (xml or json). You can use Date\Time Ryft expressions; they work out of the box. You only need to specify Date/Time type in the schema and provide <code>date_format</code> option while registering data frame. Also you can use following optional parameters:</li> 
   <li><code>partitioner</code> - Class name that implements partitioning logic for data partitioning.</li> 
   <li><code>date_format</code> - Date\Time format.</li> 
   <li><code>subitizer</code> - Separating character used for numeric values.</li> 
   <li><code>decimal</code> - Decimal specifier used for numeric values.</li> 
  </ul> 
  <p>Optional parameters should be passed wrapped into Scala Map.</p> 
  <p>Here's an example in <code>scala</code>:</p> 
  <div class="highlight highlight-source-scala">
   <pre>  <span class="pl-k">val</span> <span class="pl-en">schema</span> <span class="pl-k">=</span> <span class="pl-en">StructType</span>(<span class="pl-en">Seq</span>(
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Arrest<span class="pl-pds">"</span></span>, <span class="pl-en">BooleanType</span>), <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>CaseNumber<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Date<span class="pl-pds">"</span></span>, <span class="pl-en">DateType</span>), <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Description<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>), 
    ....
  ))
  
  sqlContext.read.ryft(schema, xml, <span class="pl-s"><span class="pl-pds">"</span>*.pcrime<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>temp_table<span class="pl-pds">"</span></span>, <span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>date_format<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>MM/dd/yyyy hh:mm:ss aa<span class="pl-pds">"</span></span>))

  <span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.sql(
    <span class="pl-s"><span class="pl-pds">"""</span>select Date, Description, Arrest from temp_table</span>
<span class="pl-s">       where Description LIKE '%VEHICLE%'</span>
<span class="pl-s">          AND  Date &gt; to_date('2015-03-15')</span>
<span class="pl-s">       ORDER BY Date</span>
<span class="pl-s">    <span class="pl-pds">"""</span></span>)</pre>
  </div> 
  <p>Same code in <code>python</code>:</p> 
  <div class="highlight highlight-source-python">
   <pre>schema <span class="pl-k">=</span> StructType([
    StructField(<span class="pl-s"><span class="pl-pds">"</span>Arrest<span class="pl-pds">"</span></span>, BooleanType()), StructField(<span class="pl-s"><span class="pl-pds">"</span>CaseNumber<span class="pl-pds">"</span></span>, IntegerType()),
    StructField(<span class="pl-s"><span class="pl-pds">"</span>Date<span class="pl-pds">"</span></span>, DateType()), StructField(<span class="pl-s"><span class="pl-pds">"</span>Description<span class="pl-pds">"</span></span>, StringType()),
    <span class="pl-c1">...</span>.
)
])

df <span class="pl-k">=</span> sqlContext.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.ryft.spark.connector.sql<span class="pl-pds">"</span></span>)
    .schema(schema)
    .option(<span class="pl-s"><span class="pl-pds">"</span>files<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>*.pcrime<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>format<span class="pl-pds">"</span></span>, xml)
    .option(<span class="pl-s"><span class="pl-pds">"</span>date_format<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>MM/dd/yyyy hh:mm:ss aa<span class="pl-pds">"</span></span>)
    .load()
df.registerTempTable(<span class="pl-s"><span class="pl-pds">"</span>temp_table<span class="pl-pds">"</span></span>)

df <span class="pl-k">=</span> sqlContext.sql(<span class="pl-s"><span class="pl-pds">"</span>select Date, ID, Description, Arrest from temp_table<span class="pl-c1">\</span></span>
<span class="pl-s">       where Description LIKE '%VEHICLE%'<span class="pl-c1">\</span></span>
<span class="pl-s">          AND  Date &gt; to_date('2015-03-15')<span class="pl-c1">\</span></span>
<span class="pl-s">          AND Arrest = true<span class="pl-c1">\</span></span>
<span class="pl-s">       ORDER BY Date<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <blockquote> 
   <p>NOTE: Since Data Frames use record fields name for both data output and query generation, it is important to have the field names in RDF be the same as the names of the tagger record. In example below, the <code>name</code> field is properly defined where the <code>job</code> is not.</p> 
  </blockquote> 
  <pre><code>...
record_start  = "&lt;log&gt;";
record_end    = "&lt;/log&gt;";
data_type    = "XML";

fields = {
        name        = ( "&lt;name&gt;",                 "&lt;/name&gt;" ),
        jobDesc     = ( "&lt;job&gt;",                  "&lt;/job&gt;" )
};

</code></pre> 
  <p>For more examples in Scala, Java, PySpark an SparkR please look in <a href="https://github.com/getryft/spark-ryft-connector/blob/master/examples/src/main" target="_blank">examples</a> directory.</p> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#persisting-rdd-into-ryft" aria-hidden="true" class="anchor" id="user-content-persisting-rdd-into-ryft" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Persisting RDD into Ryft</h2> 
  <p>A RDD can be saved to an RyftBox by using the <code>saveToRyft</code> method. You can specify int using fllowing parameters:</p> 
  <ul> 
   <li><code>path</code> - a path of stored RDD on RyftBox</li> 
   <li><code>url</code> - Ryft REST mount point URL</li> 
   <li><code>format</code> - format of stored RDD (e.g. json)</li> 
   <li><code>catalog</code> - path to appended catalog file (optional)</li> 
   <li><code>overwrite</code> - whether to overwrite file/catalog or append it, default value is false (append mode)</li> 
   <li><code>local</code> - the local/cluster flag (true by default)</li> 
   <li><code>lifetime</code> - the lifetime of the uploaded file or catalog (optional)</li> 
  </ul> 
  <p>Example in <code>scala</code>:</p> 
  <div class="highlight highlight-source-scala">
   <pre>  <span class="pl-k">val</span> <span class="pl-en">query</span> <span class="pl-k">=</span> <span class="pl-en">SimpleQuery</span>(<span class="pl-s"><span class="pl-pds">"</span>Jones<span class="pl-pds">"</span></span>)
  <span class="pl-k">val</span> <span class="pl-en">queryOptions</span> <span class="pl-k">=</span> <span class="pl-en">RyftQueryOptions</span>(<span class="pl-s"><span class="pl-pds">"</span>passengers.txt<span class="pl-pds">"</span></span>, <span class="pl-c1">10</span>, <span class="pl-c1">0</span> toByte)
  
  <span class="pl-k">val</span> <span class="pl-en">ryftRDD</span> <span class="pl-k">=</span> sc.ryftRDD(<span class="pl-en">Seq</span>(query), queryOptions)
  
  ryftRDD.saveToRyft(<span class="pl-s"><span class="pl-pds">"</span>jones.xml<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>http://ryft.com<span class="pl-pds">"</span></span>, xml, <span class="pl-s"><span class="pl-pds">"</span>passengers.catalog<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>Example in <code>java</code>:</p> 
  <div class="highlight highlight-source-java">
   <pre>    <span class="pl-k">final</span> <span class="pl-smi">SparkContext</span> sc <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">SparkContext</span>(sparkConf);
    <span class="pl-k">final</span> <span class="pl-smi">SparkContextJavaFunctions</span> javaFunctions <span class="pl-k">=</span> <span class="pl-smi">RyftJavaUtil</span><span class="pl-k">.</span>javaFunctions(sc);
        
    <span class="pl-k">final</span> <span class="pl-smi">SimpleQuery</span> query <span class="pl-k">=</span> <span class="pl-smi">RyftQueryUtil</span><span class="pl-k">.</span>toSimpleQuery(<span class="pl-s"><span class="pl-pds">"</span>Jones<span class="pl-pds">"</span></span>);
    
    <span class="pl-k">final</span> <span class="pl-smi">RyftJavaRDD</span> rdd <span class="pl-k">=</span> javaFunctions<span class="pl-k">.</span>ryftRDD(query,
            <span class="pl-smi">RyftQueryOptions</span><span class="pl-k">.</span>apply(<span class="pl-s"><span class="pl-pds">"</span>passengers.txt<span class="pl-pds">"</span></span>, <span class="pl-c1">0</span>, <span class="pl-c1">10</span>),
            <span class="pl-smi">RyftJavaUtil</span><span class="pl-k">.</span>ryftQueryToEmptyList,
            <span class="pl-smi">RyftJavaUtil</span><span class="pl-k">.</span>stringToEmptySet);
            
     <span class="pl-k">final</span> <span class="pl-k">Map&lt;<span class="pl-smi">String</span>, <span class="pl-smi">String</span>&gt;</span> params <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-k">HashMap&lt;&gt;</span>();
     params<span class="pl-k">.</span>put(<span class="pl-s"><span class="pl-pds">"</span>url<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>http://ryft.com<span class="pl-pds">"</span></span>);
     params<span class="pl-k">.</span>put(<span class="pl-s"><span class="pl-pds">"</span>format<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>xml<span class="pl-pds">"</span></span>);
     params<span class="pl-k">.</span>put(<span class="pl-s"><span class="pl-pds">"</span>catalog<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>passengers.catalog<span class="pl-pds">"</span></span>);

    javaFunctions<span class="pl-k">.</span>saveToRyft(rdd, <span class="pl-s"><span class="pl-pds">"</span>jones.xml<span class="pl-pds">"</span></span>, params);</pre>
  </div> 
  <p>Also connector supports saving each entry from PairRDD by using <code>saveEachEntry</code> method. In this case key of an entry uses as saving path and value takes as stored data.</p> 
  <p>This is a <code>scala</code> example:</p> 
  <div class="highlight highlight-source-scala">
   <pre>  <span class="pl-k">val</span> <span class="pl-en">query</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-en">SimpleQuery</span>(<span class="pl-s"><span class="pl-pds">"</span>Jones<span class="pl-pds">"</span></span>),<span class="pl-en">SimpleQuery</span>(<span class="pl-s"><span class="pl-pds">"</span>Thomas<span class="pl-pds">"</span></span>))
  <span class="pl-k">val</span> <span class="pl-en">queryOptions</span> <span class="pl-k">=</span> <span class="pl-en">RyftQueryOptions</span>(<span class="pl-s"><span class="pl-pds">"</span>passengers.txt<span class="pl-pds">"</span></span>, <span class="pl-c1">10</span>, <span class="pl-c1">0</span> toByte)
  
  <span class="pl-k">val</span> <span class="pl-en">ryftRDD</span> <span class="pl-k">=</span> sc.ryftPairRDD(query, queryOptions)

  <span class="pl-k">val</span> <span class="pl-en">result</span> <span class="pl-k">=</span> ryftRDD.map((_, <span class="pl-c1">1</span>)).reduceByKey(_ <span class="pl-k">+</span> _).saveEachEntryToRyft(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>url<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>http://ryft.com<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>format<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>xml<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>catalog<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>passengers.catalog<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#persisting-dataframe-into-ryft" aria-hidden="true" class="anchor" id="user-content-persisting-dataframe-into-ryft" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Persisting DataFrame into Ryft</h2> 
  <p>Storing DataFrame is similar to section above. Here's an example in <code>scala</code>:</p> 
  <div class="highlight highlight-source-scala">
   <pre>  <span class="pl-k">val</span> <span class="pl-en">schema</span> <span class="pl-k">=</span> <span class="pl-en">StructType</span>(<span class="pl-en">Seq</span>(
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Arrest<span class="pl-pds">"</span></span>, <span class="pl-en">BooleanType</span>), <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>CaseNumber<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Date<span class="pl-pds">"</span></span>, <span class="pl-en">DateType</span>), <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Description<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>), 
    ....
  ))
  
  sqlContext.read.ryft(schema, xml, <span class="pl-s"><span class="pl-pds">"</span>*.pcrime<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>temp_table<span class="pl-pds">"</span></span>, <span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>date_format<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>MM/dd/yyyy hh:mm:ss aa<span class="pl-pds">"</span></span>))

  <span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.sql(
    <span class="pl-s"><span class="pl-pds">"""</span>select Date, Description, Arrest from temp_table</span>
<span class="pl-s">       where Description LIKE '%VEHICLE%'</span>
<span class="pl-s">          AND  Date &gt; to_date('2015-03-15')</span>
<span class="pl-s">       ORDER BY Date</span>
<span class="pl-s">    <span class="pl-pds">"""</span></span>)
    
    df.saveToRyft(<span class="pl-s"><span class="pl-pds">"</span>2015-03-15.xml<span class="pl-pds">"</span></span>, <span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>url<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>http://ryft.com<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>format<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>xml<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>overwrite<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">true</span>);</pre>
  </div> 
  <p>Same code in <code>python</code>:</p> 
  <div class="highlight highlight-source-python">
   <pre>  schema <span class="pl-k">=</span> StructType([
      StructField(<span class="pl-s"><span class="pl-pds">"</span>Arrest<span class="pl-pds">"</span></span>, BooleanType()), StructField(<span class="pl-s"><span class="pl-pds">"</span>CaseNumber<span class="pl-pds">"</span></span>, IntegerType()),
      StructField(<span class="pl-s"><span class="pl-pds">"</span>Date<span class="pl-pds">"</span></span>, DateType()), StructField(<span class="pl-s"><span class="pl-pds">"</span>Description<span class="pl-pds">"</span></span>, StringType()),
      <span class="pl-c1">...</span>.
  )
  ])
  
  df <span class="pl-k">=</span> sqlContext.read\
      .format(<span class="pl-s"><span class="pl-pds">"</span>com.ryft.spark.connector.sql<span class="pl-pds">"</span></span>)\
      .schema(schema)\
      .option(<span class="pl-s"><span class="pl-pds">"</span>files<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>*.pcrime<span class="pl-pds">"</span></span>)\
      .option(<span class="pl-s"><span class="pl-pds">"</span>format<span class="pl-pds">"</span></span>, xml)\
      .option(<span class="pl-s"><span class="pl-pds">"</span>date_format<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>MM/dd/yyyy hh:mm:ss aa<span class="pl-pds">"</span></span>)\
      .load()
  df.registerTempTable(<span class="pl-s"><span class="pl-pds">"</span>temp_table<span class="pl-pds">"</span></span>)
  
  df <span class="pl-k">=</span> sqlContext.sql(<span class="pl-s"><span class="pl-pds">"</span>select Date, ID, Description, Arrest from temp_table<span class="pl-c1">\</span></span>
<span class="pl-s">         where Description LIKE '%VEHICLE%'<span class="pl-c1">\</span></span>
<span class="pl-s">            AND  Date &gt; to_date('2015-03-15')<span class="pl-c1">\</span></span>
<span class="pl-s">            AND Arrest = true<span class="pl-c1">\</span></span>
<span class="pl-s">         ORDER BY Date<span class="pl-pds">"</span></span>)
         
  df.write\
      .format(<span class="pl-s"><span class="pl-pds">"</span>com.ryft.spark.connector.sql<span class="pl-pds">"</span></span>)\
      .options(<span class="pl-v">url</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>http://ryft.com<span class="pl-pds">"</span></span>, <span class="pl-v">format</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>xml<span class="pl-pds">"</span></span>, <span class="pl-v">overwrite</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>true<span class="pl-pds">"</span></span>)\
      .save(<span class="pl-s"><span class="pl-pds">"</span>2015-03-15.xml<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#spark-config-options" aria-hidden="true" class="anchor" id="user-content-spark-config-options" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark Config Options</h2> 
  <p>The following options can be set via SparkConf, command line options or Zeppelin Spark interpreter settings:</p> 
  <ul> 
   <li><code>spark.ryft.rest.url</code> - semicolon separated list of Ryft rest endpoints for search. For example: <a href="" target="_blank">http://ryftone-1:8765;http://ryftone-2:8765</a></li> 
   <li><code>spark.ryft.consul.url</code> - Ryft Consul address. For example: <a href="http://ryftone-1:8500" target="_blank">http://ryftone-1:8500</a></li> 
   <li><code>spark.ryft.rest.ssl.untrusted</code> - boolean value that allows the use of self-signed SSL certificate. Set <code>true</code> only for testing.</li> 
   <li><code>spark.ryft.nodes</code> - number of Ryft hardware nodes to be used by queries. Corresponds to <code>nodes</code> rest api query parameter.</li> 
   <li><code>spark.ryft.partitioner</code> - Canonical class name that implements partitioning logic for data partitioning and collocation. See examples below.</li> 
   <li><code>spark.ryft.consul.partitioning</code> - Whether to use Consul partitioning mechanism.</li> 
   <li><code>spark.ryft.auth.username</code> - login for password authentication (required)</li> 
   <li><code>spark.ryft.auth.password</code> - password for password authentication (required)</li> 
   <li><code>spark.ryft.rest.save.numRetries</code> - amount of retries for save to Ryft, default value is 3</li> 
  </ul> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#data-partitioning-and-locality-support" aria-hidden="true" class="anchor" id="user-content-data-partitioning-and-locality-support" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Data Partitioning and locality support</h2> 
  <p>If setting <code>spark.ryft.rest.url</code> to multiple endpoints, then by default, search requests will be done on each of the servers and the results will be combined. To override this logic, do round-robin requests or execute requests depending on query value, then you can specify partitioning function or class.</p> 
  <p>The Partitioning class can be used with both RDD and DataFrame examples regardless of programming language. It is applied by loading jar file in spark context with the class that extends <code>com.ryft.spark.connector.partitioner.RyftPartitioner</code> and specifying its cannonical name via <code>spark.ryft.partitioner</code> configuration value. It can be done globally or on per query level. See <a href="https://github.com/getryft/spark-ryft-connector/blob/master/examples/src/main/scala/com/ryft/spark/connector/examples/DataFrameExample.scala" target="_blank">full example</a> or this code snippet:</p> 
  <div class="highlight highlight-source-scala">
   <pre>  sqlContext.read.ryft(schema, <span class="pl-s"><span class="pl-pds">"</span>*.pcrime<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>temp_table<span class="pl-pds">"</span></span>, <span class="pl-c1">classOf</span>[<span class="pl-en">ArrestPartitioner</span>].getCanonicalName)
  ...
  
  
  <span class="pl-k">class</span> <span class="pl-en">ArrestPartitioner</span> <span class="pl-k">extends</span> <span class="pl-e">RyftPartitioner</span> {
  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">partitions</span>(<span class="pl-v">query</span>: <span class="pl-en">RyftQuery</span>)<span class="pl-k">:</span> <span class="pl-en">Set</span>[<span class="pl-en">URL</span>] <span class="pl-k">=</span> query <span class="pl-k">match</span> {
    <span class="pl-k">case</span> <span class="pl-v">rq</span>: <span class="pl-en">RecordQuery</span> <span class="pl-k">=&gt;</span>
      partitionsAcc(rq.entries, <span class="pl-en">List</span>.empty[<span class="pl-k">String</span>]).filter(_.nonEmpty).map(<span class="pl-k">new</span> <span class="pl-en">URL</span>(_))
    <span class="pl-k">case</span> _ <span class="pl-k">=&gt;</span>
      <span class="pl-k">throw</span> <span class="pl-en">RyftSparkException</span>(s<span class="pl-s"><span class="pl-pds">"</span>Unable to find partitions for such type of query: ${query.getClass}<span class="pl-pds">"</span></span>)
  }

  <span class="pl-k">@</span>tailrec
  <span class="pl-k">private</span> <span class="pl-k">def</span> <span class="pl-en">partitionsAcc</span>(<span class="pl-v">entries</span>: <span class="pl-en">Set</span>[(<span class="pl-k">String</span>,<span class="pl-k">String</span>)], <span class="pl-v">acc</span>: <span class="pl-en">List</span>[<span class="pl-k">String</span>])<span class="pl-k">:</span> <span class="pl-en">Set</span>[<span class="pl-k">String</span>] <span class="pl-k">=</span> {
    <span class="pl-k">if</span>(entries.isEmpty) acc.toSet
    <span class="pl-k">else</span> partitionsAcc(entries.tail, byFirsLetter(entries.head) <span class="pl-k">::</span> acc)
  }

  <span class="pl-k">private</span> <span class="pl-k">def</span> <span class="pl-en">byFirsLetter</span>(<span class="pl-v">entry</span>: (<span class="pl-k">String</span>,<span class="pl-k">String</span>)) <span class="pl-k">=</span> {
    <span class="pl-k">if</span> (entry._1.equalsIgnoreCase(<span class="pl-s"><span class="pl-pds">"</span>arrest<span class="pl-pds">"</span></span>)) {
      <span class="pl-k">if</span> (entry._2.equalsIgnoreCase(<span class="pl-s"><span class="pl-pds">"</span>true<span class="pl-pds">"</span></span>)) <span class="pl-s"><span class="pl-pds">"</span>http://ryftone-1:8765<span class="pl-pds">"</span></span>
      <span class="pl-k">else</span> <span class="pl-s"><span class="pl-pds">"</span>http://ryftone-2:8765<span class="pl-pds">"</span></span>  
    } <span class="pl-k">else</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
  }
}
</pre>
  </div> 
  <p>The partitioning function works for RDD requests in Java and Scala, and can be specified as a parameter to the RDD methods. For example:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> If country name starts with [a-n] go to ryftone-2:8765</span>
<span class="pl-c"><span class="pl-c">//</span> Otherwise go to ryftone-2:8765</span>
<span class="pl-k">def</span> <span class="pl-en">byCountry</span>(<span class="pl-v">query</span>: <span class="pl-en">RyftQuery</span>)<span class="pl-k">:</span> <span class="pl-en">List</span>[<span class="pl-k">String</span>] <span class="pl-k">=</span> {
    <span class="pl-k">def</span> <span class="pl-en">byFirstLetter</span>(<span class="pl-v">country</span>: <span class="pl-k">String</span>)<span class="pl-k">:</span> <span class="pl-en">List</span>[<span class="pl-k">String</span>] <span class="pl-k">=</span> {
        <span class="pl-k">if</span> ((<span class="pl-c1">'a'</span> to <span class="pl-c1">'n'</span>).contains(country.head.toLower)) 
            <span class="pl-en">List</span>(<span class="pl-s"><span class="pl-pds">"</span>http://ryftone-1:8765<span class="pl-pds">"</span></span>)
        <span class="pl-k">else</span> 
            <span class="pl-en">List</span>(<span class="pl-s"><span class="pl-pds">"</span>http://ryftone-2:8765<span class="pl-pds">"</span></span>)   
    }
    
    <span class="pl-en">RyftPartitioner</span>.byField(query, byFirstLetter, <span class="pl-en">Seq</span>(<span class="pl-s"><span class="pl-pds">"</span>country<span class="pl-pds">"</span></span>)).toList
}
  
sc.ryftRDD(<span class="pl-en">List</span>(query), qoptions, byCountry)
    .<span class="pl-c1">asInstanceOf</span>[<span class="pl-en">RyftRDD</span>[<span class="pl-en">Map</span>[<span class="pl-k">String</span>, <span class="pl-k">String</span>]]].map(r<span class="pl-k">=&gt;</span> <span class="pl-k">new</span> <span class="pl-en">Record</span>(r))
    .toDF().cache().registerTempTable(<span class="pl-s"><span class="pl-pds">"</span>query2<span class="pl-pds">"</span></span>)
    
println(<span class="pl-s"><span class="pl-pds">"</span>query2: <span class="pl-pds">"</span></span> <span class="pl-k">+</span> sqlc.table(<span class="pl-s"><span class="pl-pds">"</span>query2<span class="pl-pds">"</span></span>).count)</pre>
  </div> 
  <p>In case of Spark node running on Ryft ONE box, you can use partitioning to implement co-location strategy. For example:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">java.net.</span><span class="pl-v">URL</span>

<span class="pl-c"><span class="pl-c">//</span> This query uses preferred node selection</span>
<span class="pl-c"><span class="pl-c">//</span> All requests to ryft boxes will request execution on the corresponding node (with same ip)</span>
<span class="pl-c"><span class="pl-c">//</span> If no nodes availble execution will fallback to other free nodes</span>

<span class="pl-k">def</span> <span class="pl-en">sameNodeSelection</span>(<span class="pl-v">url</span>: <span class="pl-k">String</span>) <span class="pl-k">=</span> {
    <span class="pl-k">val</span> <span class="pl-en">preferredNodes</span> <span class="pl-k">=</span> <span class="pl-en">Set</span>(<span class="pl-k">new</span> <span class="pl-en">URL</span>(url).getHost)
    println(<span class="pl-s"><span class="pl-pds">"</span>preferred nodes: <span class="pl-pds">"</span></span> <span class="pl-k">+</span> url <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">"</span>-&gt;<span class="pl-pds">"</span></span> <span class="pl-k">+</span> preferredNodes)
    preferredNodes
  }
  
sc.ryftRDD(<span class="pl-en">List</span>(query), qoptions, nopart, sameNodeSelection)
    .<span class="pl-c1">asInstanceOf</span>[<span class="pl-en">RyftRDD</span>[<span class="pl-en">Map</span>[<span class="pl-k">String</span>, <span class="pl-k">String</span>]]].map(r<span class="pl-k">=&gt;</span> <span class="pl-k">new</span> <span class="pl-en">Record</span>(r))
    .toDF().cache().registerTempTable(<span class="pl-s"><span class="pl-pds">"</span>query3<span class="pl-pds">"</span></span>)
    
println(<span class="pl-s"><span class="pl-pds">"</span>query3: <span class="pl-pds">"</span></span> <span class="pl-k">+</span> sqlc.table(<span class="pl-s"><span class="pl-pds">"</span>query3<span class="pl-pds">"</span></span>).count)</pre>
  </div> 
  <p>NOTE: When multiple partitioning rules are applied, the one with most priority is used from low to high: <code>partition class in sparkConfig</code>, <code>partition class in query</code>, <code>partition function</code>.</p> 
  <p>##CI and Publishing</p> 
  <p>Continuous integration and deployment are managed with <a href="https://travis-ci.org/getryft/spark-ryft-connector" target="_blank">Travis CI</a> service. As soon as a commit is pushed to the repository on Github, a continuous integration cycle is launched:</p> 
  <ul> 
   <li>automatic building for Pull Requests and commits;</li> 
   <li>automatic tests execution for Pull Requests and commits;</li> 
   <li>building and testing project against multiple Scala versions: 2.10.6 and 2.11.7;</li> 
   <li>notifying about build results;</li> 
   <li>uploading assets to Maven Central and Github Releases repositories (only for tagged commit on master branch);</li> 
  </ul> 
  <h2><a href="https://github.com/getryft/spark-ryft-connector#code-contributions" aria-hidden="true" class="anchor" id="user-content-code-contributions" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Code Contributions</h2> 
  <p>Contributions guidelines can be found in <a href="https://github.com/getryft/spark-ryft-connector/blob/master/CONTRIBUTING.md" target="_blank">CONTRIBUTING</a> file.</p> 
  <p>##License Ryft-Customized BSD License is covered in <a href="https://github.com/getryft/spark-ryft-connector/blob/master/LICENSE" target="_blank">LICENSE</a> file.</p> 
 </article>
</div>