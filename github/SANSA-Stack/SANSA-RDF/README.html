<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/SANSA-Stack/SANSA-RDF#sansa-rdf" aria-hidden="true" class="anchor" id="user-content-sansa-rdf" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SANSA RDF</h1> 
  <p><a href="https://maven-badges.herokuapp.com/maven-central/net.sansa-stack/sansa-rdf-parent_2.11" target="_blank"><img src="https://camo.githubusercontent.com/38818d3b55391c4719172fc8ec34e2c12ed0eec5/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f6e65742e73616e73612d737461636b2f73616e73612d7264662d706172656e745f322e31312f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/net.sansa-stack/sansa-rdf-parent_2.11/badge.svg" style="max-width:100%;"></a> <a href="https://ci.aksw.org/jenkins/job/SANSA%20RDF/job/develop/" target="_blank"><img src="https://camo.githubusercontent.com/b36e67ada4f88bbc3f0b73158b848f54040e1bd6/68747470733a2f2f63692e616b73772e6f72672f6a656e6b696e732f6a6f622f53414e53412532305244462f6a6f622f646576656c6f702f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://ci.aksw.org/jenkins/job/SANSA%20RDF/job/develop/badge/icon" style="max-width:100%;"></a> <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"></a></p> 
  <h2><a href="https://github.com/SANSA-Stack/SANSA-RDF#description" aria-hidden="true" class="anchor" id="user-content-description" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Description</h2> 
  <p>SANSA RDF is a library to read RDF files into <a href="https://spark.apache.org" target="_blank">Spark</a> or <a href="https://flink.apache.org" target="_blank">Flink</a>. It allows files to reside in HDFS as well as in a local file system and distributes them across Spark RDDs/Datasets or Flink DataSets.</p> 
  <p>SANSA uses the RDF data model for representing graphs consisting of triples with subject, predicate and object. RDF datasets may contains multiple RDF graphs and record information about each graph, allowing any of the upper layers of sansa (Querying and ML) to make queries that involve information from more than one graph. Instead of directly dealing with RDF datasets, the target RDF datasets need to be converted into an RDD/DataSets of triples. We name such an RDD/DataSets a main dataset. The main dataset is based on an RDD/DataSets data structure, which is a basic building block of the Spark/Flink framework. RDDs/DataSets are in-memory collections of records that can be operated on in parallel on large clusters.</p> 
  <h3><a href="https://github.com/SANSA-Stack/SANSA-RDF#sansa-rdf-spark" aria-hidden="true" class="anchor" id="user-content-sansa-rdf-spark" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SANSA RDF Spark</h3> 
  <p>The main application class is <code>net.sansa_stack.rdf.spark.App</code>. The application requires as application arguments:</p> 
  <ol> 
   <li>path to the input folder containing the data as nt (e.g. <code>/data/input</code>)</li> 
   <li>path to the output folder to write the resulting to (e.g. <code>/data/output</code>)</li> 
  </ol> 
  <p>All Spark workers should have access to the <code>/data/input</code> and <code>/data/output</code> directories.</p> 
  <h2><a href="https://github.com/SANSA-Stack/SANSA-RDF#running-the-application-on-a-spark-standalone-cluster" aria-hidden="true" class="anchor" id="user-content-running-the-application-on-a-spark-standalone-cluster" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Running the application on a Spark standalone cluster</h2> 
  <p>To run the application on a standalone Spark cluster</p> 
  <ol> 
   <li>Setup a Spark cluster</li> 
   <li>Build the application with Maven</li> 
  </ol> 
  <pre><code>cd /path/to/application
mvn clean package
</code></pre> 
  <ol start="3"> 
   <li>Submit the application to the Spark cluster</li> 
  </ol> 
  <pre><code>spark-submit \
  	--class net.sansa_stack.rdf.spark.App \
  	--master spark://spark-master:7077 \
  	/app/application.jar \
  	/data/input /data/output  
</code></pre> 
  <h2><a href="https://github.com/SANSA-Stack/SANSA-RDF#running-the-application-on-a-spark-standalone-cluster-via-docker" aria-hidden="true" class="anchor" id="user-content-running-the-application-on-a-spark-standalone-cluster-via-docker" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Running the application on a Spark standalone cluster via Docker</h2> 
  <p>To run the application, execute the following steps:</p> 
  <ol> 
   <li>Setup a Spark cluster as described on <a href="http://github.com/big-data-europe/docker-spark" target="_blank">http://github.com/big-data-europe/docker-spark</a>.</li> 
   <li>Build the Docker image: <code>docker build --rm=true -t sansa/spark-rdf .</code></li> 
   <li>Run the Docker container: <code>docker run --name Spark-RDF -e ENABLE_INIT_DAEMON=false --link spark-master:spark-master -d sansa/spark-rdf</code></li> 
  </ol> 
  <h2><a href="https://github.com/SANSA-Stack/SANSA-RDF#running-the-application-on-a-spark-standalone-cluster-via-sparkhdfs-workbench" aria-hidden="true" class="anchor" id="user-content-running-the-application-on-a-spark-standalone-cluster-via-sparkhdfs-workbench" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Running the application on a Spark standalone cluster via Spark/HDFS Workbench</h2> 
  <p>Spark/HDFS Workbench Docker Compose file contains HDFS Docker (one namenode and two datanodes), Spark Docker (one master and one worker) and HUE Docker as an HDFS File browser to upload files into HDFS easily. Then, this workbench will play a role as for Spark-RDF application to perform computations. Let's get started and deploy our pipeline with Docker Compose. Run the pipeline:</p> 
  <pre><code>docker network create hadoop
docker-compose up -d
</code></pre> 
  <p>First, let’s throw some data into our HDFS now by using Hue FileBrowser runing in our network. To perform these actions navigate to <a href="http://your.docker.host:8088/home" target="_blank">http://your.docker.host:8088/home</a>. Use “hue” username with any password to login into the FileBrowser (“hue” user is set up as a proxy user for HDFS, see hadoop.env for the configuration parameters). Click on “File Browser” in upper right corner of the screen and use GUI to create /user/root/input and /user/root/output folders and upload the data file into /input folder. Go to <a href="http://your.docker.host:50070" target="_blank">http://your.docker.host:50070</a> and check if the file exists under the path ‘/user/root/input/yourfile.nt’.</p> 
  <p>After we have all the configuration needed for our example, let’s rebuild Spark-RDF.</p> 
  <pre><code>docker build --rm=true -t sansa/spark-rdf .
</code></pre> 
  <p>And then just run this image:</p> 
  <pre><code>docker run --name Spark-RDF --net hadoop --link spark-master:spark-master \
-e ENABLE_INIT_DAEMON=false \
-d sansa/spark-rdf
</code></pre> 
  <h2><a href="https://github.com/SANSA-Stack/SANSA-RDF#usage" aria-hidden="true" class="anchor" id="user-content-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage</h2> 
  <p>The following Scala code shows how to read an RDF file (be it a local file or a file residing in HDFS) into a Spark RDD:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">net.sansa_stack.rdf.spark.io.</span><span class="pl-v">NtripleReader</span>

<span class="pl-k">val</span> <span class="pl-en">input</span> <span class="pl-k">=</span> sc.textFile(<span class="pl-s"><span class="pl-pds">"</span>hdfs://...<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">triplesRDD</span> <span class="pl-k">=</span> <span class="pl-en">NTripleReader</span>.load(sparkSession, <span class="pl-k">new</span> <span class="pl-en">File</span>(input))

triplesRDD.take(<span class="pl-c1">5</span>).foreach(println(_))</pre>
  </div> 
  <p>An overview is given in the <a href="http://sansa-stack.net/faq/#rdf-processing" target="_blank">FAQ section of the SANSA project page</a>. Further documentation about the builder objects can also be found on the <a href="http://sansa-stack.net/scaladocs/" target="_blank">ScalaDoc page</a>.</p> 
 </article>
</div>