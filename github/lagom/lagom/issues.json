{
  "data":{
    "repository":{
      "issues":{
        "nodes":[
          {
            "number":44,
            "title":"Reduce annotation clutter of Immutables examples",
            "bodyText":"Looks like we can reduce the number of annotations. Moving things to the style annotation. See http://immutables.github.io/json.html#reducing-annotation-clutter\nExample: https://twitter.com/ImmutablesOrg/status/709478308593807364\nEspecially getting rid of the JsonDeserialize is very nice.",
            "url":"https://github.com/lagom/lagom/issues/44"
          },
          {
            "number":59,
            "title":"Kubernetes support",
            "bodyText":"It would be great if Lagom also supported Kubernetes as a target production platform.\nThis would require the following things:\n\nKubernetes based ServiceLocator implementation\nPick up configuration for Akka clustering, service location and database location from Kubernetes\nsbt-native-packager based packaging to produce artifacts that can be deployed by Kubernetes\nDocumentation\n\nTo be clear, this is not going to be implemented by Lightbend, but we are creating this issue to indicate that we would be happy to accept such contributions from the community.  We recommend, at least initially, implementing it in a separate repo, which could later be pulled into the Lagom GitHub organization if that makes sense.",
            "url":"https://github.com/lagom/lagom/issues/59"
          },
          {
            "number":86,
            "title":"API for provisioning persistent stream subscriptions",
            "bodyText":"We could provide an API for provisioning persistent stream subscriptions between services.  By persistent, I mean if a disconnect occurs, it reconnects (with exponential backoff).\nThe purpose of this is to provide point to point at least once message delivery between two clustered services.\nAt a minimum we could support running as a cluster singleton, but we could also let Lagom handle partitioning, so that we can subscribe to partitioned streams - this would need to be paired with nice partitioning support in the persistent entities event stream (probably isn't any hard then simply tagging events by partition, sharding by event id over the number of partitions).  To implement this, we could simply allow the client side to shard partitions across the cluster - it would need to ask the service how many partitions there are to do so.\nAdditionally, we could also provide some helpers that would manage recording and loading offsets in Cassandra for implementing at least one delivery - this doesn't have to be part of the persistent stream API, but could be a helper component that helps in implementing the code that sets up the stream.",
            "url":"https://github.com/lagom/lagom/issues/86"
          },
          {
            "number":143,
            "title":"Add Forwarded headers in service gateway",
            "bodyText":"We should add forwarded headers to the service gateway for people that want to query remote IP/protocol.",
            "url":"https://github.com/lagom/lagom/issues/143"
          },
          {
            "number":146,
            "title":"Lagom with Gradle Support",
            "bodyText":"It would be nice to have have Gradle support with lagom, just like the recently  added  maven support",
            "url":"https://github.com/lagom/lagom/issues/146"
          },
          {
            "number":322,
            "title":"Support unmanaged services on tests.",
            "bodyText":"It should be possible to register unmanaged services on a TestService.Setup so that the current TestServiceLocator will resolve them.\nCurrently TestServiceLocator only resolves one address and that's an 'http://localhost:port` for the service under test. Same thing we allow running integration tests to unmanaged DBs configuring them via application.conf, it might be interesting to support injecting service mappings into TestServiceLocator. These may be sandbox services provided by 3rd parties I could be running my tests to so it'd be good if the unmanaged services were not restricted to localhost.",
            "url":"https://github.com/lagom/lagom/issues/322"
          },
          {
            "number":325,
            "title":"TransportExceptions serialization and missing equals/hashCode",
            "bodyText":"Often the service-impl is small enough that the user will throw a subclass of TransportException (e.g. Forbidden) from a Persistent Entity.\nCurrently the serializer picked up for some subclasses of TransportException is the java serializer which should avoided. Also, not all subclasses of TransportException implement equals and hashCode. that last bit is not a big issue but causes the PersistentEntityTestDriver to fail the serialization test for those objects ( https://github.com/lagom/lagom/blob/master/testkit/javadsl/src/main/scala/com/lightbend/lagom/javadsl/testkit/PersistentEntityTestDriver.scala#L235) even when the serialization was successful.",
            "url":"https://github.com/lagom/lagom/issues/325"
          },
          {
            "number":401,
            "title":"Small fixes required in PersistentEntity documentation",
            "bodyText":"There's few improvements to be done in Persistent Entity (scaladsl v1.3):\n\n sample code in tests uses scalactic's  ConversionCheckedTripleEquals which is deprecated in favour of TypeCheckedTripleEquals\n contains FIXME in sbt code",
            "url":"https://github.com/lagom/lagom/issues/401"
          },
          {
            "number":454,
            "title":"Explicit JPA schema generation fails silently with EclipseLink",
            "bodyText":"Lagom Version\n1.3.0-SNAPSHOT\nAPI\nJava\nLibrary Dependencies\n\"org.eclipse.persistence\" % \"org.eclipse.persistence.jpa\" % \"2.6.4\"\nExpected Behavior\n\nClone https://github.com/TimMoore/activator-lagom-java-chirper-jpa.git and check out jpa-example-h2-mem-eclipselink\nsbt runAll\nLoad http://localhost:9000\n\nActual Behavior\nEclipseLink logs an error\nInternal Exception: org.h2.jdbc.JdbcSQLException: Table \"CHIRP\" not found; SQL statement:\nSELECT USERID AS a1, MESSAGE AS a2, TIMESTAMP AS a3, UUID AS a4 FROM chirp WHERE (USERID IN (?)) ORDER BY TIMESTAMP DESC LIMIT ? OFFSET ? [42102-192]\nError Code: 42102\nCall: SELECT USERID AS a1, MESSAGE AS a2, TIMESTAMP AS a3, UUID AS a4 FROM chirp WHERE (USERID IN (?)) ORDER BY TIMESTAMP DESC LIMIT ? OFFSET ?\n\tbind => [3 parameters bound]\nQuery: ReportQuery(referenceClass=ChirpJpaEntity sql=\"SELECT USERID AS a1, MESSAGE AS a2, TIMESTAMP AS a3, UUID AS a4 FROM chirp WHERE (USERID IN ?) ORDER BY TIMESTAMP DESC LIMIT ? OFFSET ?\")\n\nDetails\nIt appears that EclipseLink does not support schema creation after the persistence unit is initially loaded (with Persistence.createEntityManagerFactory, which is called by JpaSessionImpl when it is initialized).\nI interpret this as a bug in EclipseLink, so I'll plan to follow up with them using a simplified test case.\nWorkarounds\n\nYou can set schema creation properties in persistence.xml to generate the schema as soon as the persistence unit is initialized. Keep in mind, however, that this could occur simultaneously on multiple nodes of a clustered deployment and lead to unpredictable results.\nYou can create the schema using scripts loaded manually into the database (potentially generated using JPA's ability to generate schema script files) or through some other mechanism outside of Lagom. This is recommended.",
            "url":"https://github.com/lagom/lagom/issues/454"
          },
          {
            "number":466,
            "title":"Document requirement for akka.actor.enable-additional-serialization-bindings = on ",
            "bodyText":"1.3.x Scala\nThis should either be a default (best) or at least documented.\nI have a state in a persistent entity that is Option[MyThing]\nI create serializers for it but the run time checking complains I have no serializer for None\nSolution is to add\nakka.actor.enable-additional-serialization-bindings = on\nbut this is only documented in the google group. Sorry guys I don't have time do this myself right now\nsee also discussion #297",
            "url":"https://github.com/lagom/lagom/issues/466"
          },
          {
            "number":475,
            "title":"Provide method for allowing the service gateway to serve static assets",
            "bodyText":"The intention here is to allow straight forward integration with SPAs built by other build tools, such as grunt.\nWhat we should do is allow directories to be \"mounted\" into the service gateways router, that can be served directly by the service gateway.  Configuring it might look like this in sbt:\nlagomServiceGatewayAssets in ThisBuild += (\"/\" -> ((baseDirectory in ThisBuild).value / \"web\" / \"build\"))\nor in Maven:\n<serviceGatewayAssets>\n  <assets>\n    <path>/</path>\n    <directory>${basedir}/web/build</directory>\n  </assets>\n</serviceGatewayAssets>\nIn documenting this, it should also be explained how to integrate things like grunts watch mode into the build, so that it starts when Lagom runAll starts.",
            "url":"https://github.com/lagom/lagom/issues/475"
          },
          {
            "number":481,
            "title":"Increase Typesafety on JavadslServerBuilder#createServiceInfo",
            "bodyText":"(from #349 (comment) by @TimMoore )\n\nI think we can make all of this more type safe:\n\n\nChange the type parameter of ServiceBinding and everything that it affects to T extends Service\nChange ServiceInfoProvider to take Class[_ <: Service]\nChange this method to do the same\n\nThen the dynamic type check isn't really necessary.\nThis would technically break source compatibility, but only for cases that wouldn't have worked in the first place (calling bindServices on things that aren't service interfaces).",
            "url":"https://github.com/lagom/lagom/issues/481"
          },
          {
            "number":497,
            "title":"Document / implement consistency tuning of the Cassandra Read-Side implementation",
            "bodyText":"By default, it takes several seconds for the read-side (in at least the Cassandra implementation) to be consistent with the PersistentEntity. There is no documentation to be found on this in Lagom's documentation and could give the illusion that the system is either slow or something buggy is going on.\nThe configuration for the consistency strategy follows the configuration as found in: https://github.com/akka/akka-persistence-cassandra/blob/master/src/main/resources/reference.conf#L528\nConfiguration through either eventual-consistency-delay or delayed-event-timeout parameters of the cassandra-query-journal should be documented, or be added as a configuration parameter directly in Lagom.",
            "url":"https://github.com/lagom/lagom/issues/497"
          },
          {
            "number":505,
            "title":"Document pretty printing JSON",
            "bodyText":"Pretty printing JSON is always a fairly common request. We should document it.\nThe solution for Scala is here:\nhttps://groups.google.com/d/msg/lagom-framework/TVzzoa9b-EE/rQ4RkHCpBgAJ\nWe could make that simpler by providing a non implicit pretty printing serializer out of the box, that users can import as an implicit when they want pretty printing.",
            "url":"https://github.com/lagom/lagom/issues/505"
          },
          {
            "number":506,
            "title":"Journal table auto-creation fails on Oracle",
            "bodyText":"Lagom Version\n1.2.2, 1.3.0-RC2\nAPI\nBoth\nLibrary Dependencies\n      \"com.oracle.jdbc\" % \"ojdbc7\" % \"12.1.0.2\",\n      \"com.typesafe.slick\" %% \"slick-extensions\" % \"3.1.0\"\nExpected Behavior\nPlease describe the expected behavior of the issue, starting from the first action.\n\nRun a local Oracle 12c server and create a user named and identified by \"chirp\" with the ability to create tables (not for the faint of heart... talk to me if you'd like to try to reproduce)\nCheck out the jdbc-example-oracle branch of https://github.com/TimMoore/activator-lagom-java-chirper-jdbc\nsbt runAll\n\nActual Behavior\nThese errors are logged at startup:\n2017-02-20 17:16:26,885 [debug] chirp-impl-application-akka.actor.default-dispatcher-2 - c.l.l.i.p.j.SlickProvider - Creating table, executing: create table \"journal\" (\"ordering\" NUMBER(19) NOT NULL,\"deleted\" CHAR(1) DEFAULT 0 NOT NULL check (\"deleted\" in (0, 1)),\"persistence_id\" VARCHAR2(255) NOT NULL,\"sequence_number\" NUMBER(19) NOT NULL,\"message\" BLOB NOT NULL,\"tags\" VARCHAR2(255)); alter table \"journal\" add constraint \"journal_pk\" primary key(\"ordering\",\"persistence_id\",\"sequence_number\"); create sequence \"journal__ordering_seq\" start with 1 increment by 1; create or replace trigger \"journal__ordering_trg\" before insert on \"journal\" referencing new as new for each row when (new.\"ordering\" is null) begin select \"journal__ordering_seq\".nextval into :new.\"ordering\" from sys.dual; end;\n2017-02-20 17:16:27,245 [info] chirp-impl-application-akka.actor.default-dispatcher-3 - c.l.l.i.p.c.ClusterStartupTaskActor - Cluster start task cassandraOffsetStorePrepare done.\n2017-02-20 17:16:27,337 [error] chirp-impl-application-akka.actor.default-dispatcher-3 - a.a.OneForOneStrategy - Missing IN or OUT parameter at index:: 1\njava.sql.SQLException: Missing IN or OUT parameter at index:: 1\n\tat oracle.jdbc.driver.OraclePreparedStatement.processCompletedBindRow(OraclePreparedStatement.java:2076)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4790)\n\tat oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:4901)\n\tat oracle.jdbc.driver.OracleCallableStatement.execute(OracleCallableStatement.java:5631)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.execute(OraclePreparedStatementWrapper.java:1385)\n\tat com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44)\n\tat com.zaxxer.hikari.pool.HikariProxyCallableStatement.execute(HikariProxyCallableStatement.java)\n\tat com.lightbend.lagom.internal.persistence.jdbc.SlickProvider$$anonfun$com$lightbend$lagom$internal$persistence$jdbc$SlickProvider$$createTableInternal$1$$anonfun$apply$5.apply(SlickProvider.scala:137)\n\tat com.lightbend.lagom.internal.persistence.jdbc.SlickProvider$$anonfun$com$lightbend$lagom$internal$persistence$jdbc$SlickProvider$$createTableInternal$1$$anonfun$apply$5.apply(SlickProvider.scala:136)\n\tat slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70)\n\nThe Slick 3.2.0 OracleProfile class gives a hint about the problem:\n\nOracle throws an exception if you try to refer to a :new column in a trigger statement in a PreparedStatement. Since we need to create these statements for the AutoInc emulation, we execute all DDL statements with a non-prepared Statement.\n\nHowever, Lagom does not take advantage of Slick's workaround, because it extracts the statements generated by Slick and executes them itself here: https://github.com/lagom/lagom/blob/master/persistence-jdbc/core/src/main/scala/com/lightbend/lagom/internal/persistence/jdbc/SlickProvider.scala#L140\nReproducible Test Case\nReproduction is challenging. I used Oracle's official Docker repository to get Oracle Database Standard Edition.\nThis was helpful for getting the JDBC drivers installed: http://stackoverflow.com/a/33883350/29470\nWorkaround\nCreate all tables manually and set lagom.persistence.jdbc.create-tables.auto = false (requires Lagom 1.3.0 or the upcoming 1.2.3 because of #449).",
            "url":"https://github.com/lagom/lagom/issues/506"
          },
          {
            "number":541,
            "title":"Dev mode - pressing Ctrl+D instead of Enter to stop",
            "bodyText":"I find myself constantly stopping dev mode because I want to add some blank lines in the log stream so I hit the Enter key. What do you think about switching to using the Ctrl + D combination that is also used in other tools instead of Enter?",
            "url":"https://github.com/lagom/lagom/issues/541"
          },
          {
            "number":571,
            "title":"Generate Scala.js client code from a Service API",
            "bodyText":"(from the ML)\nGiven a Service API there should be a tool to generate a Scala.js client to interact with that service.",
            "url":"https://github.com/lagom/lagom/issues/571"
          },
          {
            "number":576,
            "title":"Development mode task for cleaning data only",
            "bodyText":"I often find myself wanting to clean data during development.\nsbt clean or mvn clean does the trick, but throws out the baby with the bathwater by forcing everything to recompile.\nA task like sbt cleanData mvn lagom:cleanData would be convenient.",
            "url":"https://github.com/lagom/lagom/issues/576"
          },
          {
            "number":643,
            "title":"Provide built in Seq PathParamSerializers for multi value query parameters",
            "bodyText":"(as reported by @bmatthews68 in lagom/lagom.github.io#60)\nI would like to be able to bind multi-value parameters using pathCall() and restCall() as illustrated by the snippet below:\ntrait UserService extends Service {\n    def lookupByUsername(tenantId: String, username: Seq[String])\n    override final def descriptor = {\n        import Service._\n        named(\"users\").withCalls {\n            restCall(Method.GET, \"/tenants/:tenantId/users?username\", lookupByUsername _)\n        } \n    }\n}\ncurl http://localhost:9000/tenants/abcdef/users?username=ghijkl&username=mnopqr&username=stuvwx",
            "url":"https://github.com/lagom/lagom/issues/643"
          },
          {
            "number":667,
            "title":"Offset tracking grouping",
            "bodyText":"Currently Lagom's APIs that do offset tracking (read side processors and topic publishers) persist the offset of every message. This is not a bad default, especially for read side processors, as it allows exactly once processing. But exactly once processing isn't always necessary, and the performance cost could be high. So we should offer a configuration option to specify grouping, so that, for example, the offset can be configured to only be persisted every 10 events.",
            "url":"https://github.com/lagom/lagom/issues/667"
          },
          {
            "number":668,
            "title":"Add support for Avro serialization and Schema Registry with Kafka",
            "bodyText":"Many Kafka users prefer to use a binary serialization format such as Avro to reduce the size of messages. Avro is not self-describing and requires a schema to deserialize. This schema must exactly match the schema used to serialize the message, making compatibility a concern.\nConfluent provides a Schema Registry service that can be used along with Avro serializers in Kafka to allow for propagation of schemas between different services, as well as transforming messages that were produced with one schema into a different format expected by a consumer.\nConfluent also provides an Avro serializer that can be used with Kafka's Java client. However, this can't be used easily with Lagom (as of 1.3).\nResources\n\nConfluent Platform Docs > Schema Registry\n\"Schema evolution in Avro, Protocol Buffers and Thrift\"\n\"The dog ate my schema\"\n\"Serializing data efficiently with Apache Avro and dealing with a Schema Registry\"\nSchema Registry and REST Proxy (video)\n\nExample code\n\nhttps://github.com/confluentinc/examples/tree/3.2.x/kafka-clients/specific-avro-producer\nhttps://github.com/softwaremill/confluent-playground/tree/master/avro-serialization",
            "url":"https://github.com/lagom/lagom/issues/668"
          },
          {
            "number":871,
            "title":"Allow Compile-time DI in Lagom javadsl",
            "bodyText":"With support for compile-time DI in play 2.6, Lagom's javadsl could support that too.",
            "url":"https://github.com/lagom/lagom/issues/871"
          },
          {
            "number":908,
            "title":"scalaVersion of lagom-internal-meta-* projects is hard coded to 2.11.7",
            "bodyText":"Lagom Version - 1.3.6\nThis is an issue with the Lagom SBT plugin\nAPI : SBT/Scala\nOperating System (Centos 7)\n\nuname -a\nLinux sherpavm 3.10.0-327.10.1.el7.x86_64 #1 SMP Tue Feb 16 17:03:50 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nJDK (Oracle 1.8.0_112, OpenJDK 1.8.x, Azul Zing)\n\njava -version\nopenjdk version \"1.8.0_77\"\nOpenJDK Runtime Environment (build 1.8.0_77-b03)\nOpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode)\n\nLibrary Dependencies\nThis is an issue when using ensime.  The ensime plugin only supports a single version of scala for a project and it's dependencies.  Since the internal-meta projects are dynamic, it doesn't look like you can change the scalaVersion setting in your build.sbt.  Adding ensimeIgnoreScalaMismatch in ThisBuild := true interacts very badly with the dynamic projects.  Enumerating the projects that you want ensime to use on the ensimeConfig command line, without listing the dynamic projects is a workaround, but quite cumbersome.\nI'll be happy to submit a PR if we can agree that this should be addressed, and where to get the scalaVersion for the dynamic projects.\nExpected Behavior\nPlease describe the expected behavior of the issue, starting from the first action.\n\nscalaVersion in ThisBuild := \"2.11.8\"\nshow scalaVersion  - should displays 2.11.8 for all projects defined in build.sbt\nshow show lagom-internal-meta-project-cassandra/scalaVersion - should display 2.11.8\n\nActual Behavior\nPlease provide a description of what actually happens, working from the same starting point.\nBe descriptive: \"it doesn't work\" does not describe what the behavior actually is -- instead, say \"the page renders a 500 error code with no body content.\"  Copy and paste logs, and include any URLs.\n\nscalaVersion in ThisBuild := \"2.11.8\"\nshow scalaVersion  - should displays 2.11.8 for all projects defined in build.sbt\n\n\nshow scalaVersion\n[info]  itemImpl/:scalaVersion\n[info] \t2.11.8\n[info] searchImpl/:scalaVersion\n[info] \t2.11.8\n[info] searchApi/:scalaVersion\n[info] \t2.11.8\n[info] userApi/:scalaVersion\n[info] \t2.11.8\n[info] biddingImpl/:scalaVersion\n[info] \t2.11.8\n[info] webGateway/:scalaVersion\n[info] \t2.11.8\n[info] biddingApi/:scalaVersion\n[info] \t2.11.8\n[info] itemApi/:scalaVersion\n[info] \t2.11.8\n[info] userImpl/:scalaVersion\n[info] \t2.11.8\n[info] root/:scalaVersion\n[info] \t2.11.8\n\n\nshow show lagom-internal-meta-project-cassandra/scalaVersion\n\n\nshow lagom-internal-meta-project-cassandra/scalaVersion\n[info] 2.11.7\n\nFrom LagomPlugin.scala:\n  private val kafkaServerProject = Project(\"lagom-internal-meta-project-kafka\", file(\".\"),\n    configurations = Configurations.default,\n    settings = CorePlugin.projectSettings ++ IvyPlugin.projectSettings ++ JvmPlugin.projectSettings ++ Seq(\n    scalaVersion := \"2.11.7\",\n    libraryDependencies += LagomImport.component(\"lagom-kafka-server\"),\n    lagomKafkaStart in ThisBuild := startKafkaServerTask.value,\n    lagomKafkaStop in ThisBuild := Servers.KafkaServer.tryStop(new SbtLoggerProxy(state.value.log))\n  ))\n`\n\n### Reproducible Test Case\n\nAn unchanged https://github.com/lagom/online-auction-scala demonstrates this.  I believe that any lagom project can reproduce, given the hard-coded version numbers in LagomPlugin.scala.",
            "url":"https://github.com/lagom/lagom/issues/908"
          },
          {
            "number":919,
            "title":"Review PersistentEntity API for reply and afterPersist",
            "bodyText":"This issue is a starting point for a broader discussion we would like to initiate about the current state of PersistentEntity API.\nThis was initially triggered by PR #901. The main problem reported there is that it's very cumbersome to write command handlers that reply with the entity state while emitting many or none events.\nFor illustration:\n(examples in Java syntax)\nfinal List<Event> createdEvents = generateEvents();\nif (createdEvents.size() == 0) {\n  ctx.reply(state().getModel());\n  return ctx.done();\n} else {\n   return ctx.thenPersistAll(\n                   createdEvents, \n                    () -> {  ctx.reply(state().getModel()); }\n              );\n}\nIn its current form, the API offers no easy way to deal with the fact that events may not be emitted.\nThe ctx.thenPersistAll method receive an empty lists of events, but the afterPersist method is not called if there are zero events persisted. Which make sense, because it's called afterPersist.\nThe mechanism is based on aContext that is passed to the user. The Context brings to user space (without exposing it) the actor.sender(). In the case of reply, behind the scene the reply calls sender() ! someMessage.\nNote that we are basically piggybacking the afterPersist method to call indirectly the actor.sender() method.\nFluent API Proposal\nWe could achieve exactly the same without relying on a context and without mixing side-effect function (afterPersist) with reply functions.\nFor instance...\nPersistAll(generateEvents())\n  .replyWith( state -> state.getVersion(); );\nThe above example replies by returning the version of model. The state is the model after applying the generated events. If generatedEvents return a empty list, state is not updated.\nNote that this API removes completely the need for a Context. We just register a function State -> A that is called to transform the current state and generate the data that will be sent back to the sender.\nAnother variation could be:\nPersistAll(generateEvents())\n  .replyWith( (state, events) -> state.getVersion(); );\nIn that case, the user receives the current updated state and the generated events.\nA similar fluent API can be implement for afterPersist.\nPersistAll(generateEvents())\n  .afterPersist( events -> // do some side-effect here ) // <- symmetric api, see below\n  .replyWith( state -> state.getVersion(); );\nafterPersist callback is only called if events.nonEmpty and it returns void or Unit (scala).\nPersistOne / PersistAll asymmetry\nThe afterPersist callbacks for PersistOne and PersistAll are not symmetric.\nPersistOne has Event -> void and PersistAll has () -> void.\nThe reason for that comes from akka-persistence itself where the handler for persistAll has the same shape (Event -> void) as the handler for persist. This is probably for historical reasons, but as a consequence the Lagom API exposed it as Event -> void and () -> void.\nSome users have complained on Gitter about it. There is a easy workaround which is to have the generated events in scope when defining the command handler. However, we can easily improve the user experience by offering a symmetric API.",
            "url":"https://github.com/lagom/lagom/issues/919"
          },
          {
            "number":941,
            "title":"Add a check for slow getLocalHost() to the development enviroment",
            "bodyText":"Many Java users on macOS are affected by this bug: https://thoeni.io/post/macos-sierra-java/\nIt can cause slow startup and flaky behavior in the Lagom development environment.\nWe could add a check to the sbt/Maven plugins that prints a warning and links to that blog post if java.net.InetAddress.getLocalHost() takes more than 100ms.",
            "url":"https://github.com/lagom/lagom/issues/941"
          },
          {
            "number":996,
            "title":"Document ReadSide.register in the Cassandra/RDBMS read-side documentation",
            "bodyText":"The documentation on persistent read-sides has a section on registering read-side processors using ReadSide.register:\nhttps://www.lagomframework.com/documentation/1.3.x/java/ReadSide.html#Registering-your-read-side-processor\nThe documentation for CassandraReadSide, JdbcReadSide, etc. do not mention this, so someone reading one of those pages without also referring to the overview page might not know that these also need to be registered.",
            "url":"https://github.com/lagom/lagom/issues/996"
          },
          {
            "number":1001,
            "title":"Support Apache Ignite as a persistence back end",
            "bodyText":"https://ignite.apache.org/\nIgnite is a distributed in-memory database with SQL support.",
            "url":"https://github.com/lagom/lagom/issues/1001"
          }
        ]
      }
    }
  }
}