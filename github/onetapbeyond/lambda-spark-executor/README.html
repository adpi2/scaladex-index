<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <p><a href="https://gitpitch.com/onetapbeyond/lambda-spark-executor/master" target="_blank"><img src="https://camo.githubusercontent.com/b7570c72831d2046e0ef64d4a103aac057eb67a1/68747470733a2f2f67697470697463682e636f6d2f6173736574732f62616467652e737667" alt="GitPitch" data-canonical-src="https://gitpitch.com/assets/badge.svg" style="max-width:100%;"></a></p> 
  <p>#Apache Spark AWS Lambda Executor (SAMBA)</p> 
  <p>SAMBA is an <a href="http://spark.apache.org/" target="_blank">Apache Spark</a> package offering seamless integration with the <a href="https://aws.amazon.com/lambda/" target="_blank">AWS Lambda</a> compute service for Spark batch and streaming applications on the JVM. This library is built on top of the <a href="https://github.com/onetapbeyond/aws-gateway-executor" target="_blank">aws-gateway-executor</a> library, a lightweight Java client library for calling APIs exposed by the <a href="https://aws.amazon.com/api-gateway/" target="_blank">AWS API Gateway</a>.</p> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-motivation" aria-hidden="true" class="anchor" id="user-content-samba-motivation" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Motivation</h3> 
  <p>Within traditional Spark deployments RDD tasks are executed using fixed compute resources on worker nodes within the Spark cluster. With SAMBA, application developers can delegate selected RDD tasks to execute using on-demand AWS Lambda compute infrastructure in the cloud.</p> 
  <p>More generally, SAMBA also provides a simple yet powerful mechanism for Spark applications to integrate with general REST services exposed on the AWS API Gateway.</p> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-examples" aria-hidden="true" class="anchor" id="user-content-samba-examples" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Examples</h3> 
  <p>A number of example applications are provided to demonstrate the use of the SAMBA library to deliver RDD task delegation and REST integration.</p> 
  <ul> 
   <li>RDD Task Delegation [ <a href="https://github.com/onetapbeyond/lambda-spark-executor/blob/master/examples/scala/task-delegation" target="_blank">Scala</a> ][ <a href="https://github.com/onetapbeyond/lambda-spark-executor/blob/master/examples/java/task-delegation" target="_blank">Java</a> ]</li> 
   <li>REST Integration [ <a href="https://github.com/onetapbeyond/lambda-spark-executor/blob/master/examples/scala/rest-integration" target="_blank">Scala</a> ][ <a href="https://github.com/onetapbeyond/lambda-spark-executor/blob/master/examples/java/rest-integration" target="_blank">Java</a> ]</li> 
  </ul> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-sbt-dependency" aria-hidden="true" class="anchor" id="user-content-samba-sbt-dependency" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA SBT Dependency</h3> 
  <pre><code>libraryDependencies += "io.onetapbeyond" %% "lambda-spark-executor_2.10" % "1.0"
</code></pre> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-gradle-dependency" aria-hidden="true" class="anchor" id="user-content-samba-gradle-dependency" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Gradle Dependency</h3> 
  <pre><code>compile 'io.onetapbeyond:lambda-spark-executor_2.10:1.0'
</code></pre> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-spark-package-dependency" aria-hidden="true" class="anchor" id="user-content-samba-spark-package-dependency" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Spark Package Dependency</h3> 
  <p>Include the SAMBA package in your Spark application using spark-shell, or spark-submit. For example:</p> 
  <pre><code>$SPARK_HOME/bin/spark-shell --packages io.onetapbeyond:lambda-spark-executor_2.10:1.0
</code></pre> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-basic-usage" aria-hidden="true" class="anchor" id="user-content-samba-basic-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Basic Usage</h3> 
  <p>This library exposes a new <code>delegate</code> transformation on Spark RDDs of type <code>RDD[AWSTask]</code>. The following sections demonstrate how to use this new RDD operation to leverage <code>AWS Lambda</code> compute services directly within Spark batch and streaming applications on the JVM.</p> 
  <p>See the <a href="https://github.com/onetapbeyond/aws-gateway-executor" target="_blank">documentation</a> on the underlying <code>aws-gateway-executor</code> library for details on building <code>AWSTask</code> and handling <code>AWSResult</code>.</p> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-spark-batch-usage" aria-hidden="true" class="anchor" id="user-content-samba-spark-batch-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Spark Batch Usage</h3> 
  <p>For this example we assume an input <code>dataRDD</code>, then transform it to generate an RDD of type <code>RDD[AWSTask]</code>. In this example each <code>AWSTask</code> will invoke a sample AWS Lambda <code>score</code> computation when the RDD is eventually evaluated.</p> 
  <pre><code>import io.onetapbeyond.lambda.spark.executor.Gateway._
import io.onetapbeyond.aws.gateway.executor._

val aTaskRDD = dataRDD.map(data =&gt; {
  AWS.Task(gateway)
     .resource("/score")
     .input(data.asInput())
     .post()
  })
</code></pre> 
  <p>The set of <code>AWSTask</code> within <code>aTaskRDD</code> can be scheduled for processing on AWS Lambda infrastructure by calling the new <code>delegate</code> operation provided by SAMBA on the RDD:</p> 
  <pre><code>val aResultRDD = aTaskRDD.delegate
</code></pre> 
  <p>When <code>aTaskRDD.delegate</code> is evaluated by Spark the resultant <code>aResultRDD</code> is of type <code>RDD[AWSResult]</code>. The result returned by the sample <code>score</code> computation on the original <code>AWSTask</code> is available within these <code>AWSResult</code>. These values can be optionally cached, further processed or persisted per the needs of your Spark application.</p> 
  <p>Note, the use here of the AWS Lambda <code>score</code> function is simply representative of any computation made available by the AWS API Gateway.</p> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#samba-spark-streaming-usage" aria-hidden="true" class="anchor" id="user-content-samba-spark-streaming-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SAMBA Spark Streaming Usage</h3> 
  <p>For this example we assume an input stream <code>dataStream</code>, then transform it to generate a new stream with underlying RDDs of type <code>RDD[AWSTask]</code>. In this example each <code>AWSTask</code> will execute a sample AWS Lambda <code>score</code> computation when the stream is eventually evaluated.</p> 
  <pre><code>import io.onetapbeyond.lambda.spark.executor.Gateway._
import io.onetapbeyond.aws.gateway.executor._

val aTaskStream = dataStream.transform(rdd =&gt; {
  rdd.map(data =&gt; {
    AWS.Task(gateway)
       .resource("/score")
       .input(data.asInput())
       .post()
    })	
  })
</code></pre> 
  <p>The set of <code>AWSTask</code> within <code>aTaskRDD</code> can be scheduled for processing on AWS Lambda infrastructure by calling the new <code>delegate</code> operation provided by SAMBA on each RDD within the stream:</p> 
  <pre><code>val aResultStream = aTaskStream.transform(rdd =&gt; rdd.delegate)
</code></pre> 
  <p>When <code>aTaskStream.transform</code> is evaluated by Spark the resultant <code>aResultStream</code> has underlying RDDs of type <code>RDD[AWSResult]</code>. The result returned by the sample <code>score</code> computation on the original <code>AWSTask</code> is available within these <code>AWSResult</code>. These values can be optionally cached, further processed or persisted per the needs of your Spark application.</p> 
  <p>Note, the use here of the AWS Lambda <code>score</code> function is simply representative of any computation made available by the AWS API Gateway.</p> 
  <p>###SAMBA Spark Deployments</p> 
  <p>To understand how SAMBA delivers the computing power of AWS Lambda to Spark applications on the JVM the following sections compare and constrast the deployment of traditional Scala, Java, and Python applications with Spark applications powered by the SAMBA library.</p> 
  <p>####1. Traditional Scala | Java | Python Spark Application Deployment</p> 
  <p><a href="https://camo.githubusercontent.com/b3a69c0e7d452ffbbc0c8e6e9b7f5c7d1c5cd64e/68747470733a2f2f6f6e657461706265796f6e642e6769746875622e696f2f7265736f757263652f696d672f73616d62612f747261642d737061726b2d6465706c6f792e6a7067" target="_blank"><img src="https://camo.githubusercontent.com/b3a69c0e7d452ffbbc0c8e6e9b7f5c7d1c5cd64e/68747470733a2f2f6f6e657461706265796f6e642e6769746875622e696f2f7265736f757263652f696d672f73616d62612f747261642d737061726b2d6465706c6f792e6a7067" alt="Traditional Deployment: Spark" data-canonical-src="https://onetapbeyond.github.io/resource/img/samba/trad-spark-deploy.jpg" style="max-width:100%;"></a></p> 
  <p>Without SAMBA library support, traditional task computations execute locally on worker nodes within the cluster, sharing and therefore possibly constrained by, the resources available on each worker node.</p> 
  <p>####2. Scala | Java + AWS Lambda (SAMBA) Spark Application Deployment</p> 
  <p><a href="https://camo.githubusercontent.com/9099b2d745a9b5ed0ad63ea0279df51ee0e05566/68747470733a2f2f6f6e657461706265796f6e642e6769746875622e696f2f7265736f757263652f696d672f73616d62612f6e65772d73616d62612d6465706c6f792e6a7067" target="_blank"><img src="https://camo.githubusercontent.com/9099b2d745a9b5ed0ad63ea0279df51ee0e05566/68747470733a2f2f6f6e657461706265796f6e642e6769746875622e696f2f7265736f757263652f696d672f73616d62612f6e65772d73616d62612d6465706c6f792e6a7067" alt="New Deployment: lambda-spark-executor" data-canonical-src="https://onetapbeyond.github.io/resource/img/samba/new-samba-deploy.jpg" style="max-width:100%;"></a></p> 
  <p>SAMBA powered Spark applications benefit from the following enhancements:</p> 
  <ol> 
   <li> <p>RDD tasks can be delegated to execute on <a href="https://aws.amazon.com/lambda/" target="_blank">AWS Lambda</a> compute infrastructure in the cloud.</p> </li> 
   <li> <p>Driver programs can easily integrate with general REST services exposed on the <a href="https://aws.amazon.com/api-gateway/" target="_blank">AWS API Gateway</a>.</p> </li> 
  </ol> 
  <p>For certain application workflows and workloads Spark task delegation to <code>AWS Lambda</code> compute infrastructure will make sense. However, it is worth noting that this library is provided to <code>extend-not-replace</code> the traditional Spark computation model, so I recommend using SAMBA judiciously.</p> 
  <h3><a href="https://github.com/onetapbeyond/lambda-spark-executor#license" aria-hidden="true" class="anchor" id="user-content-license" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>License</h3> 
  <p>See the <a href="https://github.com/onetapbeyond/lambda-spark-executor/blob/master/LICENSE" target="_blank">LICENSE</a> file for license rights and limitations (Apache License 2.0).</p> 
 </article>
</div>