{
  "data":{
    "repository":{
      "issues":{
        "nodes":[
          {
            "number":166,
            "title":"Timeout issues with `sync=true` jobs",
            "bodyText":"This is timeout issue for sync=true jobs of about >40 seconds. It doesn't return any SJS ERROR response, but instead generic spray message:\n\nThe server was not able to produce a timely response to your request.\n\nProbably, it's a better idea to run such longer jobs async, but controlling this timeout and allowing sync jobs to run longer than 40sec would be very useful! If that's not possible, getting at least some response back would be cool - to track the timeout error. Now I don't see an immediate way to catch what happened...\nComment:\nIn the meanwhile, is there any relatively quick workaround for this?\nI've tried to add this config to SJS deployed ,conf file:\nspark {\n   ...\n   jobserver {\n    ...\n      spray.can.server {\n         idle-timeout = 1000s\n         request-timeout = 1000s\n      }\n   }\n}\n\nbut it didn't seem to have any effect. Is it correct place and should such settings be picked up by a server?\nAlso, according to this one can set the spray.io.ConnectionTimeouts.SetIdleTimeout and spray.http.SetRequestTimeout. Would it be easy to set those in the SJS code - if yes, could you please point the right direction?\nAnother thought - maybe, in the event of timeout (or any other issue with sync=true submissions), it's possible to return meaningful response - such as sync-related issue error message and JobID - this way the user at least immediately has the job for querying the status/results. Is there a way to return JobID no matter what - even if everything fails?",
            "url":"https://github.com/spark-jobserver/spark-jobserver/issues/166"
          },
          {
            "number":296,
            "title":"Unable to get the Spark job standard output",
            "bodyText":"Consider a spark job, started from a java application, that collects the RDD data and consider that in the application data, you want to System.out the collect result, there is currently no way to retrieve the standard output through the Spark Job Server API.\nAs an analogy, when you submit a Map/Reduce job through the WebHCat REST Api, a stdout and a stderr file are created on the edge node (where WebHCat is running). You could consider the same thing for the Spark job server.",
            "url":"https://github.com/spark-jobserver/spark-jobserver/issues/296"
          },
          {
            "number":308,
            "title":"Add example of Caching Dataframe in SparkSqlJob",
            "bodyText":"Is there an example of how to share a cached Dataframe between jobs using SparkSqlJob?\nhttps://gist.github.com/fadeddata/ad8c55f286bd5bf4eeba\nIs this even possible?\nI tried to convert a Dataframe to an RDD and used NamedRdd to cache it but that ended up with a job that never completed.",
            "url":"https://github.com/spark-jobserver/spark-jobserver/issues/308"
          },
          {
            "number":351,
            "title":"Get context information with GET /contexts/<context_id>",
            "bodyText":"Hi team,\nI was looking for the created time with contexts call. Currently we display only the context names. Can we add context created time as-well (or this already exist).\ncurl  '1.1.1.1:8090/contexts/'\n[abc,efg]",
            "url":"https://github.com/spark-jobserver/spark-jobserver/issues/351"
          }
        ]
      }
    }
  }
}