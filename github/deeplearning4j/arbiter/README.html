<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-arbiter" class="anchor" href="https://github.com/deeplearning4j/arbiter#arbiter" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Arbiter</h1> 
  <p>A tool dedicated to evaluating and tuning machine learning models. Part of the DL4J Suite of Machine Learning / Deep Learning tools for the enterprise.</p> 
  <h2><a id="user-content-modules" class="anchor" href="https://github.com/deeplearning4j/arbiter#modules" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Modules</h2> 
  <p>Arbiter contains the following modules:</p> 
  <ul> 
   <li>arbiter-core: Defines the API and core functionality, and also contains functionality for the Arbiter UI</li> 
   <li>arbiter-deeplearning4j: For hyperparameter optimization of DL4J models (MultiLayerNetwork and ComputationGraph networks)</li> 
  </ul> 
  <h2><a id="user-content-hyperparameter-optimization-functionality" class="anchor" href="https://github.com/deeplearning4j/arbiter#hyperparameter-optimization-functionality" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Hyperparameter Optimization Functionality</h2> 
  <p>The open-source version of Arbiter currently defines two methods of hyperparameter optimization:</p> 
  <ul> 
   <li>Grid search</li> 
   <li>Random search</li> 
  </ul> 
  <p>For optimization of complex models such as neural networks (those with more than a few hyperparameters), random search is superior to grid search, though Bayesian hyperparameter optimization schemes For a comparison of random and grid search methods, see <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank">Random Search for Hyper-parameter Optimization (Bergstra and Bengio, 2012)</a>.</p> 
  <h3><a id="user-content-core-concepts-and-classes-in-arbiter-for-hyperparameter-optimization" class="anchor" href="https://github.com/deeplearning4j/arbiter#core-concepts-and-classes-in-arbiter-for-hyperparameter-optimization" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Core Concepts and Classes in Arbiter for Hyperparameter Optimization</h3> 
  <p>In order to conduct hyperparameter optimization in Arbiter, it is necessary for the user to understand and define the following:</p> 
  <ul> 
   <li><strong>Parameter Space</strong>: A <code>ParameterSpace&lt;P&gt;</code> specifies the type and allowable values of hyperparameters for a model configuration of type <code>P</code>. For example, <code>P</code> could be a MultiLayerConfiguration for DL4J</li> 
   <li><strong>Candidate Generator</strong>: A <code>CandidateGenerator&lt;C&gt;</code> is used to generate candidate models configurations of some type <code>C</code>. The following implementations are defined in arbiter-core: 
    <ul> 
     <li><code>RandomSearchCandidateGenerator</code></li> 
     <li><code>GridSearchCandidateGenerator</code></li> 
    </ul></li> 
   <li><strong>Score Function</strong>: A <code>ScoreFunction&lt;M,D&gt;</code> is used to score a model of type <code>M</code> given data of type <code>D</code>. For example, in DL4J a score function might be used to calculate the classification accuracy from a DataSetIterator 
    <ul> 
     <li>A key concept here is that they score is a single numerical (double precision) value that we either want to minimize or maximize - this is the goal of hyperparameter optimization</li> 
    </ul></li> 
   <li><strong>Termination Conditions</strong>: One or more <code>TerminationCondition</code> instances must be provided to the <code>OptimizationConfiguration</code>. <code>TerminationCondition</code> instances are used to control when hyperparameter optimization should be stopped. Some built-in termination conditions: 
    <ul> 
     <li><code>MaxCandidatesCondition</code>: Terminate if more than the specified number of candidate hyperparameter configurations have been executed</li> 
     <li><code>MaxTimeCondition</code>: Terminate after a specified amount of time has elapsed since starting the optimization</li> 
    </ul></li> 
   <li><strong>Result Saver</strong>: The <code>ResultSaver&lt;C,M,A&gt;</code> interface is used to specify how the results of each hyperparameter optimization run should be saved. For example, whether saving should be done to local disk, to a database, to HDFS, or simply stored in memory. 
    <ul> 
     <li>Note that <code>ResultSaver.saveModel</code> method returns a <code>ResultReference</code> object, which provides a mechanism for re-loading both the model and score from wherever it may be saved.</li> 
    </ul></li> 
   <li><strong>Optimization Configuration</strong>: An <code>OptimizationConfiguration&lt;C,M,D,A&gt;</code> ties together the above configuration options in a fluent (builder) pattern.</li> 
   <li><strong>Candidate Executor</strong>: The <code>CandidateExecutor&lt;C,M,D,A&gt;</code> interface provides a layer of abstraction between the configuration and execution of each instance of learning. For example, the <code>LocalCandidateExecutor</code> can be used to execute learning on a single machine (in the current JVM), whereas <code>SparkCandidateExecutor</code> can be used for executing models on Spark. In principle, other execution methods (for example, on AWS) could be implemented.</li> 
   <li><strong>Optimization Runner</strong>: The <code>OptimizationRunner</code> uses an <code>OptimizationConfiguration</code> and a <code>CandidateExecutor</code> to actually run the optimization, and save the results.</li> 
  </ul> 
  <h3><a id="user-content-optimization-of-deeplearning4j-models" class="anchor" href="https://github.com/deeplearning4j/arbiter#optimization-of-deeplearning4j-models" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Optimization of DeepLearning4J Models</h3> 
  <p>(This section: forthcoming)</p> 
 </article>
</div>