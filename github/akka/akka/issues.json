{
  "data":{
    "repository":{
      "issues":{
        "nodes":[
          {
            "number":18515,
            "title":"Consider using Resume directive on DelimiterFraming to drop oversized lines",
            "bodyText":"I.e. if supervision with Resume is used, just start dropping incoming bytes from the oversized line until new delimiter is found.",
            "url":"https://github.com/akka/akka/issues/18515"
          },
          {
            "number":18590,
            "title":"Allow wildcards in DistributedPubSubMediator",
            "bodyText":"Hi,\nNot sure if such proposal will find support here, but it would be nice to have (at least basic) wildcard support in DistributedPubSubMediator, similar to what is provided by actorSelection. Consider following simple modification:\ndef publish(path: String, msg: Any, allButSelf: Boolean = false): Unit = {\n  val pattern = Helpers.makePattern(path)\n  for {\n    (address, bucket) <- registry\n    if !(allButSelf && address == selfAddress) // if we should skip sender() node and current address == self address => skip\n    valueHolder <- bucket.content.filterKeys(k => pattern.matcher(k).matches).values\n    ref <- valueHolder.ref\n  } ref forward msg\n}\nThis would allow to do something like:\nmediator ! SendToAll(\"/user/parent/foo:bar:*\", \"Hello World\", false)\n\nI'm new to Akka/Scala and not sure if I can provide a production-quality pull request, but at least I'm showing the basic idea and a straightforward implementation. I hope similar functionality could be included in Akka, to make it even more powerful ☺️",
            "url":"https://github.com/akka/akka/issues/18590"
          },
          {
            "number":18738,
            "title":"Missing ability to convert akka.protobuf.ByteString into akka.util.ByteString",
            "bodyText":"We're implementing an akka persistence query plugin. It needs to look inside stored events that are serialized using akka's built-in serialization, in this case the protobuf representation of akka.persistence.serialization.MessageFormats.PersistentMessage. For ease of use, we're re-using that class to unmarshal the persistent events.\nPersistentMessage.payload is an instance of akka.protobuf.ByteString, which contains the serialized event itself. The contents depends on how the user has configured akka-serialization: it might be java serialization, a custom probuf version, or something else entirely.\nOne of the use cases of our plugin is to allow remote views for events. To that end, we actually want to expose the contents of the events directly. Hence, we don't need to read the contents; we just want to serve it up to HTTP (as chunks).\nHowever, akka-http expects akka.util.ByteString, not akka.protobuf.ByteString. Even worse is that with current APIs, it's not possible to convert one into the other without copying the bytes. It would be very convenient to be allowed to coerce protobuf.ByteString into util.ByteString, while re-using the underlying byte array / byte buffer.",
            "url":"https://github.com/akka/akka/issues/18738"
          },
          {
            "number":18802,
            "title":"Docs: add section to the *Integration* part of reference doc about OutputStreamSource and InputStreamSink ",
            "bodyText":"per: #18707 (comment)\nbased on #18707",
            "url":"https://github.com/akka/akka/issues/18802"
          },
          {
            "number":18813,
            "title":"Guide Maven users how to configure concat for reference.conf instead of replace, in maven assembly",
            "bodyText":"(Same applies to Gradle).\nPeople doing fatjars, coming from java-land, end up sometimes getting config blow up since the default strategy of \"2 names with the same name\" is \"replace\" in those tools (Maven, Gradle), which does not make sense for reference.conf.",
            "url":"https://github.com/akka/akka/issues/18813"
          },
          {
            "number":18846,
            "title":"Akka remoting does not work with Akka OSGi unless I copy its reference.conf file",
            "bodyText":"I am trying to use Akka remoting with Akka OSGi.\nExample project\nhttps://github.com/PhilAndrew/sbt-osgi-felix-akka-blueprint-camel/tree/136f8af2a40eea8ce2c8aae066cbb08f94559c19\nThe example project can be used to test this problem.\nThis does not work, it says:\nprint components will not participate in quiesce operations\nERROR: Bundle default.sbt.osgi.felix.akka.blueprint.camel [21] Error starting reference:file:/D:/home/projects/sbt-osgi-\nfelix-akka-blueprint-camel/target/scala-2.11/classes/ (org.osgi.framework.BundleException: Activator start error in bund\nle default.sbt.osgi.felix.akka.blueprint.camel [21].)\ncom.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka.remote.log-received-messages'\n        at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152)\n        at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170)\n        at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:176)\n        at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:176)\n        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184)\n        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)\n        at com.typesafe.config.impl.SimpleConfig.getBoolean(SimpleConfig.java:214)\n        at akka.remote.RemoteSettings.<init>(RemoteSettings.scala:24)\n        at akka.remote.RemoteActorRefProvider.<init>(RemoteActorRefProvider.scala:112)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:78)\n        at scala.util.Try$.apply(Try.scala:192)\n        at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:73)\n        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84)\n        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84)\n        at scala.util.Success.flatMap(Try.scala:231)\n        at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:84)\n        at akka.actor.ActorSystemImpl.liftedTree1$1(ActorSystem.scala:626)\n        at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:619)\n        at akka.actor.ActorSystem$.apply(ActorSystem.scala:143)\n        at akka.actor.ActorSystem$.apply(ActorSystem.scala:127)\n        at akka.osgi.OsgiActorSystemFactory.createActorSystem(OsgiActorSystemFactory.scala:33)\n        at akka.osgi.ActorSystemActivator.start(ActorSystemActivator.scala:42)\n        at org.apache.felix.framework.util.SecureAction$Actions.run(SecureAction.java:1709)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at org.apache.felix.framework.util.SecureAction.startActivator(SecureAction.java:688)\n        at org.apache.felix.framework.Felix.activateBundle(Felix.java:2220)\n        at org.apache.felix.framework.Felix.startBundle(Felix.java:2138)\n        at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1365)\n        at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)\n        at java.lang.Thread.run(Thread.java:745)\nTerminate batch job (Y/N)?\n\nI believe it should work, if I want it to work I have to copy the reference.conf file out of this Akka Remoting jar and put it in resources folder, that is a workaround for the problem, I should not have to do a workaround like this for Akka Remoting in OSGi to work.",
            "url":"https://github.com/akka/akka/issues/18846"
          },
          {
            "number":18950,
            "title":"warnOldDocs not working for RP docs",
            "bodyText":"See: http://doc.akka.io/docs/akka/rp-15v09p01/",
            "url":"https://github.com/akka/akka/issues/18950"
          },
          {
            "number":18955,
            "title":"Sharidng is using only latest snapshot",
            "bodyText":"Sharding is using a snaphots, so for a very dynamic sharding configuration we have a lot of snaphosts, where for a recovery only a latest one will be used.\nI think that after successful snapshot creation shard should delete the previous one.\nOr even better, we should have a common solution that after successful snapshot all the previous journaled messages and snaphosts should be deleted",
            "url":"https://github.com/akka/akka/issues/18955"
          },
          {
            "number":18971,
            "title":"Need More Deployment Options",
            "bodyText":"Since 2.4.0 deprecates the Microkernel, there are only two deployment options outlined in the documentation now. One is to use the SBT Native plugin and the other is to use a commercial Typesafe offering.\nCan you please cover more options such as packaging with Maven as an uber jar?",
            "url":"https://github.com/akka/akka/issues/18971"
          },
          {
            "number":19045,
            "title":"Create varargs versions of all fan-in/out fluent combinators in FlowOps",
            "bodyText":"mergeAll\n concatAll\n interleaveAll\n alsoToAll",
            "url":"https://github.com/akka/akka/issues/19045"
          },
          {
            "number":19144,
            "title":".balance defined n Flow/Source, to return a SubFlow",
            "bodyText":"ktoso 11:06\nwhat's the plan about Balance being defined on Flow/Source - we don't? // cc @rkuhn @drewhk\nfound myself that it would look nice for demoing \"look, it's parallel now\"\nrkuhn 11:06\n.balance should actually give you a SubFlow so that we can materialize that a number of times to exploit parallelism while losing ordering\nand if it takes a (strict) integer argument for the parallelism then we can actually desugar that into static > junctions",
            "url":"https://github.com/akka/akka/issues/19144"
          },
          {
            "number":19191,
            "title":"ShardRegion - use counter instead of calculating size using foldLeft",
            "bodyText":"As mentioned in #19154 - instead of using totalBufferSize calculated by foldLefting shardBuffers everytime it is possible to add simple integer counter to ShardRegion actor to improve performance.",
            "url":"https://github.com/akka/akka/issues/19191"
          },
          {
            "number":19227,
            "title":"Consider adding an extra request(1) in expectComplete on the auto probe",
            "bodyText":"To avoid accidentally relying on early completion in tests. If someone wants to test the actual early completion behavior, then manualProbe is still available.",
            "url":"https://github.com/akka/akka/issues/19227"
          },
          {
            "number":19234,
            "title":"Make assertAllStagesStopped do a deadlock dump on remaining actors",
            "bodyText":"It should ping child interpreters to dump their state. Would be nice to have some logic there that serializes this so dumps are not being written overlapped.",
            "url":"https://github.com/akka/akka/issues/19234"
          },
          {
            "number":19253,
            "title":"Generalize Split strategies",
            "bodyText":"Split currently only supports one of SplitBefore and SplitAfter where all elements ultimately land in one of the to substreams.\nA combination of both could emit more than two substreams where elements outside of these start and end markers are dropped.\nThis could render StreamUtils.sliceBytesTransformer redundant which is currently used in RangeDirectives.",
            "url":"https://github.com/akka/akka/issues/19253"
          },
          {
            "number":19328,
            "title":"add persistence query methods that start \u201cnow\u201d",
            "bodyText":"For cases where the client is interested in live data we\u2019d need to either provide a way to obtain the current offset and start from there, or we allow the specification of \u201cnow\u201d as a starting offset for the query directly\u2014the latter could be done by using Long.MaxValue as a special value.",
            "url":"https://github.com/akka/akka/issues/19328"
          },
          {
            "number":19511,
            "title":"Support TLS session key logging",
            "bodyText":"http://jsslkeylog.sourceforge.net provides a means for Wireshark to tap SSL connections, but it's not maintained and not very reliable under JDK 1.8.\nIf Akka-HTTP is doing it's own TLS, could it generate the file that jSSLKeyLog generates with a lot less hackery?\nRef #16830",
            "url":"https://github.com/akka/akka/issues/19511"
          },
          {
            "number":19562,
            "title":"exponential backoff for AtLeastOnce redelivery",
            "bodyText":"currently it only have redeliverInterval,but with redeliverIntervalFactor ,we could keep the behavior the same as currently by set it default as 1,but we could reduce the overhead when there is a suddenly spike.\nso if we have the RedeliverInterval = 1s and a redeliverIntervalFactor = 2,then we have 1s,2s,4s...etc",
            "url":"https://github.com/akka/akka/issues/19562"
          },
          {
            "number":19733,
            "title":"allowLocalRoutees is not documented explicitly in reference docs",
            "bodyText":"Got some feedback that it's not documented well enough in the reference docs.",
            "url":"https://github.com/akka/akka/issues/19733"
          },
          {
            "number":19752,
            "title":"Fold inHandlers, outHandlers into GraphLogic to avoid redundat object creation",
            "bodyText":"Example can be simple stage like LastOptionStage where GraphStageLogic can have onPush, onUpstreamFinish methods and then handler can be added like: setHandler(in,this).\nBase on @rkuhn comment: https://github.com/akka/akka/pull/19634/files#r52571768",
            "url":"https://github.com/akka/akka/issues/19752"
          },
          {
            "number":19771,
            "title":"UdpListener crashes when sending a message fails",
            "bodyText":"Currently akka.io.UdpListener seems to crash if one tries to send a packet to an unreachable target address (interesting: There is a mechanism to reply CommandFailed when NonFatal exceptions happen during a sent, but it only kicks in if the target address had to be resolved by Dns.resolve).\nThe following happens when I try to Udp.Send against an address that is unreachable from the address of the bound socket.\n19:57:44.236 [DEBUG] [akka://IceAgentSpec/system/IO-UDP-FF/selectors/$a/0] akka.io.SelectionHandler - Network is unreachable\n19:57:44.238 [DEBUG] [akka://IceAgentSpec/system/IO-UDP-FF/selectors/$a/0] akka.io.UdpListener - Closing DatagramChannel after being stopped\n\nFrom a user perspective I'd rather expect to receive a Udp.CommandFailed message as a reply to my Udp.Send rather than a shutdown of the whole UdpListener.\nThere could be several reasons why a packet could not be sent:\n\nSocketException(\"Network is unreachable\") - this does currently crash the actor\nSocketException(\"Invalid argument\") - happens if we try to send to the target port 0, but probably could have other reasons. This does currently crash the actor\nTarget hostname could not be resolved - this case is currently ignored. The actor stays alive but the user does receive no CommandFailed or Ack.\nProbably a lot more, maybe even something like SocketException(\"already closed\") - I guess we would expect a listener shutdown in this case?\n\nThis is somewhat related to #13861 becase a fix of this issue would make a plain CommandFailed even harder to interpret from a user perspective.",
            "url":"https://github.com/akka/akka/issues/19771"
          },
          {
            "number":19782,
            "title":"[akka-stream] Transpose a sources sequence",
            "bodyText":"I cannot find an elegant way of transposing a sources sequence by Source.combine() in Akka Stream.\nI mean transpose as in Data.List.transpose in Haskell, or Bacon.zipAsArray() in Bacon.js. It is like a ZipN handling sequences instead of tuples.\nWould you agree to let me add a new Graph subclass and a transpose method to the Source object for this purpose? Or is my use-case too specific to be worth submitting a pull-request?\nExemples given:\nscala> val sources = Seq(\n     | Source(List(1, 2, 3, 4)),\n     | Source(List(10, 20, 30, 40)),\n     | Source(List(100, 200, 300))\n     | )\nsources: Seq[akka.stream.scaladsl.Source[Int,Unit]] = List(akka.stream.scaladsl.Source@67a54547, akka.stream.scaladsl.Source@4ee81e76, akka.stream.scaladsl.Source@479918)\n\nscala> Source.transpose(sources).runWith(Sink.seq)\nres2: scala.concurrent.Future[Seq[Seq[Int]]] = scala.concurrent.impl.Promise$DefaultPromise@22f4103\n\nscala> res2.value\nres3: Option[scala.util.Try[Seq[Seq[Int]]]] = Some(Success(Vector(Vector(1, 10, 100), Vector(2, 20, 200), Vector(3, 30, 300))))\nscala> Source.combine(\n     | Source(List(4, 5, 6)),\n     | Source(List(40, 50, 60)),\n     | Source(List(400, 500, 600))\n     | )(n => Transpose(n))\nres5: akka.stream.scaladsl.Source[Seq[Int],Unit] = akka.stream.scaladsl.Source@5af484f5\n\nscala> res5.runWith(Sink.seq)\nres6: scala.concurrent.Future[Seq[Seq[Int]]] = scala.concurrent.impl.Promise$DefaultPromise@5f7def96\n\nscala> res6.value\nres7: Option[scala.util.Try[Seq[Seq[Int]]]] = Some(Success(Vector(Vector(4, 40, 400), Vector(5, 50, 500), Vector(6, 60, 600))))",
            "url":"https://github.com/akka/akka/issues/19782"
          },
          {
            "number":19793,
            "title":"Streams Contributors guide",
            "bodyText":"Let's compile a minidoc for contributors on when, how and where to add new combinators. There are many questions here that needs at least some fuzzy initial guidance:\nwhat combinators do we accept? Of course this is not definable, but sometimes it is easier to show some counterexamples that are trivially no-go\nif a combinator is to be accepted, where should it go? I.e. FlowOps, Source, Sink or external object\nif accepted and goes into the FlowOps/Source/Sink APIs then describe all the places where modifications needs to be done\nestablish a guideline for testing these and documenting these",
            "url":"https://github.com/akka/akka/issues/19793"
          },
          {
            "number":19823,
            "title":"Provide mechanism for detecting Cluster singleton failures and cluster sharding start failures",
            "bodyText":"as discussed:\nhttps://groups.google.com/forum/?fromgroups=#!topic/akka-user/_PJtBMzKuAk\n\nSounds  good to publish this on the event bus. It's a fatal error that will not repair itself, so one could claim that we should stop the actor system, but that might be too harsh.\n\nso some failure event should be published when shard coordinator fails to start\nsimilar for cluster singleton - when it is stuck, also event can be published\nthat would allow app monitoring logic to catch failure situation and act upon it",
            "url":"https://github.com/akka/akka/issues/19823"
          },
          {
            "number":19855,
            "title":"akka-typed supervision strategies",
            "bodyText":"Think about how to design reusable strategies (with conflicting goals of immutability and ease of use), implement and document.",
            "url":"https://github.com/akka/akka/issues/19855"
          },
          {
            "number":19940,
            "title":"Expand on closing IncomingConnection in documentation",
            "bodyText":"At http://doc.akka.io/docs/akka/2.4.2/scala/stream/stream-io.html, the second paragraph after the second code block states:\n\nClosing connections is possible by cancelling the incoming connection Flow from your server logic (e.g. by connecting its downstream to a Sink.cancelled and its upstream to a Source.empty). It is also possible to shut down the server's socket by cancelling the IncomingConnection source connections.\n\nPlease provide a code example of how to do this.",
            "url":"https://github.com/akka/akka/issues/19940"
          },
          {
            "number":19960,
            "title":"Wrong next chapter in doc",
            "bodyText":"In the akka scala documentation page where the typed actors are explained, the next chapter points to 'External Contributions' where as it should point to 'Fault Tolerance' as per TOC.",
            "url":"https://github.com/akka/akka/issues/19960"
          },
          {
            "number":19998,
            "title":"Scaladoc warnings when generating docs locally - unable to link to Javadoc",
            "bodyText":"There is a number of warnings that appear when running sbt doc locally that are not logged on jenkins, for example:\n[warn] /home/w/projects/akka/akka-actor/src/main/scala/akka/io/Inet.scala:120: Could not find any member to link for \"java.net.Socket#setSendBufferSize\".\n\nThis warning basically means that it's unable to find Java class, and in the result the link is not clickable.\nThe link is not clickable both on the published documentation and in my locally produced version",
            "url":"https://github.com/akka/akka/issues/19998"
          },
          {
            "number":20185,
            "title":"Try genjavadoc \"hide private members\"",
            "bodyText":"typesafehub/genjavadoc#47",
            "url":"https://github.com/akka/akka/issues/20185"
          },
          {
            "number":20396,
            "title":"Add a consistency spec for documentation of stages",
            "bodyText":"Each stage must be in the stages overview doc :)",
            "url":"https://github.com/akka/akka/issues/20396"
          },
          {
            "number":20489,
            "title":"Feature discussion - a performance metrics prioritized router that supports cluster. ",
            "bodyText":"Incentive\nThe main incentive is actually not about the performance of the system. It's to mitigate the situation when a portion of routees start to become irresponsive (not processing messages) due to some anomalies. In such a case, especially when a cluster aware router is involved, although other routees are still working perfectly fine, the router will continue forward the same portion of traffic to the problematic ones. Ideally we want the problematic routees to be avoided or de-prioritized, and just let the working routees taking more work.\nProposed Solution\nA router that, like the (ScatterGatherFirstCompleted)[https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/routing/ScatterGatherFirstCompleted.scala], uses ask which requires the routee to reply a message. This way the router can keep track of the routees' response time and thus start to de-prioritize the slow ones. The relationship between the priority and performance is up for discussion, but I speculate that it should come with some randomization and be non-linear recipes.\nOther alternatives\nAs @johanandren suggested, another solution would be implementing work pulling. We do have a work pulling library, kanaloa that we developed and are using. Making kanaloa cluster aware is going to solve our problem for us (not without significant overhaul though) but a router solution would be beneficial to other Akka users without kanaloa or implementing the pulling pattern.",
            "url":"https://github.com/akka/akka/issues/20489"
          },
          {
            "number":20563,
            "title":"Docs: Streams: Feedback loop in \"Composing complex systems\"",
            "bodyText":"In \"Composing Complex Systems\", there are code examples which lead into a feedback loop. Since C and F are Merge Modules, C always sends an element to F, which always sends this element to C, and so forth. So, B can't Broadcast more elements and this stream hangs.\nExample Code:\nimport akka.actor.ActorSystem\nimport akka.stream.scaladsl._\nimport akka.stream.{ActorMaterializer, FlowShape, Graph}\nimport akka.{Done, NotUsed}\n\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.Future\n\nobject StreamProgram2 {\n\n  import GraphDSL.Implicits._\n\n  private val graph: Graph[FlowShape[Int, Int], NotUsed] = GraphDSL.create() { implicit builder =>\n    val B = builder.add(Broadcast[Int](2))\n    val C = builder.add(Merge[Int](2))\n    val D = Flow[Int].map(_ + 1)\n    val E = builder.add(Balance[Int](2, waitForAllDownstreams = true))\n    val F = builder.add(Merge[Int](2))\n\n    //@formatter:off\n              C <~ F\n    B ~>      C ~> F\n    B ~> D.map{x => println(s\"D$x\"); x} ~> E ~> F\n    //@formatter:on\n\n    FlowShape(B.in, E.out(1))\n  }.named(\"partial\")\n\n  def main(args: Array[String]) {\n    implicit lazy val system = ActorSystem(\"example\")\n    implicit val materializer = ActorMaterializer()\n\n    val sink: Sink[Any, Future[Done]] = Sink.foreach(println)\n    val source: Source[Int, NotUsed] = Source(1 to 999)\n\n    source.via(graph).runWith(sink).onComplete(_ => System.exit(0))\n  }\n}",
            "url":"https://github.com/akka/akka/issues/20563"
          },
          {
            "number":20606,
            "title":"Inconsistent signature of Flow#recoverWithRetries",
            "bodyText":"We discussed this issue at Gitter.\nIt feels like recovery function should be inside separate argument group, i.e.\nrecoverWithRetries(42) {\n  case _ => Source.empty\n}\nlooks much better than\nrecoverWithRetries(42, {\n  case _ => Source.empty\n})",
            "url":"https://github.com/akka/akka/issues/20606"
          },
          {
            "number":20755,
            "title":"Docs incorrectly state that SupervisorStrategy can't be changed",
            "bodyText":"In the chapter about Fault Tolerance: \"... each actor is the supervisor of its children, and as such each actor defines fault handling supervisor strategy. This strategy cannot be changed afterwards ...\"\nLooking at the ultimate source of truth \u2013 the code \u2013 I strongly doubt this is correct: Actor.supervisorStrategy is a method and from what I could see it is used \"directly\" (as opposed to receive).",
            "url":"https://github.com/akka/akka/issues/20755"
          },
          {
            "number":20773,
            "title":"Addressing documentation misleading about \"path aliases\"",
            "bodyText":"As seen here: http://stackoverflow.com/questions/37757076/akka-how-to-assign-an-actor-an-alias-path\nThe person asks how to create an \"alias path\" for actors.\n\nBlockquote As in some real file-systems there also are \u201csymbolic links\u201d, i.e. one actor may be reachable using more than one path, where all but one involve some translation which decouples part of the path from the actor\u2019s actual supervision ancestor line; these specialities are described in the sub-sections to follow.",
            "url":"https://github.com/akka/akka/issues/20773"
          },
          {
            "number":20806,
            "title":"There is no akka-persistence-query-tck",
            "bodyText":"There is no tck for akka-persistence-query. It would be nice for a plugin developer to know whether or not the plugin implements the spec correctly.",
            "url":"https://github.com/akka/akka/issues/20806"
          },
          {
            "number":20868,
            "title":"Provide similar WithServer functionality Play! has",
            "bodyText":"It would be useful if the akka-http-testkit had the same WithServer trait that Play! Framework currently has. I think this should make integration testing a whole lot easier because it doesn't require any nasty SBT tricks to have both the integrations tests and the project itself running at the same time.",
            "url":"https://github.com/akka/akka/issues/20868"
          },
          {
            "number":20891,
            "title":"Introduce automatic code formatter for Java code",
            "bodyText":"We currently don't have that, only Scala code is formatted by Scalariform.\nJava code should be formatted using some sbt plugin as well.\n2 spaces. No tabs.",
            "url":"https://github.com/akka/akka/issues/20891"
          },
          {
            "number":21028,
            "title":"Publish de Akka Stream TestKit utilities",
            "bodyText":"... such as Utils.assertAllStagesStopped.\nSee discussion",
            "url":"https://github.com/akka/akka/issues/21028"
          },
          {
            "number":21035,
            "title":"Improving Circuit Breaker usability",
            "bodyText":"This is a ticket to gather feedback and interest about improving Circuit Breaker usability.\nIf you have opinions or use cases or examples of nicer to use patterns, let us know here.\nThis ticket does not mean that there's some immediate plan to re-design them, however a discussion with the community about them (or community PRs with design improvements) would be very interesting indeed!\nSee also:\n\n#18347 should be simpler to use not only with futures\n#16617  should expose metrics\n#21036 exponential backoff",
            "url":"https://github.com/akka/akka/issues/21035"
          },
          {
            "number":21047,
            "title":"Document making sure TLSv1 is disabled",
            "bodyText":"As Will @wsargent said on the mailing list:\n\nYou can set the \"jdk.tls.client.protocols\" system property to set options for the JVM -- this is a feature that is only available in JDK 1.8 though.\nhttps://docs.oracle.com/javase/8/docs/technotes/guides/security/SunProviders.html#SunJSSE_Protocols\nOtherwise, you would have to set the security property jdk.tls.disabledAlgorithms to add TLSv1 specifically.\n\nCould be documented in Akka or ssl config and we link there?",
            "url":"https://github.com/akka/akka/issues/21047"
          },
          {
            "number":21100,
            "title":"Pruning in DistributedPubSub is not covered by tests and in the documentation",
            "bodyText":"I've tried to understand, how removed-time-to-live works in DistributedPub sub, but I haven't found any references in the docs. Also the method prune() https://github.com/akka/akka/blob/master/akka-cluster-tools/src/main/scala/akka/cluster/pubsub/DistributedPubSubMediator.scala#L812 is not covered neither by unittests nor by multi-node tests.\nAlso you have implemented a trait \"PerGroupingBuffer\" in 2.4.2, but his trait also is not covered by tests. It would be good to have tests for the whole PR: #19154",
            "url":"https://github.com/akka/akka/issues/21100"
          },
          {
            "number":21126,
            "title":"akka-stream Framing.lengthField including size field",
            "bodyText":"I'm working on a steaming protocol which sends (most) messages in the following way:\n\n1 byte message identifier\n4 byte length indicator (this length equals to the length of the payload in bytes + 4bytes for the length indicator field)\npayload\n\nI was hoping that we could modify the Framing.lengthField(fieldLength, fieldOffset, maximumFrameLength, byteOrder) function to include a length modifier. This modifier would then indicate the number of byte to be added to the value found in the message.\nIn my case, this would mean that I'd have to pass -4 to the function.\nIf desirable, I could submit a pull request this evening(-ish)",
            "url":"https://github.com/akka/akka/issues/21126"
          },
          {
            "number":21171,
            "title":"Document the pluggable DnsProvider",
            "bodyText":"The DnsProvider is a pluggable by-class-name provider but there is essentially no docs at all on the plugin interfaces, given that it has two methods that return Class[_ <: Actor] I think there is room for some improvements here, what is each actor supposed to do, what constructor parameters should it take, how is it supervised, what happens if it stops, what messages should it accept and respond to. etc.",
            "url":"https://github.com/akka/akka/issues/21171"
          },
          {
            "number":21240,
            "title":"ClusterMetrics always fails to verify Sigar provisioning on Windows 7 and 10",
            "bodyText":"Running on Windows 10 with Akka 2.4.9 and kamon sigar-loader version 1.6.6-rev002.\nIn Provision.scala.\n  def verifiedSigarInstance: SigarProxy = {\n    val sigar = new Sigar()\n    sigar.getPid\n    sigar.getLoadAverage\n    sigar.getCpuPerc\n    sigar\n  }\ndef verifiedSigarInstance will always throw an exception when it attempts to run sigar.getLoadAvarage.  This causes the ClusterMetrics extension to repeatedly attempt to provision/verify sigar, but it never succeeds. Apparently getLoadAverage simply isn't implemented for the Windows OS in \"sigar-loader\" % \"1.6.6-rev002\" which is the most recent version available as of August 21 2016.  This is also the version of sigar-loader referenced in the cluster metrics documentation.\nRunning a simple program with the following code will reproduce the error under Windows 10, and likely Windows 7 too.\nimport java.io.File;\nimport org.hyperic.sigar.Sigar;\nimport kamon.sigar.SigarProvisioner;\n\nobject ExampleMain extends App {\n\n  SigarProvisioner.provision();\n   val sigar = new Sigar()\n   val loadAverage =  sigar.getLoadAverage \n} \nThe exception is:\nAug 20, 2016 2:52:59 PM kamon.sigar.SigarProvisioner discoverLocation\nINFO: Using location provided by hard coded value.\nAug 20, 2016 2:52:59 PM kamon.sigar.SigarProvisioner provision\nINFO: Sigar library provisioned: C:\\Users\\cvanvranken\\gits\\Example\\main\\server-metrics\\native\\sigar-amd64-winnt.dll\nException in thread \"main\" org.hyperic.sigar.SigarNotImplementedException: This method has not been implemented on this platform\n    at org.hyperic.sigar.SigarNotImplementedException.<clinit>(SigarNotImplementedException.java:28)\n    at org.hyperic.sigar.Sigar.getLoadAverage(Native Method)\n\nThis function should be implemented in the hyperic/sigar repo within file: src/os/win32/win32_sigar.c, for function sigar_loadavg_get.\nThere is a comment above the function which makes me think they do not intend to fix it, the comment says:  \"there is no api for this info. closest i've seen is enumerating the entire process table and calculating an average based on process times.\"\nI think the verifiedSigarInstance function that ClusterMetrics calls should be changed so that the getLoadAverage function is not called when running on Windows, otherwise Sigar will be unuseable there.  Alternatively sigar_loadavg_get needs to be implemented.\nI commented on this for the corresponding issue in the hyperic/sigar repo and someone responded to me. He says that this is implemented in the Boundary fork of sigar.",
            "url":"https://github.com/akka/akka/issues/21240"
          },
          {
            "number":21253,
            "title":"Prioritize current doc pages with sitemap",
            "bodyText":"I'm not 100% sure but I think we could make search engines prefer current versions by using a sitemap.xml on akka.io.",
            "url":"https://github.com/akka/akka/issues/21253"
          },
          {
            "number":21261,
            "title":"Remove FanoutProcessorImpl and friends",
            "bodyText":"Implement it in terms of BroadcastHub instead (after #21183 merged). Implementation can be as simple as roughly:\nobject Sink {\n...\n  def asPublisher[T](fanout: Boolean): Sink[T, Publisher[T]] = {\n     if (fanout) BroadcastHub.sink(bufferSize).mapMaterializedValue { src =>\n        new Publisher {\n            def subscribe(s: Subscriber): Unit = src.runWith(Sink.fromSubscriber(s))\n        }\n     }\n  }\n...\n}\nWhile this might be somewhat less efficient (although not sure!), it removes a rather ugly old class and its dependencies in one simple move.",
            "url":"https://github.com/akka/akka/issues/21261"
          },
          {
            "number":21349,
            "title":"Improve docs on error handling in Akka Streams",
            "bodyText":"Opening this ticket after some feedback at a meetup.\nWe should discuss the various ways of error / failure handling, this is separate to actually making supervision a good one ( #19950 ). People seem to be confused about the various options, and what would be considered idiomatic. I don't think we explain failure vs. \"domain error\" as well in the docs there. They could use a revamp.\nE.g. we should discuss pros and cons of passing around Either/Xor instead of failing stages etc.\nSome examples would be good.\nAlso, note to self, the reason users were passing around Xors were that they wanted to report metrics about that in a centralised place (in one stage). This would have not been needed if we could monitor a stream for count of failures etc.",
            "url":"https://github.com/akka/akka/issues/21349"
          },
          {
            "number":21496,
            "title":"Extract the inline heap in TopHeavyHitters to a separate class",
            "bodyText":"As discussed in #21027 and introduced in #21495 there is a stack implementation inlined in heavy hitters which would be nicer if it was a separate class.",
            "url":"https://github.com/akka/akka/issues/21496"
          },
          {
            "number":21526,
            "title":"Sink with resource management",
            "bodyText":"Source has unfoldResource() and unfoldResourceAsync(). It does open a resource, read and close.\nIt would be useful if Sink had a similar method, which does open, write and close. It may be named as Sink.foreachResource().",
            "url":"https://github.com/akka/akka/issues/21526"
          },
          {
            "number":21626,
            "title":"It will kind to add \"include\"s to code exaples to simplify cut'n'paste for lay people",
            "bodyText":"Really it's annoing to investigate symbol origins, while some half of people not use ides with completing",
            "url":"https://github.com/akka/akka/issues/21626"
          },
          {
            "number":21630,
            "title":"Streams to ClusterSharding Sink",
            "bodyText":"I have a need at the moment to take streaming data and pump it into a cluster that utilizes cluster sharding.  There is a sinkWithActorRef available, but there are some inefficiencies with putting an Actor between the streams and the ClusterShardingRegion.  For example, one aspect that I would really love is the ability of the stream to backpressure in response to the local ShardRegions message buffers filling, or better yet, if backpressure is communicated(from remote ShardRegions or local Shards).  It is certainly possible to do this with a separate actor, but some of the information that actor would have to request anyways from the ShardRegion(maybe cluster membership status) and some I don't have access to at all(such as the state of message buffers, or the fact Shards are being handed off).  It would be super convenient if there was a sinkWithClusterSharding or something like that which would handle some of this logic for me, potentially backpressuring on handoff changes, or with remember entities on, during the recovery state.",
            "url":"https://github.com/akka/akka/issues/21630"
          },
          {
            "number":21677,
            "title":"LevelDB journal never removes persistence IDs",
            "bodyText":"I'm not 100% sure that I'm right here, but in my reading of the code it appears that even if you delete all the events for a given persistence ID, the mapping of numerical_id->persistence_id remains in the journal.\nIn my application (and I suspect this is not uncommon) we will create many actors with unique persistence IDs, which will have a medium lifespan. They rely on the Journal for recovery, but when they terminate successfully they clean up all persistence data, since they will never be needed again.\nObviously, it would take a very very long time or many, many actors -- but the leveldb files would seemingly grow unbounded even though you thought you were cleaning up after yourself, because those id->id mappings are never removed.\nFeel free to close if I'm wrong, or this is by design for some reason.",
            "url":"https://github.com/akka/akka/issues/21677"
          },
          {
            "number":21708,
            "title":"TLSActor causes `IllegalArgumentException` with non-LDH ASCII hostnames.",
            "bodyText":"When making a request using akka-http to this url: https://open_nsfw.gitlab.io/. The following exception is thrown.\nCaused by: java.lang.IllegalArgumentException: Contains non-LDH ASCII characters\n    at java.net.IDN.toASCIIInternal(IDN.java:296)\n    at java.net.IDN.toASCII(IDN.java:122)\n    at javax.net.ssl.SNIHostName.<init>(SNIHostName.java:99)\n    at akka.stream.impl.io.TLSActor$$anonfun$applySNI$1$$anonfun$apply$3.apply(TLSActor.scala:497)\n    at akka.stream.impl.io.TLSActor$$anonfun$applySNI$1$$anonfun$apply$3.apply(TLSActor.scala:482)\n    at scala.Option$WithFilter.map(Option.scala:207)\n    at akka.stream.impl.io.TLSActor$$anonfun$applySNI$1.apply(TLSActor.scala:482)\n    at akka.stream.impl.io.TLSActor$$anonfun$applySNI$1.apply(TLSActor.scala:481)\n    at scala.Option.flatMap(Option.scala:171)\n    at akka.stream.impl.io.TLSActor.applySNI(TLSActor.scala:481)\n    at akka.stream.impl.io.TLSActor.applySessionParameters(TLSActor.scala:177)\n    at akka.stream.impl.io.TLSActor.<init>(TLSActor.scala:164)\n    at akka.stream.impl.io.TLSActor$$anonfun$props$1.apply(TLSActor.scala:37)\n    at akka.stream.impl.io.TLSActor$$anonfun$props$1.apply(TLSActor.scala:37)\n    at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87)\n    at akka.actor.Props.newActor(Props.scala:213)\n    at akka.actor.ActorCell.newActor(ActorCell.scala:562)\n    at akka.actor.ActorCell.create(ActorCell.scala:588)\n    ... 9 more\n\nIt seems like changing TLSActor here to\nclone.setServerNames(Collections.singletonList(new SNIHostName(hostname.getBytes))) works around the issue.\nAre there any other issues this change would raise?",
            "url":"https://github.com/akka/akka/issues/21708"
          },
          {
            "number":21750,
            "title":"Document strategies on providing Akka Streams libs",
            "bodyText":"There should be a small section for library authors how they should expose their Akka Streams operators.\nThere's a few styles:\n\njavadsl / scaladsl if it has to add API\njust provide Graph[....] which can be used by either api, either using via of Source.fromGraph etc.\n\nWe don't really suggest these for authors so they're guessing what's best.\nA nice example of showing this to users is in eventuate: http://rbmhtechnology.github.io/eventuate/adapters/stream.html",
            "url":"https://github.com/akka/akka/issues/21750"
          },
          {
            "number":21856,
            "title":"Amend documentation about adding custom operations on FlowOps",
            "bodyText":"With Scala 2.12 the crazy type-level-hack-workaround as discovered by @rkuhn back in the days is not needed anymore.\nWe pointed it out here: http://doc.akka.io/docs/akka/rp-current/scala/stream/stream-customize.html#Extending_Flow_Combinators_with_Custom_Operators\nWith 2.12 we can show off that:\nscala> import akka.stream._\nimport akka.stream._\n\nscala> import akka.stream.scaladsl._\nimport akka.stream.scaladsl._\n\nscala> import akka.stream.stage._\nimport akka.stream.stage._\n\nscala> import scala.language.higherKinds\nimport scala.language.higherKinds\n\nscala> final case class Klangified[T](val the: T)\ndefined class Klangified\n\nscala> final implicit class KlangifyOps[O, M, FO[o,m] <: FlowOps[o,m]](val fo: FO[O, M]) {\n     |   type Repr[T] = FO[O, M]#Repr[T]\n     |   def klangify(times: Int): Repr[Klangified[O]] = fo.mapConcat(x => List.fill(times)(Klangified(x)))\n     |   def passthru: Repr[O] = fo.map(identity)\n     | }\ndefined class KlangifyOps\n\nscala> Source(1 to 10).passthru\nres11: akka.stream.scaladsl.Source[Int,akka.NotUsed] =\nSource(SourceShape(Map.out), CompositeModule [23661f5d]\n  Name: unnamed\n  Modules:\n    (iterableSource) CompositeModule [546cd17e]\n      Name: iterableSource\n      Modules:\n        (singleSource) GraphStage(SingleSource(Range 1 to 10)) [5fb58725]\n        (unnamed) [6286e224] copy of GraphStage(StatefulMapConcat) [710995de]\n      Downstreams:\n        single.out -> StatefulMapConcat.in\n      Upstreams:\n        StatefulMapConcat.in -> single.out\n      MatValue: Atomic(singleSource[5fb58725])\n    (unnamed) [435c4da0] copy of GraphStage(Map(KlangifyOps$$Lambda$2787/1867139207@65e135b3)) [30a36641]\n  Downstreams:\n    single.out -> StatefulMapConcat.in\n    StatefulMapConcat.out -> Map.in\n  Upstreams:\n    StatefulMapConcat.in -> single.out\n...\nscala> Flow[Int].passthru\nres12: akka.stream.scaladsl.Flow[Int,Int,akka.NotUsed] =\nFlow(FlowShape(Map.in,Map.out), CompositeModule [13e890d0]\n  Name: unnamed\n  Modules:\n    (map) GraphStage(Map(KlangifyOps$$Lambda$2787/1867139207@65e135b3)) [45280b87]\n  Downstreams:\n  Upstreams:\n  MatValue: Ignore)\n\nscala> res12.klangify(2)\nres13: akka.stream.scaladsl.Flow[Int,Klangified[Int],akka.NotUsed] =\nFlow(FlowShape(Map.in,StatefulMapConcat.out), CompositeModule [637ad325]\n  Name: unnamed\n  Modules:\n    (map) GraphStage(Map(KlangifyOps$$Lambda$2787/1867139207@65e135b3)) [45280b87]\n    (unnamed) [5259b832] copy of GraphStage(StatefulMapConcat) [2146991d]\n  Downstreams:\n    Map.out -> StatefulMapConcat.in\n  Upstreams:\n    StatefulMapConcat.in -> Map.out\n  MatValue: Ignore)\n\nscala> res11.klangify(2)\nres14: akka.stream.scaladsl.Source[Klangified[Int],akka.NotUsed] =\nSource(SourceShape(StatefulMapConcat.out), CompositeModule [464d5e11]\n...\nvia @viktorklang",
            "url":"https://github.com/akka/akka/issues/21856"
          },
          {
            "number":22071,
            "title":"Documentation for TLS support in Akka streams is missing",
            "bodyText":"Current documentation for Akka Streams does not mention TLS.\nRef: https://groups.google.com/d/msg/akka-user/QtSJERVs_9k/ZmXyvOCUAAAJ",
            "url":"https://github.com/akka/akka/issues/22071"
          },
          {
            "number":22074,
            "title":"Source.fromFunction / Source.continually - shortcut to eval a function for every emit",
            "bodyText":"Not sure if this has been asked already. I find it very common to construct a Source along the lines of Source.repeat, but with the need of evaluating a function and producing a new element every time the source emits.\nI usually end up writing Source.cycle(() => Iterator(Random.nextInt())) . I'm writing here to check if anyone might deem useful to have a shortcut in Source.scala - with a meaningful name (e.g. Source.fromFunction, or Source.continually).",
            "url":"https://github.com/akka/akka/issues/22074"
          },
          {
            "number":22183,
            "title":"Add notes about shutdown patterns for Streams to docs",
            "bodyText":"As a newbie, I ran into a case where I couldn't understand how to terminate the ActorSystem, used to materialize the Stream, after the latter has finished it work. I scanned through the titles in the doc and couldn't find a word about shutdown patterns. The onComplete() callback (which was an obvious choice) unexpectedly fired right after my pipeline was run.\nThanks to akka gitter chat and @ktoso, I was pointed towards \"Custom materialized values\" section, because it turned out I didn't use one in my custom Sink, and that's why onComplete() fired too early. But it was very unobvious to me as a beginner.\nSo it'd be great if there will be some small section in the beginning of the Streams doc, that has a keyword \"shutdown\" or similar and explains basic necessity of Mat values for signaling completition of a stream with an example of a proper shutdown pattern.",
            "url":"https://github.com/akka/akka/issues/22183"
          },
          {
            "number":22190,
            "title":"Describe how to use javaslang with AbstractActor",
            "bodyText":"Suggestion by @jypma matcher DSL in javaslang",
            "url":"https://github.com/akka/akka/issues/22190"
          },
          {
            "number":22220,
            "title":"Consider recipe or API for debounce",
            "bodyText":"Recently someone asked me for a way to \"not send the same thing 100 times to an actor\",\nthis is basically the debounce operation.\nIt can be implemented such:\n  import scala.concurrent.duration._\n  \n  /**\n   * Groups items within a given time interval (unless the max size is reached, then earlier),\n   * and picks the single element to signal downstream using the provided `pick` function.\n   */\n  def debounceSelect[A](interval: FiniteDuration, pick: immutable.Seq[A] => A, max: Int = 100) =\n    Flow[A].groupedWithin(max, interval).map { group => pick(group) }\n\n  case class RefreshSignal(msg: Any)\n  \n  \n  val queue: SourceQueueWithComplete[RefreshSignal] = \n    Source.queue[RefreshSignal](4, OverflowStrategy.dropHead)\n      .via(debounceSelect(1.second, _.head)) // picking any of the refresh signals\n      .to(Sink.actorRef(target, onCompleteMessage = RefreshSignal(\"done\")))\n      .run()\n  \n  val offer1: Future[QueueOfferResult] = queue.offer(RefreshSignal(\"refresh\"))\n  val offer2: Future[QueueOfferResult] = queue.offer(RefreshSignal(\"refresh\"))\n  val offer3: Future[QueueOfferResult] = queue.offer(RefreshSignal(\"refresh\"))\n  val offer4: Future[QueueOfferResult] = queue.offer(RefreshSignal(\"refresh\"))\n  \nShall we either make it a recipe or even add as an API?\nI think i've bumped into this a few times already",
            "url":"https://github.com/akka/akka/issues/22220"
          },
          {
            "number":22234,
            "title":"Add an eagerComplete flag on the Partition stage",
            "bodyText":"The Merge stage has an eagerComplete flag, it would be nice if the Partition stage has the same kind of flag.\nIt's useful for my use-cases but I think it could be useful for others too.",
            "url":"https://github.com/akka/akka/issues/22234"
          },
          {
            "number":22276,
            "title":"Retry Pattern",
            "bodyText":"I find I occasionally want to retry a future so I wrote this helper that lives in a common Library I maintain:\nimport scala.concurrent.{ ExecutionContext, Future } \nimport scala.concurrent.duration.FiniteDuration\nimport akka.actor.Scheduler\nimport akka.pattern.after\n\ndef retry[T](\n  attempt: () => Future[T],\n  attempts: Int,\n  delay: FiniteDuration,\n  scheduler: Scheduler\n)(implicit ec: ExecutionContext): Future[T] = if (attempts > 0) { \n  attempt() recoverWith { \n    case _ => after(delay, scheduler){ retry(attempt, attempts - 1, delay, scheduler) } \n  } \n} else after(delay, scheduler){ attempt() } \nIs this the sort of thing that could live in akka.pattern?",
            "url":"https://github.com/akka/akka/issues/22276"
          },
          {
            "number":22373,
            "title":"[Docs] Java documentation regarding 'ask' referencing Scala API instead of Java API",
            "bodyText":"The documentation on the Java 'Actors' page uses the Scala version of 'ask' instead of the one in PatternsCS.",
            "url":"https://github.com/akka/akka/issues/22373"
          },
          {
            "number":22385,
            "title":"Add support for skipping too long frames in Framing.delimiter",
            "bodyText":"In Akka Stream, the Framing object allows to split a stream (e.g. a FileIO) in chunks.\nMore specifically, the Framing.delimiter method splits the stream when encountering a given delimiter.\ndef delimiter(delimiter: ByteString, maximumFrameLength: Int, allowTruncation: Boolean = false): Flow[ByteString, ByteString, NotUsed]\nI've been using it to parse the lines of a file using the \\n delimiter.\nWhen a line is longer than the maximumFrameLength which is passed as a parameter, the Flow fails the Stream.\nIt means that as soon as we encounter a line too long, the stream is failed and we have no way to read the remaining of the file.\nTo circumvent this issue, we could add another parameter to the delimiter method, e.g. skipTooLongFrames which, when set to true, would just make the Flow ignore too long frames and continue to read the stream.\nThe method would look like:\ndef delimiter(delimiter: ByteString, maximumFrameLength: Int, allowTruncation: Boolean = false, skipTooLongFrames: Boolean = false): Flow[ByteString, ByteString, NotUsed]\nIs there another way to deal with this? If not, could we consider adding the parameter? Or is this going against some principles of Akka Stream?\n(Note that the same question applies for the lengthField method of the same object.)",
            "url":"https://github.com/akka/akka/issues/22385"
          },
          {
            "number":22452,
            "title":"Source.combine instance method sould require only one argument",
            "bodyText":"It is currently a copy of the static combine and should either be removed or changed so that it requires a minimum of one argument and adds itself as the first input of the resulting Source.",
            "url":"https://github.com/akka/akka/issues/22452"
          },
          {
            "number":22457,
            "title":"Java API - an abstract actor class for PersistentFsm and AtLeastOnceDelivery traits ",
            "bodyText":"This issue tracks the request mentioned in the following Akka user list topic - https://groups.google.com/forum/#!topic/akka-user/mJTjpClYjbw\nFor the benefit of the Java API users it would be beneficial to add an abstract class that implements the traits PersistentFSM & AtLeastOnceDelivery.\nKonrad Malawski mentioned that mixing in the two Scala traits PersistentFSM & AtLeastOnceDelivery should work in theory. I hope to perform testing on how such mixin works out within the next month or two.\nAs mentioned in the user list topic - it seems there are others that found such an abstract actor to be a desired feature as well. See http://www.google.com/url?q=http%3A%2F%2Fstackoverflow.com%2Fquestions%2F39374797%2Fcan-i-mix-persistenct-fsm-and-at-least-once-message-delivery-in-akka&sa=D&sntz=1&usg=AFQjCNGeoz-VVnrBiDdcxAdaCJgAoOcyAQ",
            "url":"https://github.com/akka/akka/issues/22457"
          },
          {
            "number":22566,
            "title":"[Documentation] Akka Cluster/Remote with SSL ",
            "bodyText":"I was trying to use ssl in an akka cluster. To do that, I follow the doc...  http://doc.akka.io/docs/akka/2.4/scala/remoting.html#Configuring_SSL_TLS_for_Akka_Remoting\nIMHO, it could be a good idea to include that its necessary to specify the hostname and port of the ssl connection actor. The problem is, if you don't specify those parammeters, the cluster node will be started at 127.0.0.1 always, and it's is possible that it can't discover itself due to the seed nodes maybe won't be at 127.0.0.1.\nExample:\nakka {\nremote {\nnetty.ssl.security {\nhostname = myHosy\nport = 12345\nkey-store = \"/example/path/to/mykeystore.jks\"\ntrust-store = \"/example/path/to/mytruststore.jks\"\nkey-store-password = \"changeme\"\nkey-password = \"changeme\"\ntrust-store-password = \"changeme\"\nprotocol = \"TLSv1.2\"\nenabled-algorithms = [TLS_DHE_RSA_WITH_AES_128_GCM_SHA256]\nrandom-number-generator = \"AES128CounterSecureRNG\"\n}\n}\n}",
            "url":"https://github.com/akka/akka/issues/22566"
          },
          {
            "number":22575,
            "title":"Improve Scheduler docs a bit",
            "bodyText":"We should be more helpful in the docs comment as Endre pointed out here: #22334 (comment)",
            "url":"https://github.com/akka/akka/issues/22575"
          },
          {
            "number":22600,
            "title":"[Docs] Ambiguous explanation of Distributed Data vs. Persistence Mode",
            "bodyText":"The documentation page http://doc.akka.io/docs/akka/2.5/scala/cluster-sharding.html#Distributed_Data_vs__Persistence_Mode describes the differences of using Distributed Data vs. Persistence mode in Akka Cluster Sharding. The following text in the section is ambiguous:\n\nThe state of the coordinator and the state of Remembering Entities of the shards are persistent (durable) to survive failures. Distributed Data or Persistence can be used for the storage. Distributed Data is used by default.\n\nThe state of the coordinator is not durable in ddata mode.\n\nThe state of the ShardCoordinator will be replicated inside a cluster by the Distributed Data module with WriteMajority/ReadMajority consistency. The state of the coordinator is not durable, it's not stored to disk. When all nodes in the cluster have been stopped the state is lost and not needed any more.\n\n\nThe state of Remembering Entities is also durable, i.e. it is stored to disk. The stored entities are started also after a complete cluster restart.\n\nThe \"also\" is misleading here. Only the state of the Remembering Entities is durable in ddata mode.",
            "url":"https://github.com/akka/akka/issues/22600"
          },
          {
            "number":22710,
            "title":"interleave doesn't expose eagerClose parameter",
            "bodyText":"I'm currently unable to set eagerClose = true in the stream DSL. Looks like an oversight. It should be added here as the last param.",
            "url":"https://github.com/akka/akka/issues/22710"
          },
          {
            "number":22811,
            "title":"Scala in some Java documentation",
            "bodyText":"While reading the Java version of Remoting (codename Artery) I came across a few bits of SBT and Scala. Nothing major, the rest of the page seems to be all using Java examples.\nhttp://doc.akka.io/docs/akka/2.5.0/java/remoting-artery.html#Acquiring_references_to_remote_actors\nIn the Preparing your ActorSystem for Remoting section, the dependency is not Maven but build.sbt.\nhttp://doc.akka.io/docs/akka/2.5.0/java/remoting-artery.html#Preparing_your_ActorSystem_for_Remoting\nIn the Looking up Remote Actors section, the example tell is in Scala syntax.\nhttp://doc.akka.io/docs/akka/2.5.0/java/remoting-artery.html#Looking_up_Remote_Actors\nIn the Untrusted Mode section, there is a problem with a configuration setting.\nhttp://doc.akka.io/docs/akka/2.5.0/java/remoting-artery.html#Untrusted_Mode\nIs: akka.remote.artery..trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]\nShould be: akka.remote.trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]",
            "url":"https://github.com/akka/akka/issues/22811"
          },
          {
            "number":22816,
            "title":"Uneeded org.iq80.leveldb:leveldb dependency in akka-persistence?",
            "bodyText":"Hi,\nI don't understand why akka-persistence depends on org.iq80.leveldb:leveldb:\n\nas leveldbjni states in its README:\n\n\nLastly, another project unrelated to this project separately provides a (less mature) pure Java implementation of LevelDB, see dain/leveldb. Note that both that and this project share the same Maven artefact for the Level DB API interface (org.iq80.leveldb:leveldb-api).\n\n\nakka-persistence seems to use one class from org.iq80.leveldb:leveldb but it is from its tests, no the main code.\n\nI would thus recommend (if I am not mistaken :):\n\nremoving any mention of org.iq80.leveldb:leveldb in the documentation\nremoving any non-test dependency to org.iq80.leveldb:leveldb in akka-persistence\n\nEDIT: see comment below",
            "url":"https://github.com/akka/akka/issues/22816"
          },
          {
            "number":22874,
            "title":"Add examples to Sink.actorRefWithAck and Source.queue docs",
            "bodyText":"Clarify how Sink.actorRefWithAck and Source.queue are supposed to be used by adding example code.\nhttp://doc.akka.io/docs/akka/2.5/scala/stream/stream-integrations.html#Integrating_with_Actors",
            "url":"https://github.com/akka/akka/issues/22874"
          },
          {
            "number":22904,
            "title":"Scaladoc/Javadoc links",
            "bodyText":"We should make use of http://developer.lightbend.com/docs/paradox/latest/features/linking.html#scaladoc-directive\nThis task is not top priority, can be added after first release.",
            "url":"https://github.com/akka/akka/issues/22904"
          },
          {
            "number":22906,
            "title":"add links and side bars notes to Getting Started",
            "bodyText":"prototype of the side bar notes can be seen here: http://downloads.lightbend.com/paradox/akka-docs-new/20170511-sidenotes/scala/actors.html",
            "url":"https://github.com/akka/akka/issues/22906"
          },
          {
            "number":22921,
            "title":"exclude snippets",
            "bodyText":"In rst we sometimes used the exclude like in actors.rst:\n.. includecode:: code/jdocs/actor/ActorDocTest.java#context-actorOf\n   :exclude: plus-some-behavior\n\n  //#context-actorOf\n  public class FirstActor extends AbstractActor {\n    final ActorRef child = getContext().actorOf(Props.create(MyActor.class), \"myChild\");\n    \n    //#plus-some-behavior\n    @Override\n    public Receive createReceive() {\n      return receiveBuilder()\n        .matchAny(x -> getSender().tell(x, getSelf()))\n        .build();\n    }\n    //#plus-some-behavior\n  }\n  //#context-actorOf\nThis is lost in new .md\n@@snip [ActorDocTest.java]($code$/java/jdocs/actor/ActorDocTest.java) { #context-actorOf }\n\n@bantonsson You showed me a xml example of how that could be done, but I still don't understand what I need to do for an example like this?",
            "url":"https://github.com/akka/akka/issues/22921"
          },
          {
            "number":22950,
            "title":"Source.combine method should keep Materialized Values",
            "bodyText":"Source.combine() does not keep the Materialized Values Attributes, it always returns\nSource[U, NotUsed]. It forces me to use the GraphDSL even for such a simple Graph.\nI would like to use Source.combine()  and be able to define how to handle the Materialized Values.\nExample actual:\n    Source<Integer, SourceQueueWithComplete<Integer>> queuePrimary = Source.queue(2000, OverflowStrategy.backpressure());\n    Source<Integer, SourceQueueWithComplete<Integer>> queueSecondary = Source.queue(2000, OverflowStrategy.backpressure());\n\n    Source<Integer, NotUsed> oneQueue = Source.combine(queuePrimary, queueSecondary, new ArrayList<>(), i -> Merge.<Integer> create(i));\n                    ^^^^^^^^ \n\nExample expected:\n    Source<Integer, Pair<SourceQueueWithComplete<Integer>,SourceQueueWithComplete<Integer>>> oneQueue = Source.combine(queuePrimary, queueSecondary, i -> Merge.<Integer> create(i), Keep.both());\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSee also #22452 for more API changes.",
            "url":"https://github.com/akka/akka/issues/22950"
          },
          {
            "number":22974,
            "title":"Remove tombstones from `ORMultiMap.emptyWithValueDeltas`",
            "bodyText":"More details and PR soon.",
            "url":"https://github.com/akka/akka/issues/22974"
          },
          {
            "number":23027,
            "title":"Provide an easy way to switch to the most recent version of the docs",
            "bodyText":"We used to have warnOldDocs.js, but that got lost with our move to paradox.\nWe should either reuse warnOldDocs.js or find another way to achieve this functionality.\nMay need #23024 to be reintroduced.",
            "url":"https://github.com/akka/akka/issues/23027"
          },
          {
            "number":23052,
            "title":"consolidate java/scala content",
            "bodyText":"With the @java and @scala tags and the multi-tab code snippets it's possible to consolidate java and scala content into same .md file and avoid duplication.\nExample: https://raw.githubusercontent.com/akka/akka/master/akka-docs/src/main/paradox/scala/guide/tutorial_1.md\n\n actors.md - @richard-imaoka\n ... you can help too! Please write a comment what you're working on to avoid duplicate work.\n\nCan be done piece-by-piece.",
            "url":"https://github.com/akka/akka/issues/23052"
          },
          {
            "number":23111,
            "title":"AsyncCallbacks to just-finishing stages can be lost",
            "bodyText":"AsyncCallbacks work in a fire-and-forget fashion by sending messages to the underlying ActorGraphInterpreter. While the message is in-flight a stage can shut down concurrently. When the queued asynchronous signals are executed later, they are silently discarded if the stage has gone in the meantime. This might be acceptable for some use cases but others rely on no asynchronous signals being lost. Especially, the CallbackWrapper infrastructure tries to \"gracefully\" handle those cases but will still miss asynchronously enqueued async signals that were scheduled before the stopCallback was called.\nI think we should at least add DEBUG logging when an async signal is dropped here: \n  \n    \n      akka/akka-stream/src/main/scala/akka/stream/impl/fusing/GraphInterpreter.scala\n    \n    \n         Line 436\n      in\n      87b28d0\n    \n    \n    \n    \n\n        \n          \n           if (!isStageCompleted(logic)) { \n        \n    \n  \n\n\nWe should also think about how to improve the handling, e.g. in the CallbackWrapper by notifying the CallbackWrapper when the async callback has actually be executed. All in-flight async signals should be buffered (similar to the NotInitialized state) so that they can be handled when stopCallback was called.\n/cc @agolubev who contributed CallbackWrapper.",
            "url":"https://github.com/akka/akka/issues/23111"
          },
          {
            "number":23250,
            "title":"Using serializer [akka.serialization.JavaSerializer] for message [akka.cluster.sharding.ShardRegion$ShardRegionStats]",
            "bodyText":"Hi,\nApparently there is no serializer for akka.cluster.sharding.ShardRegion$ShardRegionStats.\nThis would be nice to have one as the JavaSerializer could potentially not perform well.\nLooking at the reference.conf it includes only one serializer. This serializer doesn't make any reference to ShardRegionStats.\nThe type only defines one map[string, int] so serialization for this  type should be straightforward enough to implement.\nLooking forward to hear your opinion on it.\nRegards,\nLeo",
            "url":"https://github.com/akka/akka/issues/23250"
          },
          {
            "number":23253,
            "title":"Periodic warning logs for QueueSource buffer fill",
            "bodyText":"It would be nice to have some logs around the buffer fill in a QueueSource, similar to how ShardRegion reports when ShardCoordinator is not yet contacted.",
            "url":"https://github.com/akka/akka/issues/23253"
          },
          {
            "number":23263,
            "title":"Document that takeWhile can be used as \"complete this stream when X comes in\"",
            "bodyText":"I've found myself needing this many times before, and have needed it on two separate occasions today. When a certain message passes through my flow, I want the flow to complete.\nAn example use case is a flow for a chat room (implemented using a merge sink), if a leave chat room message passes through the flow, you want to terminate that flow.  The sender may not be able to do that, in my specific use case, a user has a single WebSocket connection that multiplexes messages for multiple chat rooms that they may currently be in. The flow is demultiplexed by a broadcast source, filtered for each room, and then sent to merge sinks for each room. The sender of the leave message doesn't want to close the WebSocket after sending the leave chat room message, so can't close the flow, it has to be closed by something in the flow post demux to dynamically remove that flow from the broadcast.",
            "url":"https://github.com/akka/akka/issues/23263"
          },
          {
            "number":23322,
            "title":"`UniformFanInShape` vs `UniformFanOutShape`  API inconsistency",
            "bodyText":"UniformFanInShape has inSeq, whereas UniformFanOutShape has outArray.\nThere's probably no good reason for this discrepancy, so correct it.",
            "url":"https://github.com/akka/akka/issues/23322"
          },
          {
            "number":23325,
            "title":"Doc emitting a ByteString over TCP isn't one to one with reading it",
            "bodyText":"Seen in the gitter channel: users expecting that if they emit one byte string from the stream TCP server or client they would get that same ByteString on the other side. It hadn't occurred to me before but maybe we should try to document this more clearly somewhere. API docs or manual, not sure.",
            "url":"https://github.com/akka/akka/issues/23325"
          },
          {
            "number":23358,
            "title":"Need support for custom trust manager implementations in akka-remote",
            "bodyText":"Need to be able to load a custom TrustManager extending X509ExtendedTrustManager and initialize the SSLEngine to perform hostname validation.\nThis would solve the hostname verification from #13874\" SSL/TLS: Add client-side certificate verification to akka-remote\" and #21669 \"Provide support (and/or documentation) for TLS certificate pinning with (old) akka-remote\".",
            "url":"https://github.com/akka/akka/issues/23358"
          },
          {
            "number":23406,
            "title":"BackoffSupervisor and restart documentation is confusing/inaccurate",
            "bodyText":"Hello,\nI've just spent a day figuring out why my actor's preRestart() and postRestart() methods were not called when my actor was restarted.\nTurns out that while there is a very clear documentation of \"What restarting means\", the documentation for BackoffSupervisor uses the term \"restart\" all the time, but does not actually implement the documented restarting lifecycle. BackoffSupervisor doesn't \"restart\" actors, but actually \"stops\" them ASAP, and the \"starts\" them later, skipping all of the documented restart lifecycle steps.\nThe documentation of BackoffSupervisor (in /general/supervision.html) should be updated to reflect that fact, either by using an extra paragraph, by not using the term \"restart\" and/or by defining and using another term for that behavior.\nNote that the API of BackoffSupervisor can take a SupervisionStrategy from which you must return restart, and then the actions taken by the supervisor is not a true restart. That is quite confusing.",
            "url":"https://github.com/akka/akka/issues/23406"
          },
          {
            "number":23439,
            "title":"GraphInterpreter should not keep references to shut-down logics",
            "bodyText":"When multiple stages are fused, a single alive stage will keep the GraphInterpreter alive and with it references to all already completed GraphStage logics (which may or may not keep a significant amount of memory).\nSee also #23437.",
            "url":"https://github.com/akka/akka/issues/23439"
          },
          {
            "number":23462,
            "title":"Sending messages to DeadLetter by DistributedPubSub should be configurable",
            "bodyText":"When there are no subscribers for a given topic DistributedPubSub will send messages published to that topic to DeadLetter. While it is valuable for debugging, it also results in numerous false alarms. IMO publishing to a topic without caring about the presence of subscribers is a common use case of publish-subscribe pattern, and therefore it should be possible to do it without generating warning.\nThe issue originally started as my question on akka-user mailing group. Where @johanandren suggested making the behaviour of DistributedPubSub configurable, making it possible to disable sending these message to dead letter, while keeping current behaviour as default.\nI'm willing to provide a PR for this.\nThe sending of messages to DeadLetter is implemented at\nline 716 in DistributedPubSubMediator.scala and it was a solution for #19009",
            "url":"https://github.com/akka/akka/issues/23462"
          },
          {
            "number":23472,
            "title":"Bug: Shard coordinator is never created if a shard proxy starts before the shard region itself",
            "bodyText":"Symptom\nStarting a proxy to a shard region (say \"myType\") before starting the shard region itself puts the actor system in a bad state. Attempting to create the shard region \"myType\" will result in success, but the shard region coordinator will be absent, and the shard region will not be able to receive messages from its proxies.\nCause\n\nThe cluster sharding guardian receives the message StartProxy(\"myType\", ...)\" when it has no child \"myType\" and proceeds to create the child \"myType\".\nThe cluster sharding guardian receives the message Start(\"myType\", ...)\". It checks whether the child \"myType\" exists. Seeing that the child exists (it created the child a moment ago), it thinks the shard region was created already and replied with Started(...) without going through creating the coordinator singleton manager.\n\nRelevant line:\n\n  \n    \n      akka/akka-cluster-sharding/src/main/scala/akka/cluster/sharding/ClusterSharding.scala\n    \n    \n         Line 462\n      in\n      b45a254\n    \n    \n    \n    \n\n        \n          \n           val shardRegion = context.child(encName).getOrElse { \n        \n    \n  \n\n\nProposed fix\nReplace line 462 of ClusterSharding.scala\nval shardRegion = context.child(encName).getOrElse { ... }\nby\nval shardRegion = context.child(cName).flatMap(_ => context.child(encName)).getOrElse { ... }\nMinimal throwing example\nIn Java:\nimport java.util.Optional;\nimport java.util.concurrent.TimeUnit;\nimport com.typesafe.config.Config;\nimport com.typesafe.config.ConfigFactory;\nimport akka.actor.ActorRef;\nimport akka.actor.ActorSystem;\nimport akka.actor.Props;\nimport akka.cluster.sharding.ClusterSharding;\nimport akka.cluster.sharding.ClusterShardingSettings;\nimport akka.cluster.sharding.ShardRegion;\nimport akka.testkit.TestActors;\nimport scala.concurrent.duration.FiniteDuration;\n\npublic final class Main {\n\n    public static void main(final String[] args) throws Exception {\n\n        final String configString = \"akka.actor.provider=\\\"akka.cluster.ClusterActorRefProvider\\\"\";\n        final Config config = ConfigFactory.parseString(configString).withFallback(ConfigFactory.defaultApplication());\n        final ActorSystem system = ActorSystem.create(\"myActorSystem\", config);\n        final ClusterSharding clusterSharding = ClusterSharding.get(system);\n        final ClusterShardingSettings shardingSettings = ClusterShardingSettings.create(system);\n        final ShardRegion.MessageExtractor messageExtractor = new ShardRegion.HashCodeMessageExtractor(10) {\n            public String entityId(final Object message) {\n                return \"dummyId\";\n            }\n        };\n\n        // start a proxy to the shard \"myType\"\n        final ActorRef shardProxy = clusterSharding.startProxy(\"myType\", Optional.empty(), messageExtractor);\n\n        // starting a proxy makes the cluster sharding guardian create the child \"myType\" without the coordinator.\n        final ActorRef childOfGuardian =\n                system.actorSelection(\"akka://myActorSystem/system/sharding/myType\")\n                        .resolveOneCS(FiniteDuration.create(5, TimeUnit.SECONDS))\n                        .toCompletableFuture()\n                        .get();\n\n        final Props props = Props.create(TestActors.EchoActor.class, TestActors.EchoActor::new);\n\n        // start the shard \"myType\" for real, but no coordinator is created\n        final ActorRef shardRegion = clusterSharding.start(\"myType\", props, shardingSettings, messageExtractor);\n\n        // shard coordinator not found:\n        // akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka://myActorSystem/), Path(/system/sharding/myTypeCoordinator)]\n        final ActorRef shardCoordinator =\n                system.actorSelection(\"akka://myActorSystem/system/sharding/myTypeCoordinator\")\n                        .resolveOneCS(FiniteDuration.create(5, TimeUnit.SECONDS))\n                        .toCompletableFuture()\n                        .get();\n    }\n}\nThe final actor tree from ActorSystem.printTree is as follows (generated with akka-cluster_2.11 v2.5.3).\n-> / LocalActorRefProvider$$anon$1 class akka.actor.LocalActorRefProvider$Guardian status=0 2 children\n   ⌊-> system LocalActorRef class akka.actor.LocalActorRefProvider$SystemGuardian status=0 11 children\n   |   ⌊-> cluster RepointableActorRef class akka.cluster.ClusterDaemon status=0 2 children\n   |   |   ⌊-> core LocalActorRef class akka.cluster.ClusterCoreSupervisor status=0 2 children\n   |   |   |   ⌊-> daemon LocalActorRef class akka.cluster.ClusterCoreDaemon status=0 no children\n   |   |   |   ⌊-> publisher LocalActorRef class akka.cluster.ClusterDomainEventPublisher status=2 no children\n   |   |   ⌊-> heartbeatReceiver LocalActorRef class akka.cluster.ClusterHeartbeatReceiver status=0 no children\n   |   ⌊-> clusterEventBusListener RepointableActorRef class akka.cluster.ClusterReadView$$anonfun$1$$anon$1 status=0 no children\n   |   ⌊-> deadLetterListener RepointableActorRef class akka.event.DeadLetterListener status=0 no children\n   |   ⌊-> endpointManager RepointableActorRef class akka.remote.EndpointManager status=0 no children\n   |   ⌊-> eventStreamUnsubscriber-1 RepointableActorRef class akka.event.EventStreamUnsubscriber status=0 no children\n   |   ⌊-> log1-Logging$DefaultLogger RepointableActorRef class akka.event.Logging$DefaultLogger status=0 no children\n   |   ⌊-> remote-deployment-watcher RepointableActorRef class akka.remote.RemoteDeploymentWatcher status=0 no children\n   |   ⌊-> remote-watcher RepointableActorRef class akka.cluster.ClusterRemoteWatcher status=0 no children\n   |   ⌊-> remoting-terminator RepointableActorRef class akka.remote.RemoteActorRefProvider$RemotingTerminator status=0 no children\n   |   ⌊-> sharding RepointableActorRef class akka.cluster.sharding.ClusterShardingGuardian status=0 2 children\n   |   |   ⌊-> myType LocalActorRef class akka.cluster.sharding.ShardRegion status=0 no children\n   |   |   ⌊-> replicator LocalActorRef class akka.cluster.ddata.Replicator status=2 1 children\n   |   |       ⌊-> durableStore LocalActorRef class akka.cluster.ddata.LmdbDurableStore status=2 no children\n   |   ⌊-> transports RepointableActorRef class akka.remote.Remoting$TransportSupervisor status=0 1 children\n   |       ⌊-> akkaprotocolmanager.tcp0 LocalActorRef class akka.remote.transport.AkkaProtocolManager status=0 no children\n   ⌊-> user LocalActorRef class akka.actor.LocalActorRefProvider$Guardian status=0 no children",
            "url":"https://github.com/akka/akka/issues/23472"
          },
          {
            "number":23499,
            "title":"Perhaps recommend MaxRAMFraction in docs around deployment/docker sections?",
            "bodyText":"A new JVM option has become available recently (in JDK8 too): -J-XX:MaxRAMFraction=1\nwhich allows the JVM be aware of limits set by cgroups onto memory.\nA good combo of options when running Java in containers / cgroups nowadays is: https://twitter.com/analytically/status/894592422382063616\nCould be a good hint to provide to our users in docs around how to deploy things.\nIf someone experienced with containers from the community would like to pick this up that'd be lovely so the wording is right etc :)",
            "url":"https://github.com/akka/akka/issues/23499"
          },
          {
            "number":23500,
            "title":"PubSub Put not working? Documentation coding examples wrong?",
            "bodyText":"Hi Guys!\nI tried to implement Distributed Send PubSub's use case with Akka 2.4.14 and I couldn't get it to work across nodes.\nThe Destination Put's call never got a SubscribeAck (not sure if it should but in the documentation's code it leads you to believe so)\nAlso, perhaps related: I noticed that the code examples where copy pasted from the topics above and perhaps led to errors: like these lines:\n//somewhere else\nActorRef sender = system.actorOf(Props.create(Publisher.class), \"sender\");\n// after a while the destinations are replicated\nsender.tell(\"hello\", null);\n\nAs we made a Destination and a Sender class, the creation of a Publisher looks out of place.\nSo, is the DistributedPubSubMediator.Put call supposed to work across nodes on the same cluster?\nI eventually ended up doing a Publish with groups and sendOneMessageToEachGroup=true for the same effect.\nBut I wasn't the only one with that problem as I found this StackOverflow thread:\nhttps://stackoverflow.com/questions/31536080/akka-distributed-pub-sub-java-implementation-not-working/45566384#45566384\nAre the docs wrong? is the Put supposed to work across the cluster? did the 2.4.14 version (that was the one shipped with Play 2.5.X) have a bug?\nCheers",
            "url":"https://github.com/akka/akka/issues/23500"
          },
          {
            "number":23516,
            "title":"[Docs] HowTo Common Patterns for Scala contains Java code",
            "bodyText":"The HowTo: Common Patterns page for Scala contains Java code examples.\nAdditionally, there are some dangling @@@",
            "url":"https://github.com/akka/akka/issues/23516"
          },
          {
            "number":23543,
            "title":"Dns resolution throws UnknownHostException with only the hostname as message",
            "bodyText":"It should provide more info for the case where the exception name is not printed along with the message as reported in akka/akka-http#1364. Right now error messages from Akka IO / Akka Stream read like this:\nakka.stream.StreamTcpException: Tcp command [Connect(potato:80,None,List(),Some(10 seconds),true)] failed because of potato\n\n\n  \n    \n      akka/akka-actor/src/main/scala/akka/io/Dns.scala\n    \n    \n        Lines 31 to 35\n      in\n      c2ff4f7\n    \n    \n    \n    \n\n        \n          \n           @throws[UnknownHostException] \n        \n\n        \n          \n           def addr: InetAddress = addrOption match { \n        \n\n        \n          \n             case Some(ipAddress) ⇒ ipAddress \n        \n\n        \n          \n             case None            ⇒ throw new UnknownHostException(name) \n        \n\n        \n          \n           }",
            "url":"https://github.com/akka/akka/issues/23543"
          },
          {
            "number":23614,
            "title":"Java AbstractActor documentation issues",
            "bodyText":"http://doc.akka.io/docs/akka/2.5.4/java/actors.html\n\nsetting the \u201cinitial behavior\u201d in the constructor by calling the receive method in the AbstractActor.\n\n\nThe argument to the receive method is\n\n\nthere is a builder named ReceiveBuilder that you can use.\n\n\nPlease note that the Akka Actor receive message loop is exhaustive\n\n\nThe argument to the receive method is\n\n\nThe AbstractActor class defines a method called receive, that is used to set the \u201cinitial behavior\u201d of the actor.\n\nAbove lines were not updated in 2.5, while the provided example is updated.\n\nprovide factory methods on the companion object of each Actor\n\nAbove line is copied from Scala. Java example uses static method.\nAlso, I assume the example in \"Recommended Practices\" could be rewritten with varargs method (as it's mentioned as more safe):\nreturn Props.create(DemoActor.class, magicNumber); instead of  return Props.create(DemoActor.class, () -> new DemoActor(magicNumber));",
            "url":"https://github.com/akka/akka/issues/23614"
          },
          {
            "number":23622,
            "title":"Memory leak when using SharedKillSwitch",
            "bodyText":"Using SharedKillSwitch for multiple materializations of a stream causes a memory leak.\nA SharedKillSwitch instance uses a single promise to signal shutdown. Each materialization of the kill-switch logic installs callbacks on the shared promise. As a result, the SharedKillSwitch instance has a hard link to all the instances of KillableGraphStageLogic. Because graph logic instances link to each other, the SharedKillSwitch prevents the whole graph from being garbage collected. When combined with multiple materialization of a graphs (for example when using a shared kill switch on Tcp() connection streams) this causes a memory leak leading to JVM terminating with OutOfMemoryException.",
            "url":"https://github.com/akka/akka/issues/23622"
          },
          {
            "number":23640,
            "title":"[akka-typed] How to easily create an empty behavior which handles signals?",
            "bodyText":"I'm struggling with creating an empty behavior (i.e. one which doesn't handle any messages) which handles signals.\nMy first naive approach was Actor.empty[Nothing].onSignal(...), but onSignal in only available for immutable and mutable behaviors.\nSo should I really extend ExtensibleBehavior? I guess this use case is quite common, e.g. for the root actor which doesn't handle any messages but watches its child actors. Therefore some easier way would be appreciated.",
            "url":"https://github.com/akka/akka/issues/23640"
          },
          {
            "number":23663,
            "title":"Akka Typed: slf4j logging",
            "bodyText":"Akka typed comes with this config\n\"akka\": {\n    \"typed\" : {\n        \"loggers\" : [\n            \"akka.typed.DefaultLogger\"\n        ],\n        \"logging-filter\" : \"akka.typed.DefaultLoggingFilter\"\n    }\n}\n\nbut there is no slf4j implementation available, using akka.event.slf4j.Slf4jLogger does not work\nFrom #23326 I understand that users should just be using slf4j directly in typed actors? But I guess ctx.system.log.[info,warning,...] is in use by the system itself and would preferably also end up in slf4j?",
            "url":"https://github.com/akka/akka/issues/23663"
          }
        ]
      }
    }
  }
}