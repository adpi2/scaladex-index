<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/dlwh/epic#epic" aria-hidden="true" class="anchor" id="user-content-epic" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Epic</h1> 
  <p>(c) 2014 David Hall.</p> 
  <p>Epic is a structured prediction framework for Scala. It also includes classes for training high-accuracy syntactic parsers, part-of-speech taggers, name entity recognizers, and more.</p> 
  <p>Epic is distributed under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html" target="_blank">Apache License, Version 2.0</a>.</p> 
  <p>The current version is 0.3.</p> 
  <h2><a href="https://github.com/dlwh/epic#documentation" aria-hidden="true" class="anchor" id="user-content-documentation" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Documentation</h2> 
  <p>Documentation will (eventually) live at the GitHub wiki: <a href="https://github.com/dlwh/epic/wiki" target="_blank">https://github.com/dlwh/epic/wiki</a></p> 
  <p>See some example usages at <a href="https://github.com/dlwh/epic-demo" target="_blank">https://github.com/dlwh/epic-demo</a>.</p> 
  <h2><a href="https://github.com/dlwh/epic#using-epic" aria-hidden="true" class="anchor" id="user-content-using-epic" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Using Epic</h2> 
  <p>Epic can be used programmatically or from the command line, using either pretrained models (<a href="https://github.com/dlwh/epic#pre-trained-models" target="_blank">see below</a>) or with models you have trained yourself.</p> 
  <p>Currently, Epic has support for three kinds of models: parsers, sequence labelers, and segmenters. Parsers produce syntactic representations of sentences. Sequence labelers are things like part-of-speech taggers. These associate each word in a sentence with a label. For instance, a part-of-speech tagger can identify nouns, verbs, etc. Segmenters break a sentence into a sequence of fields. For instance, a named entity recognition system might identify all the people, places and things in a sentence.</p> 
  <h3><a href="https://github.com/dlwh/epic#command-line-usage" aria-hidden="true" class="anchor" id="user-content-command-line-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Command-line Usage</h3> 
  <p>Epic bundles command line interfaces for using parsers, NER systems, and POS taggers (and more generally, segmentation and tagging systems). There are three classes, one for each kind of system:</p> 
  <ul> 
   <li><code>epic.parser.ParseText</code> runs a parser.</li> 
   <li><code>epic.sequences.SegmentText</code> runs an NER system, or any kind of segmentation system.</li> 
   <li><code>epic.sequences.TagText</code> runs a POS tagger, or any kind of tagging system.</li> 
  </ul> 
  <p>All of these systems expect plain text files as input, along with a path to a model file. The syntax is:</p> 
  <div class="highlight highlight-source-shell">
   <pre>java -Xmx4g -cp /path/to/epic-assembly-0.3-SNAPSHOT.jar epic.parser.ParseText --model /path/to/model.ser.gz --nthreads <span class="pl-k">&lt;</span>number of threads<span class="pl-k">&gt;</span> [files]</pre>
  </div> 
  <p>Currently, all text is output to standard out. In the future, we will support output in a way that differentiates the files. If no files are given, the system will read from standard input. By default, the system will use all available cores for execution.</p> 
  <p>Models can be downloaded from <a href="http://www.scalanlp.org/models/" target="_blank">http://www.scalanlp.org/models/</a> or from Maven Central. (<a href="https://github.com/dlwh/epic#pre-trained-models" target="_blank">See below</a>.)</p> 
  <h3><a href="https://github.com/dlwh/epic#programmatic-usage" aria-hidden="true" class="anchor" id="user-content-programmatic-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Programmatic Usage</h3> 
  <p>Epic also supports programmatic usage. All of the models assume that text has been segmented and tokenized.</p> 
  <h4><a href="https://github.com/dlwh/epic#preprocessing-text" aria-hidden="true" class="anchor" id="user-content-preprocessing-text" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Preprocessing text</h4> 
  <p>To preprocess text so that the models can use them, you will need to segment out sentences and tokenize the sentences into individual words. Epic comes with classes to do both.</p> 
  <p>Once you have a sentence, you can tokenize it using a <code>epic.preprocess.TreebankTokenizer</code>, which takes a string and returns a sequence of tokens. All told, the pipeline looks like this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">text</span> <span class="pl-k">=</span> getSomeText();

<span class="pl-k">val</span> <span class="pl-en">sentenceSplitter</span> <span class="pl-k">=</span> <span class="pl-en">MLSentenceSegmenter</span>.bundled().get
<span class="pl-k">val</span> <span class="pl-en">tokenizer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">epic.preprocess.TreebankTokenizer</span>()

<span class="pl-k">val</span> <span class="pl-en">sentences</span><span class="pl-k">:</span> <span class="pl-en">IndexedSeq</span>[<span class="pl-en">IndexedSeq</span>[<span class="pl-k">String</span>]] <span class="pl-k">=</span> sentenceSplitter(text).map(tokenizer).toIndexedSeq

<span class="pl-k">for</span>(sentence <span class="pl-k">&lt;</span><span class="pl-k">-</span> sentences) {
  <span class="pl-c"><span class="pl-c">//</span> use the sentence tokens</span>
}
</pre>
  </div> 
  <h4><a href="https://github.com/dlwh/epic#parser" aria-hidden="true" class="anchor" id="user-content-parser" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Parser</h4> 
  <p>To use the parser programmaticaly, deserialize a parser model--either using <code>epic.models.deserialize[Parser[AnnotatedLabel, String]](path)</code> or using the ParserSelector. Then, give the parser segmented and tokenized text:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">parser</span> <span class="pl-k">=</span> epic.models.deserialize[<span class="pl-en">Parser</span>[<span class="pl-en">AnnotataedLabel</span>, <span class="pl-k">String</span>]](path)

<span class="pl-c"><span class="pl-c">//</span> or:</span>

<span class="pl-k">val</span> <span class="pl-en">parser</span> <span class="pl-k">=</span> epic.models.<span class="pl-en">ParserSelector</span>.loadParser(<span class="pl-s"><span class="pl-pds">"</span>en<span class="pl-pds">"</span></span>).get <span class="pl-c"><span class="pl-c">//</span> or another 2 letter code.</span>

<span class="pl-k">val</span> <span class="pl-en">tree</span> <span class="pl-k">=</span> parser(sentence)

println(tree.render(sentence))
</pre>
  </div> 
  <p>Trees have a number of methods on them. See the class definition or <a href="http://www.scalanlp.org/api/epic" target="_blank">API docs</a>.</p> 
  <h4><a href="https://github.com/dlwh/epic#part-of-speech-tagger" aria-hidden="true" class="anchor" id="user-content-part-of-speech-tagger" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Part-of-Speech Tagger</h4> 
  <p>Using a Part-of-Speech tagger is similar to using a parser: load a model, tokenize some text, run the tagger. All taggers are (currently) <a href="http://people.cs.umass.edu/%7Emccallum/papers/crf-tutorial.pdf" target="_blank">linear chain conditional random fields</a>, or CRFs. (You don't need to understand them to use them. They are just a machine learning method for assigning a sequence of tags to a sequence of words.)</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">tagger</span> <span class="pl-k">=</span> epic.models.deserialize[<span class="pl-en">CRF</span>[<span class="pl-en">AnnotatedLabel</span>, <span class="pl-k">String</span>]](path)

<span class="pl-c"><span class="pl-c">//</span> or:</span>

<span class="pl-k">val</span> <span class="pl-en">tagger</span> <span class="pl-k">=</span> epic.models.<span class="pl-en">PosTagSelector</span>.loadTagger(<span class="pl-s"><span class="pl-pds">"</span>en<span class="pl-pds">"</span></span>).get <span class="pl-c"><span class="pl-c">//</span> or another 2 letter code.</span>

<span class="pl-k">val</span> <span class="pl-en">tags</span> <span class="pl-k">=</span> tagger.bestSequence(sentence)

println(tags.render)
</pre>
  </div> 
  <h4><a href="https://github.com/dlwh/epic#named-entity-recognition" aria-hidden="true" class="anchor" id="user-content-named-entity-recognition" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Named Entity Recognition</h4> 
  <p>Using a named entity recognizer is similar to using a pos tagger: load a model, tokenize some text, run the recognizer. All NER systems are (currently) <a href="http://people.cs.umass.edu/%7Emccallum/papers/crf-tutorial.pdf" target="_blank">linear chain semi-Markov conditional random fields</a>, or SemiCRFs. (You don't need to understand them to use them. They are just a machine learning method for segmenting text into fields.)</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">ner</span> <span class="pl-k">=</span> epic.models.deserialize[<span class="pl-en">SemiCRF</span>[<span class="pl-en">AnnotatedLabel</span>, <span class="pl-k">String</span>]](path)

<span class="pl-c"><span class="pl-c">//</span> or:</span>

<span class="pl-k">val</span> <span class="pl-en">ner</span> <span class="pl-k">=</span> epic.models.<span class="pl-en">NerSelector</span>.loadNer(<span class="pl-s"><span class="pl-pds">"</span>en<span class="pl-pds">"</span></span>).get<span class="pl-c"><span class="pl-c">//</span> or another 2 letter code.</span>

<span class="pl-k">val</span> <span class="pl-en">segments</span> <span class="pl-k">=</span> ner.bestSequence(sentence)

println(segments.render)
</pre>
  </div> 
  <p>The outside label of a SemiCRF is the label that is consider not part of a "real" segment. For instance, in NER, it is the label given to words that are not named entities.</p> 
  <h3><a href="https://github.com/dlwh/epic#pre-trained-models" aria-hidden="true" class="anchor" id="user-content-pre-trained-models" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Pre-trained Models</h3> 
  <p>Epic provides a number of pre-trained models. These are available as Maven artifacts from Maven Central, and can be loaded at runtime. To use a specific model, just depend on it (or alternatively download the jar file). You can then load the parser by calling, for example:</p> 
  <div class="highlight highlight-source-scala">
   <pre>epic.parser.models.en.span.<span class="pl-en">EnglishSpanParser</span>.load()</pre>
  </div> 
  <p>This will load the model and return a <code>Parser</code> object. If you want to not hardwire dependencies, either for internationalization or to potentially try different models, use <code>epic.models.ParserSelector.loadParser(language)</code>, where language is the <a href="http://www.loc.gov/standards/iso639-2/php/code_list.php" target="_blank">two letter code for the language</a> you want to use.</p> 
  <p>To following models are available at this time:</p> 
  <p><strong>AS OF WRITING ONLY MODELS FOR ENGLISH ARE AVAILABLE!</strong> Write me if you want these other models.</p> 
  <ul> 
   <li>Parser 
    <ul> 
     <li>English: <pre><code>"org.scalanlp" %% "epic-parser-en-span" % "2015.1.25"
</code></pre> </li> 
    </ul> </li> 
   <li>POS Taggers 
    <ul> 
     <li>English: <pre><code>"org.scalanlp" %% "epic-pos-en" % "2015.1.25"
</code></pre> </li> 
    </ul> </li> 
   <li>Named Entity Recognizers 
    <ul> 
     <li>English: <pre><code>"org.scalanlp" %% "epic-ner-en-conll" % "2015.1.25"
</code></pre> </li> 
    </ul> </li> 
  </ul> 
  <p>There is also a meta-dependency that includes the above three models:</p> 
  <pre><code>"org.scalanlp" %% "english"  % "2015.1.25"
</code></pre> 
  <p>I meant to name that "epic-english" but messed up. So it's that for now. Expect it to change.</p> 
  <p>TODO:</p> 
  <ul> 
   <li>Parser 
    <ul> 
     <li>English: <pre><code>"org.scalanlp" %% "epic-parser-en-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Basque: <pre><code>"org.scalanlp" %% "epic-parser-eu-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>French: <pre><code>"org.scalanlp" %% "epic-parser-fr-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>German: <pre><code>"org.scalanlp" %% "epic-parser-de-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Hungarian: <pre><code>"org.scalanlp" %% "epic-parser-hu-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Korean: <pre><code>"org.scalanlp" %% "epic-parser-ko-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Polish: <pre><code>"org.scalanlp" %% "epic-parser-pl-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Swedish: <pre><code>"org.scalanlp" %% "epic-parser-sv-span" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
    </ul> </li> 
   <li>POS Taggers 
    <ul> 
     <li>Basque: <pre><code>"org.scalanlp" %% "epic-pos-eu" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>French: <pre><code>"org.scalanlp" %% "epic-pos-fr" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>German: <pre><code>"org.scalanlp" %% "epic-pos-de" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Hungarian: <pre><code>"org.scalanlp" %% "epic-pos-hu" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Polish: <pre><code>"org.scalanlp" %% "epic-pos-pl" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
     <li>Swedish: <pre><code>"org.scalanlp" %% "epic-pos-sv" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
    </ul> </li> 
   <li>Named Entity Recognizers 
    <ul> 
     <li>English: <pre><code>"org.scalanlp" %% "epic-ner-en-conll" % "2014.9.15-SNAPSHOT"
</code></pre> </li> 
    </ul> </li> 
  </ul> 
  <p>If you use any of the parser models in research publications, please cite:</p> 
  <blockquote> 
   <p>David Hall, Greg Durrett, and Dan Klein. 2014. Less Grammar, More Features. In ACL.</p> 
  </blockquote> 
  <p>If you use the other things, just link to Epic.</p> 
  <h2><a href="https://github.com/dlwh/epic#building-epic" aria-hidden="true" class="anchor" id="user-content-building-epic" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Building Epic</h2> 
  <p>In order to do anything besides use pre-trained models, you will probably need to build Epic.</p> 
  <p>To build, you need a release of <a href="http://www.scala-sbt.org/0.13.2/docs/Getting-Started/Setup.html" target="_blank">SBT 0.13.2</a></p> 
  <p>then run</p> 
  <pre>$ sbt assembly
</pre> 
  <p>which will compile everything, run tests, and build a fatjar that includes all dependencies.</p> 
  <h2><a href="https://github.com/dlwh/epic#training-models" aria-hidden="true" class="anchor" id="user-content-training-models" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training Models</h2> 
  <h3><a href="https://github.com/dlwh/epic#training-parsers" aria-hidden="true" class="anchor" id="user-content-training-parsers" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training Parsers</h3> 
  <p>There are several different discriminative parsers you can train, and the trainer main class has lots of options. To get a sense of them, run the following command:</p> 
  <pre>$ java -cp target/scala-2.10/epic-assembly-0.2-SNAPSHOT.jar epic.parser.models.ParserTrainer --help
</pre> 
  <p>You'll get a list of all the available options (so many!) The important ones are:</p> 
  <pre>--treebank.path "path/to/treebank"
--cache.path "constraint.cache"
--modelFactory  XXX                              # the kind of parser to train. See below.
--opt.useStochastic true                         # turn on stochastic gradient
--opt.regularization 1.0                         # regularization constant. you need to regularize, badly.
</pre> 
  <p>There are 4 kinds of base models you can train, and you can tie them together with an <code>EPParserModel</code>, if you want. The 4 base models are:</p> 
  <ul> 
   <li>epic.parser.models.LatentModelFactory: Latent annotation (like the Berkeley parser)</li> 
   <li>epic.parser.models.LexModelFactory: Lexical annotation (kind of like the Collins parser)</li> 
   <li>epic.parser.models.StructModelFactory: Structural annotation (kind of like the Stanford parser)</li> 
   <li>epic.parser.models.SpanModelFactory: Span features (Hall, Durrett, and Klein, 2014)</li> 
  </ul> 
  <p>These models all have their own options. You can see those by specifying the modelFactory and adding --help:</p> 
  <pre>$ java -cp target/scala-2.10/epic-assembly-0.2-SNAPSHOT.jar epic.parser.models.ParserTrainer --modelFactory "model" --help
</pre> 
  <p>If you use the first three in research papers, please cite</p> 
  <blockquote> 
   <p>David Hall and Dan Klein. 2012. Training Factored PCFGs with Expectation Propagation. In EMNLP.</p> 
  </blockquote> 
  <p>If you use the <code>SpanModel</code>, please cite:</p> 
  <blockquote> 
   <p>David Hall, Greg Durrett, and Dan Klein. 2014. Less Grammar, More Features. In ACL.</p> 
  </blockquote> 
  <p>If you use something else, cite one of these, or something.</p> 
  <p>For training a SpanModel, the following configurations are known to work well in general:</p> 
  <ul> 
   <li>English:</li> 
  </ul> 
  <div class="highlight highlight-source-shell">
   <pre>epic.parser.models.ParserTrainer \
  --modelFactory epic.parser.models.SpanModelFactory \
  --cache.path constraints.cache \
  --opt.useStochastic \
  --opt.regularization 5 \
  --opt.batchSize 500 \
  --alpha 0.1 \
  --maxIterations 1000 \
  --trainer.modelFactory.annotator epic.trees.annotations.PipelineAnnotator \
  --ann.0 epic.trees.annotations.FilterAnnotations \
  --ann.1 epic.trees.annotations.ForgetHeadTag \
  --ann.2 epic.trees.annotations.Markovize \
  --vertical 1 \
  --horizontal 0 \
  --treebank.path /home/dlwh/wsj/</pre>
  </div> 
  <ul> 
   <li>Other (SPMRL languages):</li> 
  </ul> 
  <div class="highlight highlight-source-shell">
   <pre>epic.parser.models.ParserTrainer \
  --treebankType spmrl \
  --binarization head \
  --modelFactory epic.parser.models.SpanModelFactory \
  --opt.useStochastic --opt.regularization 5.0 \
  --opt.batchSize 400 --maxIterations 502 \
  --iterationsPerEval 100 \
  --alpha 0.1 \
  --trainer.modelFactory.annotator epic.trees.annotations.PipelineAnnotator \
  --ann.0 epic.trees.annotations.FilterAnnotations  \
  --ann.1 epic.trees.annotations.ForgetHeadTag \
  --ann.2 epic.trees.annotations.Markovize \
  --ann.2.vertical 1 \
  --ann.2.horizontal 0 \
  --ann.3 epic.trees.annotations.SplitPunct \
  --cache.path <span class="pl-smi">$languc</span>-constraints.cache \
  --treebank.path <span class="pl-smi">${SPMRL}</span>/<span class="pl-smi">${languc}</span>_SPMRL/gold/ptb/ \
  --supervisedHeadFinderPtbPath <span class="pl-smi">${SPMRL}</span>/<span class="pl-smi">${languc}</span>_SPMRL/gold/ptb/<span class="pl-smi">${train}</span>/<span class="pl-smi">${train}</span>.<span class="pl-smi">$lang</span>.gold.ptb \
  --supervisedHeadFinderConllPath <span class="pl-smi">${SPMRL}</span>/<span class="pl-smi">${languc}</span>_SPMRL/gold/conll/<span class="pl-smi">${train}</span>/<span class="pl-smi">${train}</span>.<span class="pl-smi">$lang</span>.gold.conll \
  --threads 8 </pre>
  </div> 
  <p>Training a parser currently needs four files that are cached to the pwd:</p> 
  <ul> 
   <li><code>xbar.gr</code>: caches the topology of the grammar</li> 
   <li><code>constraints.cache</code>, <code>constraints.cache.*</code>: remembers pruning masks computed from the base grammar.</li> 
  </ul> 
  <p>TODO: remove this reliance.</p> 
  <h4><a href="https://github.com/dlwh/epic#treebank-types" aria-hidden="true" class="anchor" id="user-content-treebank-types" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Treebank types</h4> 
  <p>There is a <code>treebank.type</code> commandline flag that supports a few different formats for treebanks. They are:</p> 
  <ul> 
   <li><code>penn</code>: Reads from the <code>wsj/</code> subdirectory of the Penn Treebank. This expects a set of directories 00-24, each of which contains a number of <code>mrg</code> files. Standard splits are used.</li> 
   <li><code>chinese</code>: Expects a number of chtbNN.mrg files in a single directory.</li> 
   <li><code>negra</code>: Expects a directory with three files, <code>negra_[1-3].mrg</code></li> 
   <li><code>conllonto</code>: Expects data formatted like the 2011 CoNLL shared task. Only reads the trees.</li> 
   <li><code>spmrl</code>: Expects a directory layout like that used in the 2012 SPMRL shared task.</li> 
   <li><code>simple</code>: Expects a directory with 3 files: <code>{train, dev, test}.txt</code></li> 
  </ul> 
  <h5><a href="https://github.com/dlwh/epic#training-a-parser-programmatically" aria-hidden="true" class="anchor" id="user-content-training-a-parser-programmatically" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training a parser programmatically</h5> 
  <p>You can also train a span model programmatically, by using the <code>SpanModelFactory.buildSimple</code> method. For example:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-en">SpanModelFactory</span>.buildSimple(trees, <span class="pl-en">OptParams</span>(regularization<span class="pl-k">=</span><span class="pl-c1">1.0</span>, useStochastic <span class="pl-k">=</span> <span class="pl-c1">true</span>))</pre>
  </div> 
  <p>The build simple model also supports using custom featurizers.</p> 
  <h4><a href="https://github.com/dlwh/epic#training-pos-taggers-and-other-sequence-models" aria-hidden="true" class="anchor" id="user-content-training-pos-taggers-and-other-sequence-models" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training POS taggers and other sequence models</h4> 
  <p>The main class <code>epic.sequences.TrainPosTagger</code> can be used to train a POS Tagger from a treebank. It expects the same treebank options (namely <code>treebank.path</code> and <code>treebank.type</code>) as the Parser trainer does, as well as the same optimization options.</p> 
  <p>The following configuration is known to work well:</p> 
  <ul> 
   <li>English:</li> 
  </ul> 
  <div class="highlight highlight-source-shell">
   <pre>     epic.sequences.TrainPosTagger \
     --treebank.path <span class="pl-smi">$PATH_TO</span>/wsj \
     --opt.regularization 2.0 \
     --useStochastic \
     --maxIterations 1000</pre>
  </div> 
  <ul> 
   <li>Others (SPMRL):</li> 
  </ul> 
  <div class="highlight highlight-source-shell">
   <pre>  epic.sequences.TrainPosTagger --opt.regularization 2.0 --useStochastic --maxIterations 1000 \
  --treebankType spmrl \
  --binarization left \
  --treebank.path <span class="pl-smi">${SPMRL}</span>/<span class="pl-smi">${languc}</span>_SPMRL/gold/ptb/</pre>
  </div> 
  <p>If you want to train other kinds of models, you will probably need to build CRFs programmatically. For inspiration, you should probably look at the source code for TrainPosTagger. It's wonderfully short:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">object</span> <span class="pl-en">TrainPosTagger</span> <span class="pl-k">extends</span> <span class="pl-e">LazyLogging</span> {
  <span class="pl-k">case</span> <span class="pl-k">class</span> <span class="pl-en">Params</span>(<span class="pl-v">opt</span>: <span class="pl-en">OptParams</span>, <span class="pl-v">treebank</span>: <span class="pl-en">ProcessedTreebank</span>, <span class="pl-v">hashFeatureScale</span>: <span class="pl-k">Double</span> <span class="pl-k">=</span> <span class="pl-c1">0.00</span>)

  <span class="pl-k">def</span> <span class="pl-en">main</span>(<span class="pl-v">args</span>: <span class="pl-en">Array</span>[<span class="pl-k">String</span>]) {
    <span class="pl-k">val</span> <span class="pl-en">params</span> <span class="pl-k">=</span> <span class="pl-en">CommandLineParser</span>.readIn[<span class="pl-en">Params</span>](args)
    logger.info(<span class="pl-s"><span class="pl-pds">"</span>Command line arguments for recovery:<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-en">Configuration</span>.fromObject(params).toCommandLineString)
    <span class="pl-k">import</span> <span class="pl-v">params.</span><span class="pl-v">_</span>
    <span class="pl-k">val</span> <span class="pl-en">train</span> <span class="pl-k">=</span> treebank.trainTrees.map(_.asTaggedSequence)
    <span class="pl-k">val</span> <span class="pl-en">test</span> <span class="pl-k">=</span> treebank.devTrees.map(_.asTaggedSequence)

    <span class="pl-k">val</span> <span class="pl-en">crf</span> <span class="pl-k">=</span> <span class="pl-en">CRF</span>.buildSimple(train, <span class="pl-en">AnnotatedLabel</span>(<span class="pl-s"><span class="pl-pds">"</span>TOP<span class="pl-pds">"</span></span>), opt <span class="pl-k">=</span> opt, hashFeatures <span class="pl-k">=</span> hashFeatureScale)

    <span class="pl-k">val</span> <span class="pl-en">stats</span> <span class="pl-k">=</span> <span class="pl-en">TaggedSequenceEval</span>.eval(crf, test)
    println(<span class="pl-s"><span class="pl-pds">"</span>Final Stats: <span class="pl-pds">"</span></span> <span class="pl-k">+</span> stats)
    println(<span class="pl-s"><span class="pl-pds">"</span>Confusion Matrix:<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> <span class="pl-k">+</span> stats.confusion)

  }

}</pre>
  </div> 
  <p>Basically, you need to create a collection of TaggedSequences, which is a pair of sequences, one for tags and one for words. Then pass in the training data to <code>CRF.buildSimple</code>, along with a start symbol (used for the "beginning of sentence" tag), an optional <a href="https://github.com/dlwh/epic#gazetteer" target="_blank">Gazetteer</a> (not shown), and an <a href="https://github.com/dlwh/epic#optparams" target="_blank">OptParams</a>, which is used to control the optimization. There is also an optional hashFeatures argument, which isn't used.</p> 
  <p>We can also pass in two [<code>WordFeaturizer</code>] instances, one for "label" features, and one for "transition" features. Most of the featurizers in Epic have a cross product form <code>(Label x Surface)</code>, where <code>Label</code> is a feature on the label (e.g. the pos tag) and the <code>Surface</code> feature is a feature on the surface string.Here, the label featurizer features are crossed with the tag, and the transition featurizer features are crossed with pairs of sucessive labels. See the wiki page on [[Featurizers]] for more detail.</p> 
  <h3><a href="https://github.com/dlwh/epic#training-ner-systems-and-other-segmentation-models" aria-hidden="true" class="anchor" id="user-content-training-ner-systems-and-other-segmentation-models" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training NER systems and other segmentation models</h3> 
  <p>Training an NER system or other SemiCRF is very similar to training a CRF. The main difference is that the inputs are Segmentations, rather than TaggedSequences. The main class <code>epic.sequences.SemiConllNERPipeline</code> can be used to train NER models, with data in the <a href="http://www.cnts.ua.ac.be/conll2003/ner/" target="_blank">CoNLL 2003 shared task format</a>. This class completely ignores all fields except the first and last. The commandline takes two paths, <code>--train</code> and <code>--test</code>, to specify training and test set files, respectively.</p> 
  <p>If you need to do something more complicated, you will need to write your own code. As an example, here is the code for <code>epic.sequences.SemiConllNERPipeline</code>. This code is somewhat more complicated, as the CoNLL sequences need to be turned into segmentations.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">def</span> <span class="pl-en">main</span>(<span class="pl-v">args</span>: <span class="pl-en">Array</span>[<span class="pl-k">String</span>]) {
    <span class="pl-k">val</span> <span class="pl-en">params</span><span class="pl-k">:</span><span class="pl-en">Params</span> <span class="pl-k">=</span> <span class="pl-en">CommandLineParser</span>.readIn[<span class="pl-en">Params</span>](args)
    logger.info(<span class="pl-s"><span class="pl-pds">"</span>Command line arguments for recovery:<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-en">Configuration</span>.fromObject(params).toCommandLineString)
    <span class="pl-k">val</span> (train,test) <span class="pl-k">=</span> {
      <span class="pl-k">val</span> <span class="pl-en">standardTrain</span> <span class="pl-k">=</span> <span class="pl-en">CONLLSequenceReader</span>.readTrain(<span class="pl-k">new</span> <span class="pl-en">FileInputStream</span>(params.path), params.path.getName).toIndexedSeq
      <span class="pl-k">val</span> <span class="pl-en">standardTest</span> <span class="pl-k">=</span> <span class="pl-en">CONLLSequenceReader</span>.readTrain(<span class="pl-k">new</span> <span class="pl-en">FileInputStream</span>(params.test), params.path.getName).toIndexedSeq

      standardTrain.take(params.nsents).map(makeSegmentation) <span class="pl-k">-</span><span class="pl-k">&gt;</span> standardTest.map(makeSegmentation)
    }


    <span class="pl-c"><span class="pl-c">//</span> you can optionally pass in an a Gazetteer, though I've not gotten much mileage with them.</span>
    <span class="pl-k">val</span> <span class="pl-en">crf</span> <span class="pl-k">=</span> <span class="pl-en">SemiCRF</span>.buildSimple(train, <span class="pl-s"><span class="pl-pds">"</span>--BEGIN--<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>O<span class="pl-pds">"</span></span>, params.opt)

    <span class="pl-k">val</span> <span class="pl-en">stats</span> <span class="pl-k">=</span> <span class="pl-en">SegmentationEval</span>.eval(crf, test)

    println(stats)


  }</pre>
  </div> 
  <p>We can also pass in featurizers, like in the CRF trainer. In this case, we can pass in a [<code>WordFeaturizer</code>] and a [<code>SpanFeaturizer</code>]. WordFeatures are like before, while SpanFeaturizer give features over the entire input span. For NER, this can be useful for adding features noting that an entity is entirely surrounded by quotation marks, for instance, or for matching against entries in a [[Gazetteer]].</p> 
  <h3><a href="https://github.com/dlwh/epic#optparams" aria-hidden="true" class="anchor" id="user-content-optparams" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>OptParams</h3> 
  <p>OptParams is a configuration class that controls the optimizer. There are a bunch of different options:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">--</span>opt.batchSize<span class="pl-k">:</span> <span class="pl-k">Int</span> <span class="pl-k">=</span> <span class="pl-c1">512</span>                                                                                                                                                                                                                 
<span class="pl-k">--</span>opt.regularization<span class="pl-k">:</span> <span class="pl-k">Double</span> <span class="pl-k">=</span> <span class="pl-c1">0.0</span>                                                                                                                                                                                                         
<span class="pl-k">--</span>opt.alpha<span class="pl-k">:</span> <span class="pl-k">Double</span> <span class="pl-k">=</span> <span class="pl-c1">0.5</span>                                                                                                                                                                                                                  
<span class="pl-k">--</span>opt.maxIterations<span class="pl-k">:</span> <span class="pl-k">Int</span> <span class="pl-k">=</span> <span class="pl-c1">1000</span>                                                                                                                                                                                                            
<span class="pl-k">--</span>opt.useL1<span class="pl-k">:</span> <span class="pl-k">Boolean</span> <span class="pl-k">=</span> <span class="pl-c1">false</span>                                                                                                                                                                                                               
<span class="pl-k">--</span>opt.tolerance<span class="pl-k">:</span> <span class="pl-k">Double</span> <span class="pl-k">=</span> <span class="pl-c1">1.0E-5</span>                                                                                                                                                                                                           
<span class="pl-k">--</span>opt.useStochastic<span class="pl-k">:</span> <span class="pl-k">Boolean</span> <span class="pl-k">=</span> <span class="pl-c1">false</span>                                                                                                                                                                                                       
<span class="pl-k">--</span>opt.randomSeed<span class="pl-k">:</span> <span class="pl-k">Int</span> <span class="pl-k">=</span> <span class="pl-c1">0</span>     </pre>
  </div> 
  <p>Regularization is generally very important. Using a value of 1.0 usually works pretty well. 5.0 works better on the SpanModel for parsing. <code>useStochastic</code> turns on stochastic gradient descent (rather than full batch optimization). It makes training much faster, usually.</p> 
 </article>
</div>