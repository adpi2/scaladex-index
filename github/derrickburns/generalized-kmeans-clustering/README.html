<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <p><a href="https://waffle.io/derrickburns/generalized-kmeans-clustering" target="_blank"><img src="https://camo.githubusercontent.com/84b36942c66684e976dda91300b8a0c81c4467ad/68747470733a2f2f62616467652e776166666c652e696f2f6465727269636b6275726e732f67656e6572616c697a65642d6b6d65616e732d636c7573746572696e672e706e673f6c6162656c3d7265616479267469746c653d5265616479" alt="Stories in Ready" data-canonical-src="https://badge.waffle.io/derrickburns/generalized-kmeans-clustering.png?label=ready&amp;title=Ready" style="max-width:100%;"></a> <a href="https://travis-ci.org/derrickburns/generalized-kmeans-clustering" target="_blank"><img src="https://camo.githubusercontent.com/be5483c6440625c17cf959fde06d0c5487b76c4b/68747470733a2f2f7472617669732d63692e6f72672f6465727269636b6275726e732f67656e6572616c697a65642d6b6d65616e732d636c7573746572696e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/derrickburns/generalized-kmeans-clustering.svg?branch=master" style="max-width:100%;"></a> <a href="https://www.codacy.com/public/derrickrburns/generalized-kmeans-clustering" target="_blank"><img src="https://camo.githubusercontent.com/3c6b9141aceffbf92b0fc92d53a47ec146cb165b/68747470733a2f2f7777772e636f646163792e636f6d2f70726f6a6563742f62616467652f3433333761336534366334393438316462623232303337646537646437656163" alt="Codacy Badge" data-canonical-src="https://www.codacy.com/project/badge/4337a3e46c49481dbb22037de7dd7eac" style="max-width:100%;"></a> <a href="https://bintray.com/derrickburns/maven/massivedatascience-clusterer/_latestVersion" target="_blank"><img src="https://camo.githubusercontent.com/0cf28e501cd50f94ebb5eb82880a9bc81d5fd3cd/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f6465727269636b6275726e732f6d6176656e2f6d61737369766564617461736369656e63652d636c757374657265722f696d616765732f646f776e6c6f61642e737667" alt="Download" data-canonical-src="https://api.bintray.com/packages/derrickburns/maven/massivedatascience-clusterer/images/download.svg" style="max-width:100%;"> </a> <a href="https://coveralls.io/r/derrickburns/generalized-kmeans-clustering?branch=master" target="_blank"><img src="https://camo.githubusercontent.com/847301924c19a1ef1670f29cb6e63840d8794c8c/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6465727269636b6275726e732f67656e6572616c697a65642d6b6d65616e732d636c7573746572696e672f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/derrickburns/generalized-kmeans-clustering/badge.svg?branch=master" style="max-width:100%;"></a></p> 
  <h1><a id="user-content-generalized-k-means-clustering" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#generalized-k-means-clustering" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Generalized K-Means Clustering</h1> 
  <p>This project generalizes the Spark MLLIB Batch K-Means (v1.1.0) clusterer and the Spark MLLIB Streaming K-Means (v1.2.0) clusterer. Most practical variants of K-means clustering are implemented or can be implemented with this package, including:</p> 
  <ul> 
   <li><a href="http://www.cs.utexas.edu/users/inderjit/public_papers/bregmanclustering_jmlr.pdf" target="_blank">clustering using general distance functions (Bregman divergences)</a></li> 
   <li><a href="http://www.eecs.tufts.edu/%7Edsculley/papers/fastkmeans.pdf" target="_blank">clustering large numbers of points using mini-batches</a></li> 
   <li><a href="http://www.ida.liu.se/%7Earnjo/papers/pakdd-ws-11.pdf" target="_blank">clustering high dimensional Euclidean data</a></li> 
   <li><a href="http://www.cs.gmu.edu/%7Ejessica/publications/ikmeans_sdm_workshop03.pdf" target="_blank">clustering high dimensional time series data</a></li> 
   <li><a href="http://snowbird.djvuzone.org/2009/abstracts/127.pdf" target="_blank">clustering using symmetrized Bregman divergences</a></li> 
   <li><a href="http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf" target="_blank">clustering via bisection</a></li> 
   <li><a href="http://theory.stanford.edu/%7Esergei/papers/vldb12-kmpar.pdf" target="_blank">clustering with near-optimality</a></li> 
   <li><a href="http://papers.nips.cc/paper/3812-streaming-k-means-approximation.pdf" target="_blank">clustering streaming data</a></li> 
  </ul> 
  <p>If you find a novel variant of k-means clustering that is provably superior in some manner, implement it using the package and send a pull request along with the paper analyzing the variant!</p> 
  <p>This code has been tested on data sets of tens of millions of points in a 700+ dimensional space using a variety of distance functions. Thanks to the excellent core Spark implementation, it rocks!</p> 
  <h1><a id="user-content-table-of-contents" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#table-of-contents" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Table of Contents</h1> 
  <ul> 
   <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#generalized-k-means-clustering" target="_blank">Generalized K-Means Clustering</a> 
    <ul> 
     <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#getting-started" target="_blank">Getting Started</a> 
      <ul> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#sbt" target="_blank">SBT</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#maven" target="_blank">Maven</a></li> 
      </ul></li> 
     <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#introduction" target="_blank">Introduction</a> 
      <ul> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#bregman-divergences" target="_blank">Bregman Divergences</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#compute-bregman-distances-efficiently-using-bregmanpoints--and-bregmancenters" target="_blank">Compute Bregman Distances Efficiently using <code>BregmanPoint</code>s and <code>BregmanCenter</code>s</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#representing-k-means-models" target="_blank">Representing K-Means Models</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-using-clusterers" target="_blank">Constructing K-Means Models using Clusterers</a></li> 
      </ul></li> 
     <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-via-lloyds-algorithm" target="_blank">Constructing K-Means Models via Lloyd's Algorithm</a> 
      <ul> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-on-weightedvectors" target="_blank">Constructing K-Means Models on <code>WeightedVector</code>s</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-iteratively" target="_blank">Constructing K-Means Models Iteratively</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#seeding-the-set-of-cluster-centers" target="_blank">Seeding the Set of Cluster Centers</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#iterative-clustering" target="_blank">Iterative Clustering</a></li> 
      </ul></li> 
     <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#creating-a-custom-k-means-clusterer" target="_blank">Creating a Custom K-means Clusterer</a> 
      <ul> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#custom-bregmandivergence" target="_blank">Custom <code>BregmanDivergence</code></a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#custom-bregmanpointops" target="_blank">Custom <code>BregmanPointOps</code></a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#custom-embeddings" target="_blank">Custom <code>Embedding</code></a></li> 
      </ul></li> 
     <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#creating-k-means-models-using-the-kmeansmodel-helper-object" target="_blank">Creating K-Means Models using the <code>KMeansModel</code> Helper Object</a></li> 
     <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#other-differences-with-spark-mllib-12-k-means-clusterer" target="_blank">Other Differences with Spark MLLIB 1.2 K-Means Clusterer</a> 
      <ul> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#variable-number-of-clusters-in-parallel-runs" target="_blank">Variable number of clusters in parallel runs</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#sparse-data" target="_blank">Sparse Data</a></li> 
       <li><a href="https://github.com/derrickburns/generalized-kmeans-clustering#cluster-backfilling" target="_blank">Cluster Backfilling</a></li> 
      </ul></li> 
    </ul></li> 
  </ul> 
  <h3><a id="user-content-getting-started" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#getting-started" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Getting Started</h3> 
  <p>The massivedatascience-clusterer project is built for both Scala 2.10.x and 2.11.x against Spark v1.2.0.</p> 
  <h4><a id="user-content-sbt" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#sbt" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SBT</h4> 
  <p><a href="https://bintray.com/derrickburns/maven/massivedatascience-clusterer/view?source=watch" alt="Get automatic notifications about new &quot;massivedatascience-clusterer&quot; versions" target="_blank"><img src="https://camo.githubusercontent.com/b387cc1b97e5edacfb25f46471c1847f8eab90fa/68747470733a2f2f7777772e62696e747261792e636f6d2f646f63732f696d616765732f62696e747261795f62616467655f636f6c6f722e706e67" data-canonical-src="https://www.bintray.com/docs/images/bintray_badge_color.png" style="max-width:100%;"></a></p> 
  <p>If you are using SBT, simply add the following to your <code>build.sbt</code> file:</p> 
  <div class="highlight highlight-source-scala">
   <pre>resolvers <span class="pl-k">+</span><span class="pl-k">=</span> <span class="pl-en">Resolver</span>.bintrayRepo(<span class="pl-s"><span class="pl-pds">"</span>derrickburns<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>maven<span class="pl-pds">"</span></span>)

libraryDependencies <span class="pl-k">++</span><span class="pl-k">=</span> <span class="pl-en">Seq</span>(
  <span class="pl-s"><span class="pl-pds">"</span>com.massivedatascience<span class="pl-pds">"</span></span> <span class="pl-k">%%</span> <span class="pl-s"><span class="pl-pds">"</span>massivedatascience-clusterer<span class="pl-pds">"</span></span> <span class="pl-k">%</span> <span class="pl-s"><span class="pl-pds">"</span>x.y.z<span class="pl-pds">"</span></span>
)</pre>
  </div> 
  <h4><a id="user-content-maven" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#maven" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Maven</h4> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;<span class="pl-ent">dependency</span>&gt;
  &lt;<span class="pl-ent">groupId</span>&gt;com.massivedatascience&lt;/<span class="pl-ent">groupId</span>&gt;
  &lt;<span class="pl-ent">artifactId</span>&gt;massivedatascience-clusterer_2.10&lt;/<span class="pl-ent">artifactId</span>&gt;
  &lt;<span class="pl-ent">version</span>&gt;x.y.z&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;

&lt;<span class="pl-ent">dependency</span>&gt;
  &lt;<span class="pl-ent">groupId</span>&gt;com.massivedatascience&lt;/<span class="pl-ent">groupId</span>&gt;
  &lt;<span class="pl-ent">artifactId</span>&gt;massivedatascience-clusterer_2.11&lt;/<span class="pl-ent">artifactId</span>&gt;
  &lt;<span class="pl-ent">version</span>&gt;x.y.z&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;



&lt;<span class="pl-ent">repositories</span>&gt;
    &lt;<span class="pl-ent">repository</span>&gt;
        &lt;<span class="pl-ent">id</span>&gt;bintray&lt;/<span class="pl-ent">id</span>&gt;
        &lt;<span class="pl-ent">name</span>&gt;bintray&lt;/<span class="pl-ent">name</span>&gt;
        &lt;<span class="pl-ent">url</span>&gt;http://dl.bintray.com/derrickburns/maven/&lt;/<span class="pl-ent">url</span>&gt;
    &lt;/<span class="pl-ent">repository</span>&gt;
&lt;/<span class="pl-ent">repositories</span>&gt;</pre>
  </div> 
  <h3><a id="user-content-introduction" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#introduction" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Introduction</h3> 
  <p>The goal of K-Means clustering is to produce a set of clusters of a set of points that satisfies certain optimality constraints. That model is called a K-Means model. It is fundamentally a set of points and a function that defines the distance from an arbitrary point to a cluster center.</p> 
  <p>The K-Means algorithm computes a K-Means model using an iterative algorithm known as <a href="http://en.wikipedia.org/wiki/Lloyd%27s_algorithm" target="_blank">Lloyd's algorithm</a>. Each iteration of Lloyd's algorithm assigns a set of points to clusters, then updates the cluster centers to acknowledge the assignment of the points to the cluster.</p> 
  <p>The update of clusters is a form of averaging. Newly added points are averaged into the cluster while (optionally) reassigned points are removed from their prior clusters.</p> 
  <h4><a id="user-content-bregman-divergences" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#bregman-divergences" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Bregman Divergences</h4> 
  <p>While one can assign a point to a cluster using any distance function, Lloyd's algorithm only converges for a certain set of distance functions called <a href="http://www.cs.utexas.edu/users/inderjit/public_papers/bregmanclustering_jmlr.pdf" target="_blank">Bregman divergences</a>. Bregman divergences must define two methods, <code>convex</code> to evaluate a function on a point and <code>gradientOfConvex</code> to evaluate the gradient of the function on a point.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.divergence</span>

<span class="pl-k">trait</span> <span class="pl-en">BregmanDivergence</span> {
  <span class="pl-k">def</span> <span class="pl-en">convex</span>(<span class="pl-v">v</span>: <span class="pl-en">Vector</span>)<span class="pl-k">:</span> <span class="pl-k">Double</span>

  <span class="pl-k">def</span> <span class="pl-en">gradientOfConvex</span>(<span class="pl-v">v</span>: <span class="pl-en">Vector</span>)<span class="pl-k">:</span> <span class="pl-en">Vector</span>
}
</pre>
  </div> 
  <p>For example, by defining <code>convex</code> to be the vector norm (i.e. the sum of the squares of the coordinates), one gets a distance function that equals the square of the well known Euclidean distance. We name it the <code>SquaredEuclideanDistanceDivergence</code>.</p> 
  <p>In addition to the squared Euclidean distance function, this implementation provides several other very useful distance functions. The provided <code>BregmanDivergence</code>s may be accessed by supplying the name of the desired object to the apply method of the companion object.</p> 
  <table>
   <thead> 
    <tr> 
     <th>Name</th> 
     <th>Space</th> 
     <th>Divergence</th> 
     <th>Input</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td><code>SquaredEuclideanDistanceDivergence</code></td> 
     <td>R^d</td> 
     <td>Squared Euclidean</td> 
     <td></td> 
    </tr> 
    <tr> 
     <td><code>RealKullbackLeiblerSimplexDivergence</code></td> 
     <td>R+^d</td> 
     <td><a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank">Kullback-Leibler</a></td> 
     <td>Dense</td> 
    </tr> 
    <tr> 
     <td><code>NaturalKLSimplexDivergence</code></td> 
     <td>N+^d</td> 
     <td>Kullback-Leibler</td> 
     <td>Dense</td> 
    </tr> 
    <tr> 
     <td><code>RealKLDivergence</code></td> 
     <td>R^d</td> 
     <td>Kullback-Leibler</td> 
     <td>Dense</td> 
    </tr> 
    <tr> 
     <td><code>NaturalKLDivergence</code></td> 
     <td>N^d</td> 
     <td>Kullback-Leibler</td> 
     <td>Dense</td> 
    </tr> 
    <tr> 
     <td><code>ItakuraSaitoDivergence</code></td> 
     <td>R+^d</td> 
     <td>Kullback-Leibler</td> 
     <td>Sparse</td> 
    </tr> 
    <tr> 
     <td><code>LogisticLossDivergence</code></td> 
     <td>R</td> 
     <td>Logistic Loss</td> 
     <td></td> 
    </tr> 
    <tr> 
     <td><code>GeneralizedIDivergence</code></td> 
     <td>R</td> 
     <td>Generalized I</td> 
     <td></td> 
    </tr> 
   </tbody>
  </table> 
  <p>When selecting a distance function, consider the domain of the input data. For example, frequency data is integral. Similarity of frequencies or distributions are best performed using the Kullback-Leibler divergence.</p> 
  <h4><a id="user-content-compute-bregman-distances-efficiently-using-bregmanpoints--and-bregmancenters" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#compute-bregman-distances-efficiently-using-bregmanpoints--and-bregmancenters" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Compute Bregman Distances Efficiently using <code>BregmanPoint</code>s and <code>BregmanCenter</code>s</h4> 
  <p>For efficient repeated computation of distance between a fixed set of points and varying cluster centers, is it convenient to pre-compute certain information and associate that information with the point or the cluster center. The class that represent an enriched point is <code>BregmanPoint</code>. The class that represent the enriched cluster center is <code>BregmanCenter</code>. Users of this package do not construct instances of these objects directly.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.divergence</span>

<span class="pl-k">trait</span> <span class="pl-en">BregmanPoint</span>

<span class="pl-k">trait</span> <span class="pl-en">BregmanCenter</span></pre>
  </div> 
  <p>We enrich a Bregman divergence with a set of commonly used operations, including factory methods <code>toPoint</code> and <code>toCenter</code> to contruct instances of the aforementioned <code>BregmanPoint</code> and <code>BregmanCenter</code>.</p> 
  <p>The enriched trait is the <code>BregmanPointOps</code>.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">trait</span> <span class="pl-en">BregmanPointOps</span>  {
  <span class="pl-k">type</span> <span class="pl-en">P</span> <span class="pl-k">=</span> <span class="pl-en">BregmanPoint</span>
  <span class="pl-k">type</span> <span class="pl-en">C</span> <span class="pl-k">=</span> <span class="pl-en">BregmanCenter</span>

  <span class="pl-k">val</span> <span class="pl-en">divergence</span><span class="pl-k">:</span> <span class="pl-en">BregmanDivergence</span>

  <span class="pl-k">def</span> <span class="pl-en">toPoint</span>(<span class="pl-v">v</span>: <span class="pl-en">WeightedVector</span>)<span class="pl-k">:</span> <span class="pl-en">P</span>

  <span class="pl-k">def</span> <span class="pl-en">toCenter</span>(<span class="pl-v">v</span>: <span class="pl-en">WeightedVector</span>)<span class="pl-k">:</span> <span class="pl-en">C</span>

  <span class="pl-k">def</span> <span class="pl-en">centerMoved</span>(<span class="pl-v">v</span>: <span class="pl-en">P</span>, <span class="pl-v">w</span>: <span class="pl-en">C</span>)<span class="pl-k">:</span> <span class="pl-k">Boolean</span>

  <span class="pl-k">def</span> <span class="pl-en">findClosest</span>(<span class="pl-v">centers</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">C</span>], <span class="pl-v">point</span>: <span class="pl-en">P</span>)<span class="pl-k">:</span> (<span class="pl-k">Int</span>, <span class="pl-k">Double</span>)

  <span class="pl-k">def</span> <span class="pl-en">findClosestCluster</span>(<span class="pl-v">centers</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">C</span>], <span class="pl-v">point</span>: <span class="pl-en">P</span>)<span class="pl-k">:</span> <span class="pl-k">Int</span>

  <span class="pl-k">def</span> <span class="pl-en">distortion</span>(<span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">P</span>], <span class="pl-v">centers</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">C</span>])

  <span class="pl-k">def</span> <span class="pl-en">pointCost</span>(<span class="pl-v">centers</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">C</span>], <span class="pl-v">point</span>: <span class="pl-en">P</span>)<span class="pl-k">:</span> <span class="pl-k">Double</span>

  <span class="pl-k">def</span> <span class="pl-en">distance</span>(<span class="pl-v">p</span>: <span class="pl-en">BregmanPoint</span>, <span class="pl-v">c</span>: <span class="pl-en">BregmanCenter</span>)<span class="pl-k">:</span> <span class="pl-k">Double</span>
}

<span class="pl-k">object</span> <span class="pl-en">BregmanPointOps</span> {

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">distanceFunction</span>: <span class="pl-k">String</span>)<span class="pl-k">:</span> <span class="pl-en">BregmanPointOps</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

}</pre>
  </div> 
  <p>One may construct instances of <code>BregmanPointOps</code> using the companion object.</p> 
  <table>
   <thead> 
    <tr> 
     <th>Name</th> 
     <th>Divergence</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td><code>EUCLIDEAN</code></td> 
     <td>Squared Euclidean</td> 
    </tr> 
    <tr> 
     <td><code>RELATIVE_ENTROPY</code></td> 
     <td><a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank">Kullback-Leibler</a></td> 
    </tr> 
    <tr> 
     <td><code>DISCRETE_KL</code></td> 
     <td>Kullback-Leibler</td> 
    </tr> 
    <tr> 
     <td><code>DISCRETE_SMOOTHED_KL</code></td> 
     <td>Kullback-Leibler</td> 
    </tr> 
    <tr> 
     <td><code>SPARSE_SMOOTHED_KL</code></td> 
     <td>Kullback-Leibler</td> 
    </tr> 
    <tr> 
     <td><code>LOGISTIC_LOSS</code></td> 
     <td>Logistic Loss</td> 
    </tr> 
    <tr> 
     <td><code>GENERALIZED_I</code></td> 
     <td>Generalized I</td> 
    </tr> 
    <tr> 
     <td><code>ITAKURA_SAITO</code></td> 
     <td><a href="http://en.wikipedia.org/wiki/Itakura%E2%80%93Saito_distance" target="_blank">Itakura-Saito</a></td> 
    </tr> 
   </tbody>
  </table> 
  <h4><a id="user-content-representing-k-means-models" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#representing-k-means-models" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Representing K-Means Models</h4> 
  <p>With these definitions, we define our realization of a k-means model, <code>KMeansModel</code>, which we enrich with operations to find closest clusters to a point and to compute distances:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">trait</span> <span class="pl-en">KMeansModel</span> {

  <span class="pl-k">val</span> <span class="pl-en">pointOps</span><span class="pl-k">:</span> <span class="pl-en">BregmanPointOps</span>

  <span class="pl-k">def</span> <span class="pl-en">centers</span><span class="pl-k">:</span> <span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>]


  <span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-v">point</span>: <span class="pl-en">Vector</span>)<span class="pl-k">:</span> <span class="pl-k">Int</span>

  <span class="pl-k">def</span> <span class="pl-en">predictClusterAndDistance</span>(<span class="pl-v">point</span>: <span class="pl-en">Vector</span>)<span class="pl-k">:</span> (<span class="pl-k">Int</span>, <span class="pl-k">Double</span>)

  <span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-v">points</span>: <span class="pl-en">RDD</span>[<span class="pl-en">Vector</span>])<span class="pl-k">:</span> <span class="pl-en">RDD</span>[<span class="pl-k">Int</span>]

  <span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-v">points</span>: <span class="pl-en">JavaRDD</span>[<span class="pl-en">Vector</span>])<span class="pl-k">:</span> <span class="pl-en">JavaRDD</span>[java.lang.<span class="pl-en">Integer</span>]

  <span class="pl-k">def</span> <span class="pl-en">computeCost</span>(<span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">Vector</span>])<span class="pl-k">:</span> <span class="pl-k">Double</span>


  <span class="pl-k">def</span> <span class="pl-en">predictWeighted</span>(<span class="pl-v">point</span>: <span class="pl-en">WeightedVector</span>)<span class="pl-k">:</span> <span class="pl-k">Int</span>

  <span class="pl-k">def</span> <span class="pl-en">predictClusterAndDistanceWeighted</span>(<span class="pl-v">point</span>: <span class="pl-en">WeightedVector</span>)<span class="pl-k">:</span> (<span class="pl-k">Int</span>, <span class="pl-k">Double</span>)

  <span class="pl-k">def</span> <span class="pl-en">predictWeighted</span>(<span class="pl-v">points</span>: <span class="pl-en">RDD</span>[<span class="pl-en">WeightedVector</span>])<span class="pl-k">:</span> <span class="pl-en">RDD</span>[<span class="pl-k">Int</span>]

  <span class="pl-k">def</span> <span class="pl-en">computeCostWeighted</span>(<span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">WeightedVector</span>])<span class="pl-k">:</span> <span class="pl-k">Double</span>


  <span class="pl-k">def</span> <span class="pl-en">predictBregman</span>(<span class="pl-v">point</span>: <span class="pl-en">BregmanPoint</span>)<span class="pl-k">:</span> <span class="pl-k">Int</span>

  <span class="pl-k">def</span> <span class="pl-en">predictClusterAndDistanceBregman</span>(<span class="pl-v">point</span>: <span class="pl-en">BregmanPoint</span>)<span class="pl-k">:</span> (<span class="pl-k">Int</span>, <span class="pl-k">Double</span>)

  <span class="pl-k">def</span> <span class="pl-en">predictBregman</span>(<span class="pl-v">points</span>: <span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>])<span class="pl-k">:</span> <span class="pl-en">RDD</span>[<span class="pl-k">Int</span>]

  <span class="pl-k">def</span> <span class="pl-en">computeCostBregman</span>(<span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>)<span class="pl-k">:</span> <span class="pl-k">Double</span>
}</pre>
  </div> 
  <h4><a id="user-content-constructing-k-means-models-using-clusterers" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-using-clusterers" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Constructing K-Means Models using Clusterers</h4> 
  <p>One may construct K-Means models using one of the provided clusterers that implement Lloyd's algorithm.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">trait</span> <span class="pl-en">MultiKMeansClusterer</span> <span class="pl-k">extends</span> <span class="pl-e">Serializable</span> <span class="pl-k">with</span> <span class="pl-e">Logging</span> {
  <span class="pl-k">def</span> <span class="pl-en">cluster</span>(
    <span class="pl-v">maxIterations</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">pointOps</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>],
    <span class="pl-v">centers</span>: <span class="pl-en">Seq</span>[<span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>]])<span class="pl-k">:</span> <span class="pl-en">Seq</span>[(<span class="pl-k">Double</span>, <span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>])]

  <span class="pl-k">def</span> <span class="pl-en">best</span>(
    <span class="pl-v">maxIterations</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">pointOps</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>],
    <span class="pl-v">centers</span>: <span class="pl-en">Seq</span>[<span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>]])<span class="pl-k">:</span> (<span class="pl-k">Double</span>, <span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>]) <span class="pl-k">=</span> {
    cluster(maxIterations, pointOps, data, centers).minBy(_._1)
  }
}

<span class="pl-k">object</span> <span class="pl-en">MultiKMeansClusterer</span> {
  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">clustererName</span>: <span class="pl-k">String</span>)<span class="pl-k">:</span> <span class="pl-en">MultiKMeansClusterer</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}</pre>
  </div> 
  <p>The <code>COLUMN_TRACKING</code> algorithm tracks the assignments of points to clusters and the distance of points to their assigned cluster. In later iterations of Lloyd's algorithm, this information can be used to reduce the number of distance calculations needed to accurately reassign points. This is a novel implementation.</p> 
  <p>The <code>MINI_BATCH_10</code> algorithm implements the <a href="http://www.eecs.tufts.edu/%7Edsculley/papers/fastkmeans.pdf" target="_blank">mini-batch algorithm</a>. This implementation should be used when the number of points is much larger than the dimension of the data and the number of clusters desired.</p> 
  <p>The <code>RESEED</code> algorithm fills empty clusters with newly seeded cluster centers in an effort to reach the target number of desired clusters.</p> 
  <p>Objects implementing these algorithms may be constructed using the <code>apply</code> method of the companion object <code>MultiKMeansClusterer</code>.</p> 
  <table>
   <thead> 
    <tr> 
     <th>Name</th> 
     <th>Algorithm</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td><code>COLUMN_TRACKING</code></td> 
     <td>high performance implementation that performs less work on later rounds</td> 
    </tr> 
    <tr> 
     <td><code>MINI_BATCH_10</code></td> 
     <td>a mini-batch clusterer that samples 10% of the data each round to update centroids</td> 
    </tr> 
    <tr> 
     <td><code>RESEED</code></td> 
     <td>a clusterer that re-seeds empty clusters</td> 
    </tr> 
   </tbody>
  </table> 
  <h3><a id="user-content-constructing-k-means-models-via-lloyds-algorithm" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-via-lloyds-algorithm" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Constructing K-Means Models via Lloyd's Algorithm</h3> 
  <p>A <code>KMeansModel</code> can be constructed from any set of cluster centers and distance function. However, the more interesting models satisfy an optimality constraint. If we sum the distances from the points in a given set to their closest cluster centers, we get a number called the "distortion" or "cost". A K-Means Model is locally optimal with respect to a set of points if each cluster center is determined by the mean of the points assigned to that cluster. Computing such a <code>KMeansModel</code> given a set of points is called "training" the model on those points.</p> 
  <p>The simplest way to train a <code>KMeansModel</code> on a fixed set of points is to use the <code>KMeans.train</code> method. This method is most similar in style to the one provided by the Spark 1.2.0 K-Means clusterer.</p> 
  <p>For dense data in a low dimension space using the squared Euclidean distance function, one may simply call <code>KMeans.train</code> with the data and the desired number of clusters:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.com.massivedatascience.</span><span class="pl-v">clusterer</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.mllib.linalg.</span><span class="pl-v">Vector</span>

<span class="pl-k">val</span> <span class="pl-en">model</span> <span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-en">KMeans</span>.train(<span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">Vector</span>], <span class="pl-v">k</span>: <span class="pl-k">Int</span>)</pre>
  </div> 
  <p>The full signature of the <code>KMeans.train</code> method is:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeans</span> {
<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   *</span>
<span class="pl-c">   * Train a K-Means model using Lloyd's algorithm.</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">data</span> input data</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">k</span>  number of clusters desired</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">maxIterations</span> maximum number of iterations of Lloyd's algorithm</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">runs</span> number of parallel clusterings to run</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">mode</span> initialization algorithm to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">distanceFunctionNames</span> the distance functions to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">clustererName</span> which k-means implementation to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">embeddingNames</span> sequence of embeddings to use, from lowest dimension to greatest</span>
<span class="pl-c">   * <span class="pl-k">@return</span> K-Means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">train</span>(
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">Vector</span>],
    <span class="pl-v">k</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">maxIterations</span>: <span class="pl-k">Int</span> <span class="pl-k">=</span> <span class="pl-en">KMeans</span>.defaultMaxIterations,
    <span class="pl-v">runs</span>: <span class="pl-k">Int</span> <span class="pl-k">=</span> <span class="pl-en">KMeans</span>.defaultNumRuns,
    <span class="pl-v">mode</span>: <span class="pl-k">String</span> <span class="pl-k">=</span> <span class="pl-en">KMeansSelector</span>.<span class="pl-en">K_MEANS_PARALLEL</span>,
    <span class="pl-v">distanceFunctionNames</span>: <span class="pl-en">Seq</span>[<span class="pl-k">String</span>] <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-en">BregmanPointOps</span>.<span class="pl-en">EUCLIDEAN</span>),
    <span class="pl-v">clustererName</span>: <span class="pl-k">String</span> <span class="pl-k">=</span> <span class="pl-en">MultiKMeansClusterer</span>.<span class="pl-en">COLUMN_TRACKING</span>,
    <span class="pl-v">embeddingNames</span>: <span class="pl-en">List</span>[<span class="pl-k">String</span>] <span class="pl-k">=</span> <span class="pl-en">List</span>(<span class="pl-en">Embedding</span>.<span class="pl-en">IDENTITY_EMBEDDING</span>))<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}</pre>
  </div> 
  <p>Many of these parameters will be familiar to anyone who is familiar with the Spark 1.1 clusterer.</p> 
  <p>Similar to the Spark clusterer, we support data provided as <code>Vectors</code>, a request for a number <code>k</code> of clusters desired, a limit <code>maxIterations</code> on the number of iterations of Lloyd's algorithm, and the number of parallel <code>runs</code> of the clusterer.</p> 
  <p>We also offer different initialization <code>mode</code>s. But unlike the Spark clusterer, we do not support setting the number of initialization steps for the mode at this level of the interface.</p> 
  <p>The <code>K-Means.train</code> helper methods allows one to name a sequence of embeddings. Several embeddings are provided that may be constructed using the <code>apply</code> method of the companion object <code>Embedding</code>.</p> 
  <table>
   <thead> 
    <tr> 
     <th>Name</th> 
     <th>Algorithm</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td><code>IDENTITY_EMBEDDING</code></td> 
     <td>Identity</td> 
    </tr> 
    <tr> 
     <td><code>HAAR_EMBEDDING</code></td> 
     <td><a href="http://www.cs.gmu.edu/%7Ejessica/publications/ikmeans_sdm_workshop03.pdf" target="_blank">Haar Transform</a></td> 
    </tr> 
    <tr> 
     <td><code>LOW_DIMENSIONAL_RI</code></td> 
     <td><a href="https://www.sics.se/%7Emange/papers/RI_intro.pdf" target="_blank">Random Indexing</a> with dimension 64 and epsilon = 0.1</td> 
    </tr> 
    <tr> 
     <td><code>MEDIUM_DIMENSIONAL_RI</code></td> 
     <td>Random Indexing with dimension 256 and epsilon = 0.1</td> 
    </tr> 
    <tr> 
     <td><code>HIGH_DIMENSIONAL_RI</code></td> 
     <td>Random Indexing with dimension 1024 and epsilon = 0.1</td> 
    </tr> 
    <tr> 
     <td><code>SYMMETRIZING_KL_EMBEDDING</code></td> 
     <td><a href="http://www-users.cs.umn.edu/%7Ebanerjee/papers/13/bregman-metric.pdf" target="_blank">Symmetrizing KL Embedding</a></td> 
    </tr> 
   </tbody>
  </table> 
  <p>Different distance functions may be used for each embedding. There must be exactly one distance function per embedding provided.</p> 
  <h4><a id="user-content-constructing-k-means-models-on-weightedvectors" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-on-weightedvectors" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Constructing K-Means Models on <code>WeightedVector</code>s</h4> 
  <p>Often, data points that are clustered have varying significance, i.e. they are weighted. This clusterer operates on weighted vectors. Use these <code>WeightedVector</code> companion object to construct weighted vectors.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.linalg</span>

<span class="pl-k">trait</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">extends</span> <span class="pl-e">Serializable</span> {
  <span class="pl-k">def</span> <span class="pl-en">weight</span><span class="pl-k">:</span> <span class="pl-k">Double</span>

  <span class="pl-k">def</span> <span class="pl-en">inhomogeneous</span><span class="pl-k">:</span> <span class="pl-en">Vector</span>

  <span class="pl-k">def</span> <span class="pl-en">homogeneous</span><span class="pl-k">:</span> <span class="pl-en">Vector</span>

  <span class="pl-k">def</span> <span class="pl-en">size</span><span class="pl-k">:</span> <span class="pl-k">Int</span> <span class="pl-k">=</span> homogeneous.size
}

<span class="pl-k">object</span> <span class="pl-en">WeightedVector</span> {

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">v</span>: <span class="pl-en">Vector</span>)<span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">v</span>: <span class="pl-en">Array</span>[<span class="pl-k">Double</span>])<span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">v</span>: <span class="pl-en">Vector</span>, <span class="pl-v">weight</span>: <span class="pl-k">Double</span>)<span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">v</span>: <span class="pl-en">Array</span>[<span class="pl-k">Double</span>], <span class="pl-v">weight</span>: <span class="pl-k">Double</span>)<span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

  <span class="pl-k">def</span> <span class="pl-en">fromInhomogeneousWeighted</span>(<span class="pl-v">v</span>: <span class="pl-en">Array</span>[<span class="pl-k">Double</span>], <span class="pl-v">weight</span>: <span class="pl-k">Double</span>)<span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

  <span class="pl-k">def</span> <span class="pl-en">fromInhomogeneousWeighted</span>(<span class="pl-v">v</span>: <span class="pl-en">Vector</span>, <span class="pl-v">weight</span>: <span class="pl-k">Double</span>)<span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}</pre>
  </div> 
  <p>Indeed, the <code>KMeans.train</code> helper translates the parameters into a call to the underlying <code>KMeans.trainWeighted</code> method.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeans</span> {
&nbsp;&nbsp;<span class="pl-c"><span class="pl-c">/*</span>*</span>
<span class="pl-c">&nbsp;&nbsp; *</span>
<span class="pl-c">&nbsp;&nbsp; * Train a K-Means model using Lloyd's algorithm on WeightedVectors</span>
<span class="pl-c">&nbsp;&nbsp; *</span>
<span class="pl-c">&nbsp;&nbsp; * @param data input data</span>
<span class="pl-c">&nbsp;&nbsp; * @param runConfig run configuration</span>
<span class="pl-c">&nbsp;&nbsp; * @param pointOps the distance functions to use</span>
<span class="pl-c">&nbsp;&nbsp; * @param initializer initialization algorithm to use</span>
<span class="pl-c">&nbsp;&nbsp; * @param embeddings sequence of embeddings to use, from lowest dimension to greatest</span>
<span class="pl-c">&nbsp;&nbsp; * @param clusterer which k-means implementation to use</span>
<span class="pl-c">&nbsp;&nbsp; * @return K-Means model</span>
<span class="pl-c">&nbsp;&nbsp; <span class="pl-c">*/</span></span>

&nbsp;&nbsp;<span class="pl-k">def</span> <span class="pl-en">trainWeighted</span>(
&nbsp;&nbsp;  <span class="pl-v">runConfig</span>: <span class="pl-en">RunConfig</span>,
&nbsp;&nbsp;  <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">WeightedVector</span>],
&nbsp;&nbsp;  <span class="pl-v">initializer</span>: <span class="pl-en">KMeansSelector</span>,
&nbsp;&nbsp;  <span class="pl-v">pointOps</span>: <span class="pl-en">Seq</span>[<span class="pl-en">BregmanPointOps</span>],
&nbsp;&nbsp;  <span class="pl-v">embeddings</span>: <span class="pl-en">Seq</span>[<span class="pl-en">Embedding</span>],
&nbsp;&nbsp;  <span class="pl-v">clusterer</span>: <span class="pl-en">MultiKMeansClusterer</span>)<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
&nbsp;&nbsp;}
}</pre>
  </div> 
  <p>The <code>KMeans.trainWeighted</code> method ultimately makes various calls to the underlying <code>KMeans.simpleTrain</code> method, which clusters the provided <code>BregmanPoint</code>s using the provided <code>BregmanPointOps</code> and the provided <code>KMeansSelector</code> with the provided <code>MultiKMeansClusterer</code>.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeans</span> {
<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">runConfig</span> run configuration</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">data</span> input data</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">pointOps</span> the distance functions to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">initializer</span> initialization algorithm to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">clusterer</span> which k-means implementation to use</span>
<span class="pl-c">   * <span class="pl-k">@return</span> K-Means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">simpleTrain</span>(
    <span class="pl-v">runConfig</span>: <span class="pl-en">RunConfig</span>,
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>],
    <span class="pl-v">pointOps</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">initializer</span>: <span class="pl-en">KMeansSelector</span>,
    <span class="pl-v">clusterer</span>: <span class="pl-en">MultiKMeansClusterer</span>)<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
    }
}</pre>
  </div> 
  <h4><a id="user-content-constructing-k-means-models-iteratively" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#constructing-k-means-models-iteratively" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Constructing K-Means Models Iteratively</h4> 
  <p>If multiple embeddings are provided, the <code>KMeans.train</code> method actually performs the embeddings and trains on the embedded data sets iteratively.</p> 
  <p>For example, for high dimensional data, one way wish to embed the data into a lower dimension before clustering to reduce running time.</p> 
  <p>For time series data, <a href="http://www.cs.gmu.edu/%7Ejessica/publications/ikmeans_sdm_workshop03.pdf" target="_blank">the Haar Transform</a> has been used successfully to reduce running time while maintaining or improving quality.</p> 
  <p>For high-dimensional sparse data, <a href="http://en.wikipedia.org/wiki/Random_indexing" target="_blank">random indexing</a> can be used to map the data into a low dimensional dense space.</p> 
  <p>One may also perform clustering recursively, using lower dimensional clustering to derive initial conditions for higher dimensional clustering.</p> 
  <p>Should you wish to train a model iteratively on data sets derived maps of a shared original data set, you may use <code>KMeans.iterativelyTrain</code>.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeans</span> {
<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Train on a series of data sets, where the data sets were derived from the same</span>
<span class="pl-c">   * original data set via embeddings. Use the cluster assignments of one stage to</span>
<span class="pl-c">   * initialize the clusters of the next stage.</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">runConfig</span> run configuration</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">dataSets</span>  input data sets to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">initializer</span>  initialization algorithm to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">pointOps</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">clusterer</span>  clustering implementation to use</span>
<span class="pl-c">   * <span class="pl-k">@return</span></span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">iterativelyTrain</span>(
    <span class="pl-v">runConfig</span>: <span class="pl-en">RunConfig</span>,
    <span class="pl-v">pointOps</span>: <span class="pl-en">Seq</span>[<span class="pl-en">BregmanPointOps</span>],
    <span class="pl-v">dataSets</span>: <span class="pl-en">Seq</span>[<span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>]],
    <span class="pl-v">initializer</span>: <span class="pl-en">KMeansSelector</span>,
    <span class="pl-v">clusterer</span>: <span class="pl-en">MultiKMeansClusterer</span>)<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
</pre>
  </div> 
  <h4><a id="user-content-seeding-the-set-of-cluster-centers" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#seeding-the-set-of-cluster-centers" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Seeding the Set of Cluster Centers</h4> 
  <p>Any K-Means model may be used as seed value to Lloyd's algorithm. In fact, our clusterers accept multiple seed sets. The <code>K-Means.train</code> helper methods allows one to name an initialization method.</p> 
  <p>Two algorithms are implemented that produce viable seed sets. They may be constructed by using the <code>apply</code> method of the companion object<code>KMeansSelector</code>".</p> 
  <table>
   <thead> 
    <tr> 
     <th>Name</th> 
     <th>Algorithm</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td><code>RANDOM</code></td> 
     <td>Random selection of initial k centers</td> 
    </tr> 
    <tr> 
     <td><code>K_MEANS_PARALLEL</code></td> 
     <td>a 5 step <a href="http://theory.stanford.edu/%7Esergei/papers/vldb12-kmpar.pdf" target="_blank">K-Means Parallel implementation</a></td> 
    </tr> 
   </tbody>
  </table> 
  <p>Under the covers, these initializers implement the <code>KMeansSelector</code> trait</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">trait</span> <span class="pl-en">KMeansSelector</span> <span class="pl-k">extends</span> <span class="pl-e">Serializable</span> {
  <span class="pl-k">def</span> <span class="pl-en">init</span>(
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">d</span>: <span class="pl-en">RDD</span>[<span class="pl-en">BregmanPoint</span>],
    <span class="pl-v">numClusters</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">initialInfo</span>: <span class="pl-en">Option</span>[(<span class="pl-en">Seq</span>[<span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>]], <span class="pl-en">Seq</span>[<span class="pl-en">RDD</span>[<span class="pl-k">Double</span>]])] <span class="pl-k">=</span> <span class="pl-c1">None</span>,
    <span class="pl-v">runs</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">seed</span>: <span class="pl-k">Long</span>)<span class="pl-k">:</span> <span class="pl-en">Seq</span>[<span class="pl-en">IndexedSeq</span>[<span class="pl-en">BregmanCenter</span>]]
}

<span class="pl-k">object</span> <span class="pl-en">KMeansSelector</span> {
  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">name</span>: <span class="pl-k">String</span>)<span class="pl-k">:</span> <span class="pl-en">KMeansSelector</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}</pre>
  </div> 
  <h4><a id="user-content-iterative-clustering" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#iterative-clustering" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Iterative Clustering</h4> 
  <p>K-means clustering can be performed iteratively using different embeddings of the data. For example, with high-dimensional time series data, it may be advantageous to:</p> 
  <ul> 
   <li>Down-sample the data via the Haar transform (aka averaging)</li> 
   <li>Solve the K-means clustering problem on the down-sampled data</li> 
   <li>Assign the downsampled points to clusters.</li> 
   <li>Create a new KMeansModel from the using the assignments on the original data</li> 
   <li>Solve the K-Means clustering on the KMeansModel so constructed</li> 
  </ul> 
  <p>This technique has been named the <a href="http://www.cs.gmu.edu/%7Ejessica/publications/ikmeans_sdm_workshop03.pdf" target="_blank">"Anytime" Algorithm</a>.</p> 
  <p>The <code>com.massivedatascience.clusterer.KMeans</code> helper method provides a method, <code>timeSeriesTrain</code> that embeds the data iteratively.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeans</span> {

  <span class="pl-k">def</span> <span class="pl-en">timeSeriesTrain</span>(
    <span class="pl-v">runConfig</span>: <span class="pl-en">RunConfig</span>,
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">WeightedVector</span>],
    <span class="pl-v">initializer</span>: <span class="pl-en">KMeansSelector</span>,
    <span class="pl-v">pointOps</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">clusterer</span>: <span class="pl-en">MultiKMeansClusterer</span>,
    <span class="pl-v">embedding</span>: <span class="pl-en">Embedding</span> <span class="pl-k">=</span> <span class="pl-en">Embedding</span>(<span class="pl-en">HAAR_EMBEDDING</span>))<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
  }
}</pre>
  </div> 
  <p>High dimensional data can be clustered directly, but the cost is proportional to the dimension. If the divergence of interest is squared Euclidean distance, one can using <a href="http://en.wikipedia.org/wiki/Random_indexing" target="_blank">Random Indexing</a> to down-sample the data while preserving distances between clusters, with high probability.</p> 
  <p>The <code>com.massivedatascience.clusterer.KMeans</code> helper method provides a method, <code>sparseTrain</code> that embeds into various dimensions using randoming indexing.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeans</span> {

  <span class="pl-k">def</span> <span class="pl-en">sparseTrain</span>(<span class="pl-v">raw</span>: <span class="pl-en">RDD</span>[<span class="pl-en">Vector</span>], <span class="pl-v">k</span>: <span class="pl-k">Int</span>)<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> {
    train(raw, k,
      embeddingNames <span class="pl-k">=</span> <span class="pl-en">List</span>(<span class="pl-en">Embedding</span>.<span class="pl-en">LOW_DIMENSIONAL_RI</span>, <span class="pl-en">Embedding</span>.<span class="pl-en">MEDIUM_DIMENSIONAL_RI</span>,
        <span class="pl-en">Embedding</span>.<span class="pl-en">HIGH_DIMENSIONAL_RI</span>))
  }
}</pre>
  </div> 
  <h3><a id="user-content-creating-a-custom-k-means-clusterer" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#creating-a-custom-k-means-clusterer" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Creating a Custom K-means Clusterer</h3> 
  <p>There are many ways to create your our custom K-means clusterer from these components.</p> 
  <h4><a id="user-content-custom-bregmandivergence" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#custom-bregmandivergence" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Custom <code>BregmanDivergence</code></h4> 
  <p>You may create your own custom <code>BregmanDivergence</code> given a suitable continuously-differentiable real-valued and strictly convex function defined on a closed convex set in R^^N using the <code>apply</code> method of the companion object. Send a pull request to have it added the the package.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.divergence</span>

<span class="pl-k">object</span> <span class="pl-en">BregmanDivergence</span> {

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a Bregman Divergence from</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">f</span> any continuously-differentiable real-valued and strictly</span>
<span class="pl-c">   *          convex function defined on a closed convex set in R^^N</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">gradientF</span> the gradient of f</span>
<span class="pl-c">   * <span class="pl-k">@return</span> a Bregman Divergence on that function</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">f</span>: (<span class="pl-en">Vector</span>) <span class="pl-k">=&gt;</span> <span class="pl-k">Double</span>, <span class="pl-v">gradientF</span>: (<span class="pl-en">Vector</span>) <span class="pl-k">=&gt;</span> <span class="pl-en">Vector</span>)<span class="pl-k">:</span> <span class="pl-en">BregmanDivergence</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}</pre>
  </div> 
  <h4><a id="user-content-custom-bregmanpointops" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#custom-bregmanpointops" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Custom <code>BregmanPointOps</code></h4> 
  <p>You may create your own custom <code>BregmanPointsOps</code> from your own implementation of the <code>BregmanDivergence</code> trait given a <code>BregmanDivergence</code> using the <code>apply</code> method of the companion object. Send a pull request to have it added the the package.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">BregmanPointOps</span> {

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">d</span>: <span class="pl-en">BregmanDivergence</span>)<span class="pl-k">:</span> <span class="pl-en">BregmanPointOps</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

  <span class="pl-k">def</span> <span class="pl-en">apply</span>(<span class="pl-v">d</span>: <span class="pl-en">BregmanDivergence</span>, <span class="pl-v">factor</span>: <span class="pl-k">Double</span>)<span class="pl-k">:</span> <span class="pl-en">BregmanPointOps</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}</pre>
  </div> 
  <h4><a id="user-content-custom-embedding" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#custom-embedding" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Custom <code>Embedding</code></h4> 
  <p>Perhaps you have a dimensionality reduction method that is not provided by one of the standard embeddings. You may create your own embedding.</p> 
  <p>For example, If the number of clusters desired is small, but the dimension is high, one may also use the method of <a href="http://www.cs.toronto.edu/%7Ezouzias/downloads/papers/NIPS2010_kmeans.pdf" target="_blank">Random Projections</a>. At present, no embedding is provided for random projects, but, hey, I have to leave something for you to do! Send a pull request!!!</p> 
  <h3><a id="user-content-creating-k-means-models-using-the-kmeansmodel-helper-object" class="anchor" href="https://github.com/derrickburns/generalized-kmeans-clustering#creating-k-means-models-using-the-kmeansmodel-helper-object" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Creating K-Means Models using the <code>KMeansModel</code> Helper Object</h3> 
  <p>Training a K-Means model from a set of points using <code>KMeans.train</code> is one way to create a <code>KMeansModel</code>. However, there are many others that are useful. The <code>KMeansModel</code> companion object provides a number of these constructors.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">com.massivedatascience.clusterer</span>

<span class="pl-k">object</span> <span class="pl-en">KMeansModel</span> {

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-means model from given cluster centers and weights</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">centers</span> initial cluster centers in homogeneous coordinates</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">weights</span> initial cluster weights</span>
<span class="pl-c">   * <span class="pl-k">@return</span>  k-means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">fromVectorsAndWeights</span>(
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">centers</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">Vector</span>],
    <span class="pl-v">weights</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-k">Double</span>]) <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-means model from given weighted vectors</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">centers</span> initial cluster centers as weighted vectors</span>
<span class="pl-c">   * <span class="pl-k">@return</span>  k-means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">fromWeightedVectors</span>[<span class="pl-en">T</span> <span class="pl-k">&lt;</span><span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">:</span> <span class="pl-en">ClassTag</span>](
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">centers</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">T</span>]) <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-means model by selecting a set of k points at random</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">k</span> number of centers desired</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">dim</span> dimension of space</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">weight</span> initial weight of points</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">seed</span> random number seed</span>
<span class="pl-c">   * <span class="pl-k">@return</span>  k-means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">usingRandomGenerator</span>(<span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">k</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">dim</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">weight</span>: <span class="pl-k">Double</span>,
    <span class="pl-v">seed</span>: <span class="pl-k">Long</span> <span class="pl-k">=</span> <span class="pl-en">XORShiftRandom</span>.random.nextLong()) <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-Means model using the KMeans++ algorithm on an initial set of candidate centers</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">data</span> initial candidate centers</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">weights</span> initial weights</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">k</span> number of clusters desired</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">perRound</span> number of candidates to add per round</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">numPreselected</span> initial sub-sequence of candidates to always select</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">seed</span> random number seed</span>
<span class="pl-c">   * <span class="pl-k">@return</span>  k-means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">fromCenters</span>[<span class="pl-en">T</span> <span class="pl-k">&lt;</span><span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">:</span> <span class="pl-en">ClassTag</span>](
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">data</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-en">T</span>],
    <span class="pl-v">weights</span>: <span class="pl-en">IndexedSeq</span>[<span class="pl-k">Double</span>],
    <span class="pl-v">k</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">perRound</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">numPreselected</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">seed</span>: <span class="pl-k">Long</span> <span class="pl-k">=</span> <span class="pl-en">XORShiftRandom</span>.random.nextLong())<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-Means Model from a streaming k-means model.</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">streamingKMeansModel</span> mutable streaming model</span>
<span class="pl-c">   * <span class="pl-k">@return</span> immutable k-means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">fromStreamingModel</span>(<span class="pl-v">streamingKMeansModel</span>: <span class="pl-en">StreamingKMeansModel</span>)<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-Means Model from a set of assignments of points to clusters</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">points</span> initial bregman points</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">assignments</span> assignments of points to clusters</span>
<span class="pl-c">   * <span class="pl-k">@return</span></span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">fromAssignments</span>[<span class="pl-en">T</span> <span class="pl-k">&lt;</span><span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">:</span> <span class="pl-en">ClassTag</span>](
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">points</span>: <span class="pl-en">RDD</span>[<span class="pl-en">T</span>],
    <span class="pl-v">assignments</span>: <span class="pl-en">RDD</span>[<span class="pl-k">Int</span>])<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Create a K-Means Model using K-Means || algorithm from an RDD of Bregman points.</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">data</span> initial points</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">k</span>  number of cluster centers desired</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">numSteps</span> number of iterations of k-Means ||</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">sampleRate</span> fractions of points to use in weighting clusters</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">seed</span> random number seed</span>
<span class="pl-c">   * <span class="pl-k">@return</span>  k-means model</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">usingKMeansParallel</span>[<span class="pl-en">T</span> <span class="pl-k">&lt;</span><span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">:</span> <span class="pl-en">ClassTag</span>](
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">T</span>],
    <span class="pl-v">k</span>: <span class="pl-k">Int</span>,
    <span class="pl-v">numSteps</span>: <span class="pl-k">Int</span> <span class="pl-k">=</span> <span class="pl-c1">2</span>,
    <span class="pl-v">sampleRate</span>: <span class="pl-k">Double</span> <span class="pl-k">=</span> <span class="pl-c1">1.0</span>,
    <span class="pl-v">seed</span>: <span class="pl-k">Long</span> <span class="pl-k">=</span> <span class="pl-en">XORShiftRandom</span>.random.nextLong())<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>

<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Construct a K-Means model using the Lloyd's algorithm given a set of initial</span>
<span class="pl-c">   * K-Means models.</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">ops</span> distance function</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">data</span> points to fit</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">initialModels</span>  initial k-means models</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">clusterer</span> k-means clusterer to use</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">seed</span> random number seed</span>
<span class="pl-c">   * <span class="pl-k">@return</span>  the best K-means model found</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">usingLloyds</span>[<span class="pl-en">T</span> <span class="pl-k">&lt;</span><span class="pl-k">:</span> <span class="pl-en">WeightedVector</span> <span class="pl-k">:</span> <span class="pl-en">ClassTag</span>](
    <span class="pl-v">ops</span>: <span class="pl-en">BregmanPointOps</span>,
    <span class="pl-v">data</span>: <span class="pl-en">RDD</span>[<span class="pl-en">T</span>],
    <span class="pl-v">initialModels</span>: <span class="pl-en">Seq</span>[<span class="pl-en">KMeansModel</span>],
    <span class="pl-v">clusterer</span>: <span class="pl-en">MultiKMeansClusterer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">ColumnTrackingKMeans</span>(),
    <span class="pl-v">seed</span>: <span class="pl-k">Long</span> <span class="pl-k">=</span> <span class="pl-en">XORShiftRandom</span>.random.nextLong())<span class="pl-k">:</span> <span class="pl-en">KMeansModel</span> <span class="pl-k">=</span> <span class="pl-k">???</span>
}
</pre>
  </div> 
 </article>
</div>