<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-hbase-rdd" class="anchor" href="https://github.com/unicredit/hbase-rdd#hbase-rdd" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>HBase RDD</h1> 
  <p><a href="https://raw.githubusercontent.com/unicredit/hbase-rdd/master/docs/logo.png" target="_blank"><img src="https://raw.githubusercontent.com/unicredit/hbase-rdd/master/docs/logo.png" alt="logo" style="max-width:100%;"></a> <a href="https://index.scala-lang.org/unicredit/hbase-rdd/hbase-rdd" target="_blank"><img src="https://camo.githubusercontent.com/eca35751f4cb0ced0e5b9631082a22b7c14d0e98/68747470733a2f2f696e6465782e7363616c612d6c616e672e6f72672f756e696372656469742f68626173652d7264642f68626173652d7264642f6c61746573742e7376673f636f6c6f723d6f72616e6765" alt="Latest version" data-canonical-src="https://index.scala-lang.org/unicredit/hbase-rdd/hbase-rdd/latest.svg?color=orange" style="max-width:100%;"></a></p> 
  <p>This project allows to connect Apache Spark to HBase. Currently it is compiled with Scala 2.10 and 2.11, using the versions of Spark and HBase available on CDH5.5. Version <code>0.6.0</code> of this project works on CDH5.3, version <code>0.4.0</code> works on CDH5.1 and version <code>0.2.2-SNAPSHOT</code> works on CDH5.0. Other combinations of versions may be made available in the future.</p> 
  <h2><a id="user-content-table-of-contents" class="anchor" href="https://github.com/unicredit/hbase-rdd#table-of-contents" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Table of contents</h2> 
  <ul> 
   <li><a href="https://github.com/unicredit/hbase-rdd#installation" target="_blank">Installation</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd#preliminary" target="_blank">Preliminary</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd#a-note-on-types" target="_blank">A note on types</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd#reading-from-hbase" target="_blank">Reading from HBase</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd#writing-to-hbase" target="_blank">Writing to HBase</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd#deleting-from-hbase" target="_blank">Deleting from HBase</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd#bulk-load-to-hbase-using-hfiles" target="_blank">Bulk loading to HBase</a></li> 
   <li><a href="https://github.com/unicredit/hbase-rdd-examples" target="_blank">Example project</a></li> 
  </ul> 
  <h2><a id="user-content-installation" class="anchor" href="https://github.com/unicredit/hbase-rdd#installation" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Installation</h2> 
  <p>This guide assumes you are using SBT. Usage of similar tools like Maven or Leiningen should work with minor differences as well.</p> 
  <p>HBase RDD can be added as a dependency in sbt with:</p> 
  <pre><code>dependencies += "eu.unicredit" %% "hbase-rdd" % "0.8.0"
</code></pre> 
  <p>Currently, the project depends on the following artifacts:</p> 
  <pre><code>"org.apache.spark" %% "spark-core" % "1.5.0" % "provided",
"org.apache.hbase" % "hbase-common" % "1.0.0-cdh5.5.2" % "provided",
"org.apache.hbase" % "hbase-client" % "1.0.0-cdh5.5.2" % "provided",
"org.apache.hbase" % "hbase-server" % "1.0.0-cdh5.5.2" % "provided",
"org.json4s" %% "json4s-jackson" % "3.2.11" % "provided"
</code></pre> 
  <p>All dependencies appear with <code>provided</code> scope, so you will have to either have these dependencies in your project, or have the corresponding artifacts available locally in your cluster. Most of them are available in the Cloudera repositories, which you can add with the following line:</p> 
  <pre><code>resolvers ++= Seq(
  "Cloudera repos" at "https://repository.cloudera.com/artifactory/cloudera-repos",
  "Cloudera releases" at "https://repository.cloudera.com/artifactory/libs-release"
)
</code></pre> 
  <h2><a id="user-content-usage" class="anchor" href="https://github.com/unicredit/hbase-rdd#usage" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage</h2> 
  <h3><a id="user-content-preliminary" class="anchor" href="https://github.com/unicredit/hbase-rdd#preliminary" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Preliminary</h3> 
  <p>First, add the following import to get the necessary implicits:</p> 
  <pre><code>import unicredit.spark.hbase._
</code></pre> 
  <p>Then, you have to give configuration parameters to connect to HBase. This is done by providing an implicit instance of <code>unicredit.spark.hbase.HBaseConfig</code>. This can be done in a few ways, in increasing generality.</p> 
  <h4><a id="user-content-with-hbase-sitexml" class="anchor" href="https://github.com/unicredit/hbase-rdd#with-hbase-sitexml" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>With <code>hbase-site.xml</code></h4> 
  <p>If you happen to have on the classpath <code>hbase-site.xml</code> with the right configuration parameters, you can just do</p> 
  <pre><code>implicit val config = HBaseConfig()
</code></pre> 
  <p>Otherwise, you will have to configure HBase RDD programmatically.</p> 
  <h4><a id="user-content-with-a-case-class" class="anchor" href="https://github.com/unicredit/hbase-rdd#with-a-case-class" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>With a case class</h4> 
  <p>The easiest way is to have a case class having two string members <code>quorum</code> and <code>rootdir</code>. Then, something like the following will work</p> 
  <pre><code>case class Config(
  quorum: String,
  rootdir: String,
  ... // Possibly other parameters
)
val c = Config(...)
implicit val config = HBaseConfig(c)
</code></pre> 
  <h4><a id="user-content-with-a-map" class="anchor" href="https://github.com/unicredit/hbase-rdd#with-a-map" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>With a map</h4> 
  <p>In order to customize more parameters, one can provide a sequence of <code>(String, String)</code>, like</p> 
  <pre><code>implicit val config = HBaseConfig(
  "hbase.rootdir" -&gt; "...",
  "hbase.zookeeper.quorum" -&gt; "...",
  ...
)
</code></pre> 
  <h4><a id="user-content-with-a-hadoop-configuration-object" class="anchor" href="https://github.com/unicredit/hbase-rdd#with-a-hadoop-configuration-object" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>With a Hadoop configuration object</h4> 
  <p>Finally, HBaseConfig can be instantiated from an existing <code>org.apache.hadoop.conf.Configuration</code></p> 
  <pre><code>val conf: Configuration = ...
implicit val config = HBaseConfig(conf)
</code></pre> 
  <h3><a id="user-content-a-note-on-types" class="anchor" href="https://github.com/unicredit/hbase-rdd#a-note-on-types" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>A note on types</h3> 
  <p>In HBase, every data, including tables and column names, is stored as an <code>Array[Byte]</code>. For simplicity, we assume that all table, column and column family names are actually strings.</p> 
  <p>The content of the cells, on the other hand, can have any type that can be converted to and from <code>Array[Byte]</code>. In order to do this, we have defined two traits under <code>unicredit.spark.hbase</code>:</p> 
  <pre><code>trait Reads[A] { def read(data: Array[Byte]): A }
trait Writes[A] { def write(data: A): Array[Byte] }
</code></pre> 
  <p>Methods that read a type <code>A</code> from HBase will need an implicit <code>Reads[A]</code> in scope, and symmetrically methods that write to HBase require an implicit <code>Writes[A]</code>.</p> 
  <p>By default, we provide implicit readers and writers for <code>String</code>, <code>org.json4s.JValue</code> and the quite trivial <code>Array[Byte]</code>.</p> 
  <h3><a id="user-content-reading-from-hbase" class="anchor" href="https://github.com/unicredit/hbase-rdd#reading-from-hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Reading from HBase</h3> 
  <p>Some methods are added to <code>SparkContext</code> in order to read from HBase.</p> 
  <p>If you know which columns to read, then you can use <code>sc.hbase()</code>. Assuming the columns <code>cf1:col1</code>, <code>cf1:col2</code> and <code>cf2:col3</code> in table <code>t1</code> are to be read, and that the content is serialized as an UTF-8 string, then one can do</p> 
  <pre><code>val table = "t1"
val columns = Map(
  "cf1" -&gt; Set("col1", "col2"),
  "cf2" -&gt; Set("col3")
)
val rdd = sc.hbase[String](table, columns)
</code></pre> 
  <p>In general, <code>sc.hbase[K, Q, V]</code> has type parameters which represent the types of row key, qualifier and content of the cells, and it returns a <code>RDD[(K, Map[String, Map[Q, V]])]</code>. Each element of the resulting RDD is a key/value pair, where the key is the rowkey from HBase and the value is a nested map which associates column family and column to the value. Missing columns are omitted from the map, so for instance one can project the above on the <code>col2</code> column doing something like</p> 
  <pre><code>rdd.flatMap({ case (k, v) =&gt;
  v("cf1") get "col2" map { col =&gt;
    k -&gt; col
  }
  // or equivalently
  // Try(k -&gt; v("cf1")("col2")).toOption
})
</code></pre> 
  <p>You can also read with <code>sc.hbase[K, V](table, columns)</code> where only the type parameters for row key and value are explicit, while qualifier is a String, or <code>sc.hbase[V](table, columns)</code>, where types of row key and qualifier are String. These alternatives apply for all <code>sc.hbase()</code> and <code>sc.hbaseTS()</code> methods.</p> 
  <p>A second possibility is to get the whole column families. This can be useful if you do not know in advance which will be the column names. You can do this with the method <code>sc.hbase[A]</code>, like</p> 
  <pre><code>val table = "t1"
val families = Set("cf1", "cf2")
val rdd = sc.hbase[String](table, families)
</code></pre> 
  <p>The output, like <code>sc.hbase[A]</code>, is a <code>RDD[(String, Map[String, Map[String, A]])]</code>.</p> 
  <p>If you need to read also timestamps, you can use in both cases <code>sc.hbaseTS[K, Q, V]</code> and obtain a <code>RDD[(K, Map[String, Map[Q, (V, Long)]])]</code>. Each element of the resulting RDD is a key/value pair, where the key is the rowkey from HBase and the value is a nested map which associates column family and column to the tuple (value, timestamp).</p> 
  <p>Finally, there is a lower level access to the raw <code>org.apache.hadoop.hbase.client.Result</code> instances. For this, just do</p> 
  <pre><code>val table = "t1"
val rdd = sc.hbase[K](table)
</code></pre> 
  <p>The return value of <code>sc.hbase</code> is a <code>RDD[(K, Result)]</code>. The first element is the rowkey, while the second one is an instance of <code>org.apache.hadoop.hbase.client.Result</code>, so you can use the raw HBase API to query it.</p> 
  <p>HBase side filters are also supported by providing a custom Filter or Scan object:</p> 
  <pre><code>val filter = new PrefixFilter(Bytes.toBytes("abc"))
val table = "t1"
val families = Set("cf1", "cf2")
val rdd = sc.hbase[String](table, families, filter)
</code></pre> 
  <h3><a id="user-content-writing-to-hbase" class="anchor" href="https://github.com/unicredit/hbase-rdd#writing-to-hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Writing to HBase</h3> 
  <p>In order to write to HBase, some methods are added on certain types of RDD.</p> 
  <p>The first one is parallel to the way you read from HBase. Assume you have an <code>RDD[(K, Map[String, Map[Q, V]])]</code> and there is <code>Writes[K]</code>, <code>Writes[Q]</code>, and <code>Writes[V]</code> in scope. Then you can write to HBase with the method <code>toHBase</code>, like</p> 
  <pre><code>val table = "t1"
val rdd: RDD[(K, Map[String, Map[Q, V]])] = ...
rdd.toHBase(table)
</code></pre> 
  <p>A simplified form is available in the case that one only needs to write on a single column family. Then a similar method is available on <code>RDD[(K, Map[Q, V])]</code>, which can be used as follows</p> 
  <pre><code>val table = "t1"
val cf = "cf1"
val rdd: RDD[(K, Map[Q, V])] = ...
rdd.toHBase(table, cf)
</code></pre> 
  <p>or, if you have a fixed set of columns, like</p> 
  <pre><code>val table = "t1"
val cf = "cf1"
val headers: Seq[Q] = ...
val rdd: RDD[(K, Seq[V])] = ...
rdd.toHBase(table, cf, headers)
</code></pre> 
  <p>where <code>headers</code> are column names for <code>Seq[V]</code> values.</p> 
  <p>If you need to write timestamps, you can use a tuple (V, Long) in your RDD, where the second element represents the timestamp, like</p> 
  <pre><code>val rdd: RDD[(K, Map[String, Map[Q, (V, Long)]])] = ...
</code></pre> 
  <p>or, for the simplified form, like</p> 
  <pre><code>val rdd: RDD[(K, Map[Q, (V, Long)])] = ...
</code></pre> 
  <p>or, with a fixed set of columns</p> 
  <pre><code>val rdd: RDD[(K, Seq[(V, Long)])] = ...
</code></pre> 
  <p>You can have a look at <code>WriteTsvToHBase.scala</code> in <a href="https://github.com/unicredit/hbase-rdd-examples" target="_blank">hbase-rdd-examples project</a> on how to write a TSV file from <code>Hdfs</code> to <code>HBase</code></p> 
  <h3><a id="user-content-deleting-from-hbase" class="anchor" href="https://github.com/unicredit/hbase-rdd#deleting-from-hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Deleting from HBase</h3> 
  <p>In order to delete from HBase, some methods are added on certain types of RDD.</p> 
  <p>Assume you have an <code>RDD[(K, Map[String, Set[Q])]</code> of row keys and a map of families / set of columns. Then you can delete from HBase with the method <code>deleteHBase</code>, like</p> 
  <pre><code>val table = "t1"
val rdd: RDD[(K, Map[String, Set[Q])] = ...
rdd.deleteHBase(table)
</code></pre> 
  <p>A simplified form is available in the case that one only needs to delete from a single column family. Then a similar method is available on <code>RDD[(K, Set[Q])]</code> of row keys and a set of columns, which can be used as follows</p> 
  <pre><code>val table = "t1"
val cf = "cf1"
val rdd: RDD[(K, Set[Q])] = ...
rdd.deleteHBase(table, cf)
</code></pre> 
  <p>or, if you want to delete a fixed set of columns of one column family, or whole column families, or whole rows, some methods are available on <code>RDD[K]</code> of row keys, which can be used as follows</p> 
  <pre><code>val table = "t1"
val cf = "cf1"
val headers: Set[Q] = ...
val rdd: RDD[K] = ...
rdd.deleteHBase(table, cf, headers)
</code></pre> 
  <p>or</p> 
  <pre><code>val cfs = Set("cf1", "cf2")
rdd.deleteHBase(table, cfs)
</code></pre> 
  <p>or</p> 
  <pre><code>rdd.deleteHBase(table)
</code></pre> 
  <p>If you need to delete with timestamps, you can use a tuple (String, Long) in your RDD, where the first element is a column and the second element represents the timestamp, like</p> 
  <pre><code>val rdd: RDD[(K, Map[String, Set[(Q, Long)]])] = ...
</code></pre> 
  <p>or, for the simplified form, like</p> 
  <pre><code>val rdd: RDD[(K, Set[(Q, Long)])] = ...
</code></pre> 
  <h3><a id="user-content-bulk-load-to-hbase-using-hfiles" class="anchor" href="https://github.com/unicredit/hbase-rdd#bulk-load-to-hbase-using-hfiles" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Bulk load to HBase, using HFiles</h3> 
  <p>In case of massive writing to HBase, writing Put objects directly into the table can be inefficient and can cause HBase to be unresponsive (e.g. it can trigger region splitting). A better approach is to create HFiles instead, and than call LoadIncrementalHFiles job to move them to HBase's file system. Unfortunately this approach is quite cumbersome, as it implies the following steps:</p> 
  <ol> 
   <li> <p>Make sure the table exists and has region splits so that rows are evenly distributed into regions (for better performance).</p> </li> 
   <li> <p>Implement and execute a map (and reduce) job to write ordered Put or KeyValue objects to HFile files, using HFileOutputFormat2 output format. The reduce phase is configured behind the scenes with a call to HFileOutputFormat2.configureIncrementalLoad.</p> </li> 
   <li> <p>Execute LoadIncrementalHFiles job to move HFile files to HBase's file system.</p> </li> 
   <li> <p>Cleanup temporary files and folders</p> </li> 
  </ol> 
  <p>Now you can perform steps 2 to 4 with a call to <code>toHBaseBulk</code>, like</p> 
  <pre><code>val table = "t1"
val rdd: RDD[(K, Map[String, Map[Q, V]])] = ...
rdd.toHBaseBulk(table)
</code></pre> 
  <p>A simplified form is available in the case that one only needs to write on a single column family</p> 
  <pre><code>val table = "t1"
val cf = "cf1"
val rdd: RDD[(K, Map[Q, V])] = ...
rdd.toHBaseBulk(table, cf)
</code></pre> 
  <p>or, if you have a fixed set of columns, like</p> 
  <pre><code>val table = "t1"
val cf = "cf1"
val headers: Seq[Q] = ...
val rdd: RDD[(K, Seq[V])] = ...
rdd.toHBaseBulk(table, cf, headers)
</code></pre> 
  <p>where <code>headers</code> are column names for <code>Seq[A]</code> values.</p> 
  <p>If you need to write timestamps, you can use a tuple <code>(A, Long)</code> in your <code>RDD</code>, where the second element represents the timestamp, like</p> 
  <pre><code>val rdd: RDD[(K, Map[String, Map[Q, (V, Long)]])] = ...
</code></pre> 
  <p>or, for the simplified form, like</p> 
  <pre><code>val rdd: RDD[(K, Map[Q, (V, Long)])] = ...
</code></pre> 
  <p>or, in case of a fixed set of columns, like</p> 
  <pre><code>val rdd: RDD[(K, Seq[(V, Long)])] = ...
</code></pre> 
  <p>But what about step 1? For this, an <code>Admin</code> object with a few helper methods come to the rescue. You must open a connection to HBase (as required since version 1.0.0), by instancing it</p> 
  <pre><code>  val admin = Admin()
</code></pre> 
  <p>and then</p> 
  <ul> 
   <li><code>admin.tableExists(tableName: String, family: String)</code>: checks if the table exists, and returns true or false accordingly. If the table <code>tableName</code> exists but the column family <code>family</code> does not, an <code>IllegalArgumentException</code> is thrown</li> 
   <li><code>admin.tableExists(tableName: String, families: Set[String])</code>: checks if the table exists, and returns true or false accordingly. If the table <code>tableName</code> exists but at least one of <code>families</code> does not, an <code>IllegalArgumentException</code> is thrown</li> 
   <li><code>admin.snapshot(tableName: String)</code>: creates a snapshot of table <code>tableName</code>, named <code>&lt;tablename&gt;_yyyyMMddHHmmss</code> (suffix is the date and time of the snapshot operation)</li> 
   <li><code>admin.snapshot(tableName: String, snapshotName: String)</code>: creates a snapshot of table <code>tableName</code>, named `snapshotName</li> 
   <li><code>admin.createTable(tableName: String, family: String, splitKeys: Seq[K])</code>: creates a table <code>tableName</code> with column family <code>family</code> and regions defined by a sorted sequence of split keys <code>splitKeys</code></li> 
   <li><code>admin.createTable(tableName: String, families: Set[String], splitKeys: Seq[K])</code>: creates a table <code>tableName</code> with column families <code>families</code> and regions defined by a sorted sequence of split keys <code>splitKeys</code></li> 
   <li><code>admin.createTable(tableName: String, families: Set[String])</code>: creates a table <code>tableName</code> with column families <code>families</code></li> 
   <li><code>admin.createTable(tableName: String, families: String*)</code>: creates a table <code>tableName</code> with column families <code>families</code></li> 
   <li><code>admin.disableTable(tableName: String)</code>: disables table <code>tableName</code> (a table must be disabled before deletion)</li> 
   <li><code>admin.deleteTable(tableName: String)</code>: deletes table <code>tableName</code></li> 
   <li><code>admin.truncateTable(tableName: String, preserveSplits: Boolean)</code>: truncates table <code>tableName</code>, optionally preserving region splits</li> 
   <li><code>admin.computeSplits(rdd: RDD[K], regionsCount: Int)</code>: given an <code>RDD</code>of keys and desired number of regions (<code>regionsCount</code>), returns a sorted sequence of split keys, to be used with <code>createTable()</code></li> 
  </ul> 
  <p>finally you must close the connection to HBase with</p> 
  <pre><code>admin.close
</code></pre> 
  <p>You can have a look at <code>ImportTsvToHFiles.scala</code> in <a href="https://github.com/unicredit/hbase-rdd-examples" target="_blank">hbase-rdd-examples project</a> on how to bulk load a TSV file from <code>Hdfs</code> to <code>HBase</code></p> 
  <h4><a id="user-content-set-the-number-of-hfiles-per-region-per-family" class="anchor" href="https://github.com/unicredit/hbase-rdd#set-the-number-of-hfiles-per-region-per-family" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Set the Number of HFiles per Region per Family</h4> 
  <p>For best performance, HBase should use 1 HFile per region per family. On the other hand, the more HFiles you use, the more partitions you have in your Spark job, hence Spark tasks run faster and consume less memory heap. You can fine tune this opposite requirement by passing an additional optional parameter to <code>toHBaseBulk()</code> method, <code>numFilesPerRegionPerFamily=&lt;N&gt;</code> where N (default is 1) is a number between 1 and <code>hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily</code> parameter (default is 32), e.g.</p> 
  <pre><code>rdd.toHBaseBulk(table, numFilesPerRegionPerFamily=32)
</code></pre> 
  <p>or</p> 
  <pre><code>rdd.toHBaseBulk(table, cf, numFilesPerRegionPerFamily=32)
</code></pre> 
  <p>or</p> 
  <pre><code>rdd.toHBaseBulk(table, cf, headers, numFilesPerRegionPerFamily=32)
</code></pre> 
 </article>
</div>