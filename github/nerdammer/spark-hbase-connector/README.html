<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-spark-hbase-connector" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#spark-hbase-connector" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark-HBase Connector</h1> 
  <p><a href="https://travis-ci.org/nerdammer/spark-hbase-connector" target="_blank"><img src="https://camo.githubusercontent.com/cf2764f434a0bef0aa9540f70136f8d60e139d17/68747470733a2f2f7472617669732d63692e6f72672f6e657264616d6d65722f737061726b2d68626173652d636f6e6e6563746f722e7376673f6272616e63683d6d6173746572" alt="Build status" data-canonical-src="https://travis-ci.org/nerdammer/spark-hbase-connector.svg?branch=master" style="max-width:100%;"></a></p> 
  <p>This library lets your Apache Spark application interact with Apache HBase using a simple and elegant API.</p> 
  <p>If you want to read and write data to HBase, you don't need using the Hadoop API anymore, you can just use Spark.</p> 
  <h2><a id="user-content-including-the-library" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#including-the-library" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Including the library</h2> 
  <p>The spark-hbase-connector is available in Sonatype repository. You can just add the following dependency in <code>sbt</code>:</p> 
  <pre><code>libraryDependencies += "it.nerdammer.bigdata" % "spark-hbase-connector_2.10" % "1.0.3"
</code></pre> 
  <p>The Maven style version of the dependency is:</p> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;<span class="pl-ent">dependency</span>&gt;
  &lt;<span class="pl-ent">groupId</span>&gt;it.nerdammer.bigdata&lt;/<span class="pl-ent">groupId</span>&gt;
  &lt;<span class="pl-ent">artifactId</span>&gt;spark-hbase-connector_2.10&lt;/<span class="pl-ent">artifactId</span>&gt;
  &lt;<span class="pl-ent">version</span>&gt;1.0.3&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre>
  </div> 
  <p>If you don't like sbt or Maven, you can also check out this Github repo and execute the following command from the root folder:</p> 
  <pre><code>sbt package
</code></pre> 
  <p>SBT will create the library jar under <code>target/scala-2.10</code>.</p> 
  <p>Note that the library depends on the following artifacts:</p> 
  <pre><code>libraryDependencies +=  "org.apache.spark" % "spark-core_2.10" % "1.6.0" % "provided"

libraryDependencies +=  "org.apache.hbase" % "hbase-common" % "1.0.3" excludeAll(ExclusionRule(organization = "javax.servlet", name="javax.servlet-api"), ExclusionRule(organization = "org.mortbay.jetty", name="jetty"), ExclusionRule(organization = "org.mortbay.jetty", name="servlet-api-2.5"))

libraryDependencies +=  "org.apache.hbase" % "hbase-client" % "1.0.3" excludeAll(ExclusionRule(organization = "javax.servlet", name="javax.servlet-api"), ExclusionRule(organization = "org.mortbay.jetty", name="jetty"), ExclusionRule(organization = "org.mortbay.jetty", name="servlet-api-2.5"))

libraryDependencies +=  "org.apache.hbase" % "hbase-server" % "1.0.3" excludeAll(ExclusionRule(organization = "javax.servlet", name="javax.servlet-api"), ExclusionRule(organization = "org.mortbay.jetty", name="jetty"), ExclusionRule(organization = "org.mortbay.jetty", name="servlet-api-2.5"))


libraryDependencies +=  "org.scalatest" % "scalatest_2.10" % "2.2.4" % "test"

</code></pre> 
  <p>Check also if the current branch is passing all tests in Travis-CI before checking out (See "build" icon above).</p> 
  <h2><a id="user-content-setting-the-hbase-host" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#setting-the-hbase-host" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Setting the HBase host</h2> 
  <p>The HBase Zookeeper quorum host can be set in multiple ways.</p> 
  <p>(1) Passing the host to the <code>spark-submit</code> command:</p> 
  <pre><code>spark-submit --conf spark.hbase.host=thehost ...
</code></pre> 
  <p>(2) Using the hbase-site.xml file (in the root of your jar, i.e. <code>src/main/resources/hbase-site.xml</code>):</p> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;?<span class="pl-ent">xml</span><span class="pl-e"> version</span>=<span class="pl-s"><span class="pl-pds">"</span>1.0<span class="pl-pds">"</span></span><span class="pl-e"> encoding</span>=<span class="pl-s"><span class="pl-pds">"</span>UTF-8<span class="pl-pds">"</span></span>?&gt;
&lt;<span class="pl-ent">configuration</span>&gt;
    &lt;<span class="pl-ent">property</span>&gt;
        &lt;<span class="pl-ent">name</span>&gt;hbase.zookeeper.quorum&lt;/<span class="pl-ent">name</span>&gt;
        &lt;<span class="pl-ent">value</span>&gt;thehost&lt;/<span class="pl-ent">value</span>&gt;
    &lt;/<span class="pl-ent">property</span>&gt;

    <span class="pl-c"><span class="pl-c">&lt;!--</span> Put any other property here, it will be used <span class="pl-c">--&gt;</span></span>
&lt;/<span class="pl-ent">configuration</span>&gt;</pre>
  </div> 
  <p>(3) If you have access to the JVM parameters:</p> 
  <pre><code>java -Dspark.hbase.host=thehost -jar ....
</code></pre> 
  <p>(4) Using the <em>scala</em> code:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">sparkConf</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SparkConf</span>()
...
sparkConf.set(<span class="pl-s"><span class="pl-pds">"</span>spark.hbase.host<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>thehost<span class="pl-pds">"</span></span>)
...
<span class="pl-k">val</span> <span class="pl-en">sc</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SparkContext</span>(sparkConf)</pre>
  </div> 
  <h2><a id="user-content-writing-to-hbase-basic" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#writing-to-hbase-basic" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Writing to HBase (Basic)</h2> 
  <p>Writing to HBase is very easy. Remember to import the implicit conversions:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">it.nerdammer.spark.hbase.</span><span class="pl-v">_</span></pre>
  </div> 
  <p>You have just to create a sample RDD, as the following one:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.parallelize(<span class="pl-c1">1</span> to <span class="pl-c1">100</span>)
            .map(i <span class="pl-k">=&gt;</span> (i.toString, i<span class="pl-k">+</span><span class="pl-c1">1</span>, <span class="pl-s"><span class="pl-pds">"</span>Hello<span class="pl-pds">"</span></span>))</pre>
  </div> 
  <p>This <em>rdd</em> is made of tuples like <code>("1", 2, "Hello")</code> or <code>("27", 28, "Hello")</code>. The first element of each tuple is considered the <strong>row id</strong>, the others will be assigned to columns.</p> 
  <div class="highlight highlight-source-scala">
   <pre>rdd.toHBaseTable(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
    .toColumns(<span class="pl-s"><span class="pl-pds">"</span>column1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>column2<span class="pl-pds">"</span></span>)
    .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>mycf<span class="pl-pds">"</span></span>)
    .save()</pre>
  </div> 
  <p>You are done. HBase now contains <em>100</em> rows in table <em>mytable</em>, each row containing two values for columns <em>mycf:column1</em> and <em>mycf:column2</em>.</p> 
  <h2><a id="user-content-reading-from-hbase-basic" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#reading-from-hbase-basic" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Reading from HBase (Basic)</h2> 
  <p>Reading from HBase is easier. Remember to import the implicit conversions:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">it.nerdammer.spark.hbase.</span><span class="pl-v">_</span></pre>
  </div> 
  <p>If you want to read the data written in the previous example, you just need to write:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">hBaseRDD</span> <span class="pl-k">=</span> sc.hbaseTable[(<span class="pl-k">String</span>, <span class="pl-k">Int</span>, <span class="pl-k">String</span>)](<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
    .select(<span class="pl-s"><span class="pl-pds">"</span>column1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>column2<span class="pl-pds">"</span></span>)
    .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>mycf<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>Now <em>hBaseRDD</em> contains all the data found in the table. Each object in the RDD is a tuple containing (in order) the <em>row id</em>, the corresponding value of <em>column1</em> (Int) and <em>column2</em> (String).</p> 
  <p>If you don't want the <em>row id</em> but, you only want to see the columns, just remove the first element from the tuple specs:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">hBaseRDD</span> <span class="pl-k">=</span> sc.hbaseTable[(<span class="pl-k">Int</span>, <span class="pl-k">String</span>)](<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
    .select(<span class="pl-s"><span class="pl-pds">"</span>column1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>column2<span class="pl-pds">"</span></span>)
    .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>mycf<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>This way, only the columns that you have chosen will be selected.</p> 
  <h2><a id="user-content-other-topics" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#other-topics" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Other Topics</h2> 
  <h3><a id="user-content-filtering" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#filtering" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Filtering</h3> 
  <p>It is possible to filter the results by prefixes of row keys. Filtering also supports additional salting prefixes (see the <a href="https://github.com/nerdammer/spark-hbase-connector#salting" target="_blank">salting</a> section).</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.hbaseTable[(<span class="pl-k">String</span>, <span class="pl-k">String</span>)](<span class="pl-s"><span class="pl-pds">"</span>table<span class="pl-pds">"</span></span>)
      .select(<span class="pl-s"><span class="pl-pds">"</span>col<span class="pl-pds">"</span></span>)
      .inColumnFamily(columnFamily)
      .withStartRow(<span class="pl-s"><span class="pl-pds">"</span>00000<span class="pl-pds">"</span></span>)
      .withStopRow(<span class="pl-s"><span class="pl-pds">"</span>00500<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>The example above retrieves all rows having a row key <em>greater or equal</em> to <code>00000</code> and <em>lower</em> than <code>00500</code>. The options <code>withStartRow</code> and <code>withStopRow</code> can also be used separately.</p> 
  <h3><a id="user-content-managing-empty-columns" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#managing-empty-columns" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Managing Empty Columns</h3> 
  <p>Empty columns are managed by using <code>Option[T]</code> types:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.hbaseTable[(<span class="pl-en">Option</span>[<span class="pl-k">String</span>], <span class="pl-k">String</span>)](<span class="pl-s"><span class="pl-pds">"</span>table<span class="pl-pds">"</span></span>)
      .select(<span class="pl-s"><span class="pl-pds">"</span>column1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>column2<span class="pl-pds">"</span></span>)
      .inColumnFamily(columnFamily)

rdd.foreach(t <span class="pl-k">=&gt;</span> {
    <span class="pl-k">if</span>(t._1.nonEmpty) println(t._1.get)
})</pre>
  </div> 
  <p>You can use the <code>Option[T]</code> type every time you are not sure whether a given column is present in your HBase RDD.</p> 
  <h3><a id="user-content-using-different-column-families" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#using-different-column-families" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Using different column families</h3> 
  <p>Different column families can be used both when reading or writing an RDD.</p> 
  <div class="highlight highlight-source-scala">
   <pre>data.toHBaseTable(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
      .toColumns(<span class="pl-s"><span class="pl-pds">"</span>column1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>cf2:column2<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>cf1<span class="pl-pds">"</span></span>)
      .save()</pre>
  </div> 
  <p>In the example above, <code>cf1</code> refers only to <code>column1</code>, because <code>cf2:column2</code> is already <em>fully qualified</em>.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">count</span> <span class="pl-k">=</span> sc.hbaseTable[(<span class="pl-k">String</span>, <span class="pl-k">String</span>)](<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
      .select(<span class="pl-s"><span class="pl-pds">"</span>cf1:column1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>column2<span class="pl-pds">"</span></span>)
      inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>cf2<span class="pl-pds">"</span></span>)
      .count</pre>
  </div> 
  <p>In the <em>reading example</em> above, the default column family <code>cf2</code> applies only to <code>column2</code>.</p> 
  <h3><a id="user-content-usage-in-spark-streaming" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#usage-in-spark-streaming" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage in Spark Streaming</h3> 
  <p>The connector can be used in Spark Streaming applications with the same API.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> stream is a DStream[(Int, Int)]</span>

stream.foreachRDD(rdd <span class="pl-k">=&gt;</span>
    rdd.toHBaseTable(<span class="pl-s"><span class="pl-pds">"</span>table<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>cf<span class="pl-pds">"</span></span>)
      .toColumns(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>)
      .save()
    )</pre>
  </div> 
  <h2><a id="user-content-advanced" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#advanced" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Advanced</h2> 
  <h3><a id="user-content-salting-prefixes" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#salting-prefixes" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Salting Prefixes<a name="user-content-salting" target="_blank" href=""></a></h3> 
  <p>Salting is supported in reads and writes. Only <em>string valued row id</em> are supported at the moment, so salting prefixes should also be of <em>String</em> type.</p> 
  <div class="highlight highlight-source-scala">
   <pre>sc.parallelize(<span class="pl-c1">1</span> to <span class="pl-c1">1000</span>)
      .map(i <span class="pl-k">=&gt;</span> (pad(i.toString, <span class="pl-c1">5</span>), <span class="pl-s"><span class="pl-pds">"</span>A value<span class="pl-pds">"</span></span>))
      .toHBaseTable(table)
      .inColumnFamily(columnFamily)
      .toColumns(<span class="pl-s"><span class="pl-pds">"</span>col<span class="pl-pds">"</span></span>)
      .withSalting((<span class="pl-c1">0</span> to <span class="pl-c1">9</span>).map(s <span class="pl-k">=&gt;</span> s.toString))
      .save()</pre>
  </div> 
  <p>In the example above, each row id is composed of <em>5</em> digits: from <code>00001</code> to <code>01000</code>. The <em>salting</em> property adds a random digit in front, so you will have records like: <code>800001</code>, <code>600031</code>, ...</p> 
  <p>When reading the RDD, you have just to declare the salting type used in the table and ignore it when using bounds (startRow or stopRow). The library takes care of dealing with salting.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.hbaseTable[<span class="pl-k">String</span>](table)
      .select(<span class="pl-s"><span class="pl-pds">"</span>col<span class="pl-pds">"</span></span>)
      .inColumnFamily(columnFamily)
      .withStartRow(<span class="pl-s"><span class="pl-pds">"</span>00501<span class="pl-pds">"</span></span>)
      .withSalting((<span class="pl-c1">0</span> to <span class="pl-c1">9</span>).map(s <span class="pl-k">=&gt;</span> s.toString))</pre>
  </div> 
  <h3><a id="user-content-custom-mapping-with-case-classes" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#custom-mapping-with-case-classes" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Custom Mapping with Case Classes</h3> 
  <p>Custom mapping can be used in place of the default tuple-mapping technique. Just define a case class for your type:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">case</span> <span class="pl-k">class</span> <span class="pl-en">MyData</span>(<span class="pl-v">id</span>: <span class="pl-k">Int</span>, <span class="pl-v">prg</span>: <span class="pl-k">Int</span>, <span class="pl-v">name</span>: <span class="pl-k">String</span>)</pre>
  </div> 
  <p>and define an object that contains <em>implicit</em> writer and reader for your type</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myDataWriter</span><span class="pl-k">:</span> <span class="pl-en">FieldWriter</span>[<span class="pl-en">MyData</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">FieldWriter</span>[<span class="pl-en">MyData</span>] {
    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">map</span>(<span class="pl-v">data</span>: <span class="pl-en">MyData</span>)<span class="pl-k">:</span> <span class="pl-en">HBaseData</span> <span class="pl-k">=</span>
      <span class="pl-en">Seq</span>(
        <span class="pl-en">Some</span>(<span class="pl-en">Bytes</span>.toBytes(data.id)),
        <span class="pl-en">Some</span>(<span class="pl-en">Bytes</span>.toBytes(data.prg)),
        <span class="pl-en">Some</span>(<span class="pl-en">Bytes</span>.toBytes(data.name))
      )

    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">columns</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-s"><span class="pl-pds">"</span>prg<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>)
}</pre>
  </div> 
  <p>Do not forget to override the <em>columns</em> method.</p> 
  <p>Then, you can define an <em>implicit</em> reader:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myDataReader</span><span class="pl-k">:</span> <span class="pl-en">FieldReader</span>[<span class="pl-en">MyData</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">FieldReader</span>[<span class="pl-en">MyData</span>] {
    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">map</span>(<span class="pl-v">data</span>: <span class="pl-en">HBaseData</span>)<span class="pl-k">:</span> <span class="pl-en">MyData</span> <span class="pl-k">=</span> <span class="pl-en">MyData</span>(
      id <span class="pl-k">=</span> <span class="pl-en">Bytes</span>.toInt(data.head.get),
      prg <span class="pl-k">=</span> <span class="pl-en">Bytes</span>.toInt(data.drop(<span class="pl-c1">1</span>).head.get),
      name <span class="pl-k">=</span> <span class="pl-en">Bytes</span>.toString(data.drop(<span class="pl-c1">2</span>).head.get)
    )

    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">columns</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-s"><span class="pl-pds">"</span>prg<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>)
}</pre>
  </div> 
  <p>Once you have done, make sure that the implicits are imported and that it does not produce a non-serializable task (Spark will check it at runtime).</p> 
  <p>You can now use your converters easily:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">data</span> <span class="pl-k">=</span> sc.parallelize(<span class="pl-c1">1</span> to <span class="pl-c1">100</span>).map(i <span class="pl-k">=&gt;</span> <span class="pl-k">new</span> <span class="pl-en">MyData</span>(i, i, <span class="pl-s"><span class="pl-pds">"</span>Name<span class="pl-pds">"</span></span> <span class="pl-k">+</span> i.toString))
<span class="pl-c"><span class="pl-c">//</span> data is an RDD[MyData]</span>

data.toHBaseTable(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
  .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>mycf<span class="pl-pds">"</span></span>)
  .save()

<span class="pl-k">val</span> <span class="pl-en">read</span> <span class="pl-k">=</span> sc.hbaseTable[<span class="pl-en">MyData</span>](<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
  .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>mycf<span class="pl-pds">"</span></span>)
</pre>
  </div> 
  <p>The converters above are low level and use directly the HBase API. Since this connector provides you with many predefined converters for simple and complex types, probably you would like to reuse them. The new <em>FieldReaderProxy</em> and <em>FieldWriterProxy</em> API has been created for this purpose.</p> 
  <h3><a id="user-content-high-level-converters-using-fieldwriterproxy" class="anchor" href="https://github.com/nerdammer/spark-hbase-connector#high-level-converters-using-fieldwriterproxy" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>High-level converters using FieldWriterProxy</h3> 
  <p>You can create a new <em>FieldWriterProxy</em> by declaring a conversion from your custom type to a predefined type. In this case, the predefined type it is a tuple composed of three basic fields:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> MySimpleData is a case class</span>

<span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myDataWriter</span><span class="pl-k">:</span> <span class="pl-en">FieldWriter</span>[<span class="pl-en">MySimpleData</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">FieldWriterProxy</span>[<span class="pl-en">MySimpleData</span>, (<span class="pl-k">Int</span>, <span class="pl-k">Int</span>, <span class="pl-k">String</span>)] {

  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">convert</span>(<span class="pl-v">data</span>: <span class="pl-en">MySimpleData</span>) <span class="pl-k">=</span> (data.id, data.prg, data.name) <span class="pl-c"><span class="pl-c">//</span> the first element is the row id</span>

  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">columns</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-s"><span class="pl-pds">"</span>prg<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>)
}</pre>
  </div> 
  <p>The corresponding <em>FieldReaderProxy</em> converts back a tuple of three basic fields into objects of class <em>MySimpleData</em>:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myDataReader</span><span class="pl-k">:</span> <span class="pl-en">FieldReader</span>[<span class="pl-en">MySimpleData</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">FieldReaderProxy</span>[(<span class="pl-k">Int</span>, <span class="pl-k">Int</span>, <span class="pl-k">String</span>), <span class="pl-en">MySimpleData</span>] {

  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">columns</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-s"><span class="pl-pds">"</span>prg<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>)

  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">convert</span>(<span class="pl-v">data</span>: (<span class="pl-k">Int</span>, <span class="pl-k">Int</span>, <span class="pl-k">String</span>)) <span class="pl-k">=</span> <span class="pl-en">MySimpleData</span>(data._1, data._2, data._3)
}
</pre>
  </div> 
  <p>Note that we have not used the HBase API. Currently, <em>FieldWriterProxy</em> can read and write tuples up to 22 fields (including the row id).</p> 
 </article>
</div>