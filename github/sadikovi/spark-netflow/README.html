<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-spark-netflow" class="anchor" href="https://github.com/sadikovi/spark-netflow#spark-netflow" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>spark-netflow</h1> 
  <p>A library for reading NetFlow files from <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">Spark SQL</a>.</p> 
  <p><a href="https://travis-ci.org/sadikovi/spark-netflow" target="_blank"><img src="https://camo.githubusercontent.com/6321652a6918b148aa886bfeaaea14b10773f503/68747470733a2f2f7472617669732d63692e6f72672f736164696b6f76692f737061726b2d6e6574666c6f772e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/sadikovi/spark-netflow.svg?branch=master" style="max-width:100%;"></a> <a href="https://codecov.io/gh/sadikovi/spark-netflow" target="_blank"><img src="https://camo.githubusercontent.com/73857996eab52a83101af3f52adbae81c7315115/68747470733a2f2f636f6465636f762e696f2f67682f736164696b6f76692f737061726b2d6e6574666c6f772f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/sadikovi/spark-netflow/branch/master/graph/badge.svg" style="max-width:100%;"></a></p> 
  <h2><a id="user-content-requirements" class="anchor" href="https://github.com/sadikovi/spark-netflow#requirements" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Requirements</h2> 
  <table>
   <thead> 
    <tr> 
     <th>Spark version</th> 
     <th>spark-netflow latest version</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td>1.4+</td> 
     <td><a href="http://spark-packages.org/package/sadikovi/spark-netflow" target="_blank">1.3.0</a></td> 
    </tr> 
   </tbody>
  </table> 
  <h2><a id="user-content-linking" class="anchor" href="https://github.com/sadikovi/spark-netflow#linking" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Linking</h2> 
  <p>The spark-netflow library can be added to Spark by using the <code>--packages</code> command line option. For example, run this to include it when starting the spark shell:</p> 
  <div class="highlight highlight-source-shell">
   <pre> <span class="pl-smi">$SPARK_HOME</span>/bin/spark-shell --packages sadikovi:spark-netflow:1.3.0-s_2.10</pre>
  </div> 
  <p>Change to <code>sadikovi:spark-netflow:1.3.0-s_2.11</code> for Scala 2.11.x</p> 
  <h2><a id="user-content-features" class="anchor" href="https://github.com/sadikovi/spark-netflow#features" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Features</h2> 
  <ul> 
   <li>Column pruning</li> 
   <li>Predicate pushdown to the NetFlow file</li> 
   <li>Auto statistics on <code>unix_secs</code> (filter records and entire files based on predicate)</li> 
   <li>Manual statistics on certain columns depending on version (when option provided)</li> 
   <li>Fields conversion (IP addresses, protocol, etc.)</li> 
   <li>NetFlow version 5 support (<a href="https://github.com/sadikovi/spark-netflow/blob/master/docs/NETFLOW_V5.md" target="_blank">list of columns</a>)</li> 
   <li>NetFlow version 7 support (<a href="https://github.com/sadikovi/spark-netflow/blob/master/docs/NETFLOW_V7.md" target="_blank">list of columns</a>)</li> 
   <li>Reading files from local file system and HDFS</li> 
   <li>Different partition strategies</li> 
  </ul> 
  <h3><a id="user-content-options" class="anchor" href="https://github.com/sadikovi/spark-netflow#options" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Options</h3> 
  <p>Currently supported options:</p> 
  <table>
   <thead> 
    <tr> 
     <th>Name</th> 
     <th align="center">Since</th> 
     <th align="center">Example</th> 
     <th>Description</th> 
    </tr> 
   </thead>
   <tbody> 
    <tr> 
     <td><code>version</code></td> 
     <td align="center"><code>0.0.1</code></td> 
     <td align="center"><em>5, 7</em></td> 
     <td>version to use when parsing NetFlow files, your own version provider can be passed, by default will resolve from files provided</td> 
    </tr> 
    <tr> 
     <td><code>buffer</code></td> 
     <td align="center"><code>0.0.2</code></td> 
     <td align="center"><em>1024, 32Kb, 3Mb, etc</em></td> 
     <td>buffer size for NetFlow compressed stream (default: <code>1Mb</code>)</td> 
    </tr> 
    <tr> 
     <td><code>stringify</code></td> 
     <td align="center"><code>0.0.2</code></td> 
     <td align="center"><em>true, false</em></td> 
     <td>convert certain fields (e.g. IP, protocol) into human-readable format, though it is recommended to turn it off when performance matters (default: <code>true</code>)</td> 
    </tr> 
    <tr> 
     <td><code>predicate-pushdown</code></td> 
     <td align="center"><code>0.2.0</code></td> 
     <td align="center"><em>true, false</em></td> 
     <td>use predicate pushdown at NetFlow library level (default: <code>true</code>)</td> 
    </tr> 
    <tr> 
     <td><code>partitions</code></td> 
     <td align="center"><code>0.2.1</code></td> 
     <td align="center"><em>default, auto, 1, 2, 100, 1024, etc</em></td> 
     <td>partition mode to use, can be <code>default</code>, <code>auto</code>, or any number of partitions (default: <code>default</code>)</td> 
    </tr> 
    <tr> 
     <td><code>statistics</code></td> 
     <td align="center"><code>1.0.0</code></td> 
     <td align="center"><em>true, false, file:/.../, hdfs://.../</em></td> 
     <td>use manual statistics for certain columns, see details for more information (default: <code>false</code>)</td> 
    </tr> 
   </tbody>
  </table> 
  <h3><a id="user-content-details-on-partition-mode" class="anchor" href="https://github.com/sadikovi/spark-netflow#details-on-partition-mode" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Details on partition mode</h3> 
  <p><strong>spark-netflow</strong> supports three different partition modes:</p> 
  <ul> 
   <li><code>default</code> mode puts every file into its own partition (default behaviour since the first version of package).</li> 
   <li>specific number of slices can be specified, e.g. <code>sqlContext.read.option("partitions", "210")</code>, this will use standard RDD functionality to split files into provided number of slices.</li> 
   <li><code>auto</code> will try to split provided files into partitions the best way possible following the rule that each partition should be as close as possible to the best partition size. Best partition size is chosen based on mean of the files' sizes (considering possible skewness of the dataset) and provided best size using <code>spark.sql.netflow.partition.size</code>, default is <code>144Mb</code>. Note that auto mode will not be triggered, if number of files is less than default minimum number of partitions <code>spark.sql.netflow.partition.num</code> with default <code>sparkContext.defaultParallelism * 2</code>. Still default values should be pretty good for most of the workloads, including compressed files.</li> 
  </ul> 
  <p>Tweak settings for auto mode:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> Best partition size (note that will be compared to the truncated mean of files provided)</span>
<span class="pl-c"><span class="pl-c">//</span> Chosen to keep roughly 10,000,000 records in each partition, if possible</span>
sqlContext.setConf(<span class="pl-s"><span class="pl-pds">"</span>spark.sql.netflow.partition.size<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>144Mb<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">//</span> Minimum number of partitions before considering auto mode, increases cluster utilization for</span>
<span class="pl-c"><span class="pl-c">//</span> small batches</span>
sqlContext.setConf(<span class="pl-s"><span class="pl-pds">"</span>spark.sql.netflow.partition.num<span class="pl-pds">"</span></span>, s<span class="pl-s"><span class="pl-pds">"</span>${sc.defaultParallelism * 2}<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a id="user-content-details-on-statistics" class="anchor" href="https://github.com/sadikovi/spark-netflow#details-on-statistics" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Details on statistics</h3> 
  <p><strong>spark-netflow</strong> supports collecting statistics for NetFlow files when option <code>statistics</code> is used. Currently there are several values supported:</p> 
  <ul> 
   <li><code>false</code> statistics are disabled, this is default value.</li> 
   <li><code>true</code> statistics are enabled, generated file is stored in the same directory as original file.</li> 
   <li><code>file:/.../</code> or <code>hdfs://.../</code> statistics are enabled, directory provided (can be local file system or HDFS) is considered to be a root of where statistics are stored. Package saves statistics files by reconstructing original file directory from the root provided, e.g. file location is <code>file:/tmp/netflow/ft-v5</code>, option value is <code>hdfs://.../dir</code>, then statistics file is stored as <code>hdfs://.../dir/tmp/netflow/.statistics-ft-v5</code>.</li> 
  </ul> 
  <p>Columns that are used to collect statistics are version dependent. For version 5 and 7 <code>srcip</code>, <code>dstip</code>, <code>srcport</code>, <code>dstport</code>, and <code>protocol</code> are used. Note that <strong>statistics/filtering on time are always enabled</strong> since they are provided by original file.</p> 
  <p>Statistics are either written lazily or read, if available. Package automatically figures out which files have and do not have statistics, and will perform writes or reads accordingly. Collecting statistics is lazy, package will only collect them when conditions are met, such as no filters specified when selecting data, columns that are selected contain all statistics columns, and all data is scanned/requested. The easiest way to trigger that is running <code>count()</code> on DataFrame. Using statistics does not require any special conditions apart from enabling option.</p> 
  <h3><a id="user-content-dealing-with-corrupt-files" class="anchor" href="https://github.com/sadikovi/spark-netflow#dealing-with-corrupt-files" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Dealing with corrupt files</h3> 
  <p>Package supports Spark option <code>spark.files.ignoreCorruptFiles</code>. When set to <code>true</code>, corrupt files are ignored (corrupt header, wrong format) or partially read (corrupt data block in a middle of a file). By default option is set to <code>false</code>, similar to Spark.</p> 
  <h2><a id="user-content-example" class="anchor" href="https://github.com/sadikovi/spark-netflow#example" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Example</h2> 
  <h3><a id="user-content-scala-api" class="anchor" href="https://github.com/sadikovi/spark-netflow#scala-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Scala API</h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> You can provide only format, package will infer version from provided files, or you can enforce</span>
<span class="pl-c"><span class="pl-c">//</span> version of the files with `version` option.</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.github.sadikovi.spark.netflow<span class="pl-pds">"</span></span>).load(<span class="pl-s"><span class="pl-pds">"</span>...<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> You can read files from local file system or HDFS</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.github.sadikovi.spark.netflow<span class="pl-pds">"</span></span>).
  option(<span class="pl-s"><span class="pl-pds">"</span>version<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>5<span class="pl-pds">"</span></span>).load(<span class="pl-s"><span class="pl-pds">"</span>file:/...<span class="pl-pds">"</span></span>).
  select(<span class="pl-s"><span class="pl-pds">"</span>srcip<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>dstip<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>packets<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> You can also specify buffer size when reading compressed NetFlow files</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.github.sadikovi.spark.netflow<span class="pl-pds">"</span></span>).
  option(<span class="pl-s"><span class="pl-pds">"</span>version<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>5<span class="pl-pds">"</span></span>).option(<span class="pl-s"><span class="pl-pds">"</span>buffer<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>50Mb<span class="pl-pds">"</span></span>).load(<span class="pl-s"><span class="pl-pds">"</span>hdfs://sandbox:8020/tmp/...<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>Alternatively you can use shortcuts for NetFlow files</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.github.sadikovi.spark.netflow.</span><span class="pl-v">_</span>

<span class="pl-c"><span class="pl-c">//</span> this will read version 5 with default buffer size</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read.netflow5(<span class="pl-s"><span class="pl-pds">"</span>hdfs:/...<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> this will read version 7 without fields conversion</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read.option(<span class="pl-s"><span class="pl-pds">"</span>stringify<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>false<span class="pl-pds">"</span></span>).netflow7(<span class="pl-s"><span class="pl-pds">"</span>file:/...<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a id="user-content-python-api" class="anchor" href="https://github.com/sadikovi/spark-netflow#python-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Python API</h3> 
  <div class="highlight highlight-source-python">
   <pre>df <span class="pl-k">=</span> sqlContext.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.github.sadikovi.spark.netflow<span class="pl-pds">"</span></span>).option(<span class="pl-s"><span class="pl-pds">"</span>version<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>5<span class="pl-pds">"</span></span>).
  load(<span class="pl-s"><span class="pl-pds">"</span>file:/...<span class="pl-pds">"</span></span>).select(<span class="pl-s"><span class="pl-pds">"</span>srcip<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>srcport<span class="pl-pds">"</span></span>)

res <span class="pl-k">=</span> df.where(<span class="pl-s"><span class="pl-pds">"</span>srcip &gt; 10<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a id="user-content-sql-api" class="anchor" href="https://github.com/sadikovi/spark-netflow#sql-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SQL API</h3> 
  <div class="highlight highlight-source-sql">
   <pre>CREATE TEMPORARY TABLE ips
USING <span class="pl-c1">com</span>.<span class="pl-c1">github</span>.<span class="pl-c1">sadikovi</span>.<span class="pl-c1">spark</span>.netflow
OPTIONS (<span class="pl-k">path</span> <span class="pl-s"><span class="pl-pds">"</span>file:/...<span class="pl-pds">"</span></span>, version <span class="pl-s"><span class="pl-pds">"</span>5<span class="pl-pds">"</span></span>);

<span class="pl-k">SELECT</span> srcip, dstip, srcport, dstport <span class="pl-k">FROM</span> ips <span class="pl-k">LIMIT</span> <span class="pl-c1">10</span>;</pre>
  </div> 
  <h2><a id="user-content-building-from-source" class="anchor" href="https://github.com/sadikovi/spark-netflow#building-from-source" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Building From Source</h2> 
  <p>This library is built using <code>sbt</code>, to build a JAR file simply run <code>sbt package</code> from project root.</p> 
  <h2><a id="user-content-testing" class="anchor" href="https://github.com/sadikovi/spark-netflow#testing" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Testing</h2> 
  <p>Run <code>sbt test</code> from project root.</p> 
  <h2><a id="user-content-running-benchmark" class="anchor" href="https://github.com/sadikovi/spark-netflow#running-benchmark" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Running benchmark</h2> 
  <p>Run <code>sbt package</code> to package project, next run <code>spark-submit</code> with following options:</p> 
  <div class="highlight highlight-source-shell">
   <pre>$ spark-submit --class com.github.sadikovi.spark.benchmark.NetFlowReadBenchmark \
  target/scala-2.10/spark-netflow_2.10-1.3.0.jar \
  --iterations 5 \
  --files <span class="pl-s"><span class="pl-pds">'</span>file:/Users/sadikovi/developer/spark-netflow/temp/ftn/0[1,2,3]/ft*<span class="pl-pds">'</span></span> \
  --version 5</pre>
  </div> 
  <p>Latest benchmarks:</p> 
  <pre><code>- Iterations: 5
- Files: file:/Users/sadikovi/developer/spark-netflow/temp/ftn/0[1,2,3]/ft*
- Version: 5
Running benchmark: NetFlow full scan
  Running case: Scan, stringify = F
  Running case: Scan, stringify = T                                             

Intel(R) Core(TM) i5-4258U CPU @ 2.40GHz
NetFlow full scan:                  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------
Scan, stringify = F                       593 /  671       1686.2       59303.3       1.0X
Scan, stringify = T                      1264 / 1280        790.9      126431.2       0.5X

Running benchmark: NetFlow predicate scan
  Running case: Predicate pushdown = F, high
  Running case: Predicate pushdown = T, high                                    
  Running case: Predicate pushdown = F, low                                     
  Running case: Predicate pushdown = T, low                                     

Intel(R) Core(TM) i5-4258U CPU @ 2.40GHz
NetFlow predicate scan:             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------
Predicate pushdown = F, high             1294 / 1305        773.0      129361.1       1.0X
Predicate pushdown = T, high             1306 / 1330        765.5      130628.6       1.0X
Predicate pushdown = F, low              1081 / 1127        924.8      108129.1       1.2X
Predicate pushdown = T, low               272 /  275       3673.6       27221.0       4.8X

Running benchmark: NetFlow aggregated report
  Running case: Aggregated report

Intel(R) Core(TM) i5-4258U CPU @ 2.40GHz
NetFlow aggregated report:          Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------
Aggregated report                        1551 / 1690        644.6      155143.4       1.0X
</code></pre> 
  <h2><a id="user-content-using-netflowlib-library-separately" class="anchor" href="https://github.com/sadikovi/spark-netflow#using-netflowlib-library-separately" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Using <code>netflowlib</code> library separately</h2> 
  <p>You can use <code>netflowlib</code> without using <code>spark-netflow</code> package. Here some basic concepts and examples:</p> 
  <ul> 
   <li><code>com.github.sadikovi.netflowlib.predicate.Columns.*</code> all available column types in the library, check out <code>com.github.sadikovi.netflowlib.version.*</code> classes to see what columns are already defined for a specific NetFlow format.</li> 
   <li><code>com.github.sadikovi.netflowlib.predicate.FilterApi</code> utility class to create predicates for NetFlow file</li> 
   <li><code>com.github.sadikovi.netflowlib.statistics.StatisticsTypes</code> statistics that you can use to reduce boundaries of filter or allow filter to be evaluated before scanning the file. For example, library creates statistics on time, so time filter can be resolved upfront</li> 
   <li><code>com.github.sadikovi.netflowlib.NetFlowReader</code> main entry to work with NetFlow file, gives access to file header and iterator of rows, allows to pass additional predicate and statistics</li> 
   <li><code>com.github.sadikovi.netflowlib.NetFlowHeader</code> header information can be accessed using this class from <code>NetFlowReader.getHeader()</code>, see class for more information on flags available</li> 
  </ul> 
  <p>Note that library has only one external dependency on <code>io.netty.buffer.ByteBuf</code> buffers, which could be replaced with standard Java buffer functionality, but since it was built for being used as part of a spark-package, this dependency comes with Spark.</p> 
  <p>Here is the general usage pattern:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.github.sadikovi.netflowlib.</span><span class="pl-v">NetFlowReader</span>
<span class="pl-k">import</span> <span class="pl-v">com.github.sadikovi.netflowlib.version.</span><span class="pl-v">NetFlowV5</span>

<span class="pl-c"><span class="pl-c">//</span> Create input stream by opening NetFlow file, e.g. `fs.open(hadoopFile)`</span>
<span class="pl-k">val</span> <span class="pl-en">stm</span><span class="pl-k">:</span> <span class="pl-en">DataInputStream</span> <span class="pl-k">=</span> ...
<span class="pl-c"><span class="pl-c">//</span> Prepare reader based on input stream and buffer size, you can use</span>
<span class="pl-c"><span class="pl-c">//</span> overloaded alternative with default buffer size</span>
<span class="pl-k">val</span> <span class="pl-en">reader</span> <span class="pl-k">=</span> <span class="pl-en">NetFlowReader</span>.prepareReader(stm, <span class="pl-c1">10000</span>)
<span class="pl-c"><span class="pl-c">//</span> Check out header, optional</span>
<span class="pl-k">val</span> <span class="pl-en">header</span> <span class="pl-k">=</span> reader.getHeader()
<span class="pl-c"><span class="pl-c">//</span> Actual NetFlow version of the file</span>
<span class="pl-k">val</span> <span class="pl-en">actualVersion</span> <span class="pl-k">=</span> header.getFlowVersion()
<span class="pl-c"><span class="pl-c">//</span> Whether or not file is compressed</span>
<span class="pl-k">val</span> <span class="pl-en">isCompressed</span> <span class="pl-k">=</span> header.isCompressed()

<span class="pl-c"><span class="pl-c">//</span> This is list of fields that will be returned in iterator as values in</span>
<span class="pl-c"><span class="pl-c">//</span> array (same order)</span>
<span class="pl-k">val</span> <span class="pl-en">fields</span> <span class="pl-k">=</span> <span class="pl-en">Array</span>(
  <span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_UNIX_SECS</span>,
  <span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_SRCADDR</span>,
  <span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_DSTADDR</span>,
  <span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_SRCPORT</span>,
  <span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_DSTPORT</span>
)

<span class="pl-c"><span class="pl-c">//</span> Build record buffer and iterator that you can use to get values.</span>
<span class="pl-c"><span class="pl-c">//</span> Note that you can also use set of filters, if you want to get</span>
<span class="pl-c"><span class="pl-c">//</span> particular records</span>
<span class="pl-k">val</span> <span class="pl-en">recordBuffer</span> <span class="pl-k">=</span> reader.prepareRecordBuffer(fields)
<span class="pl-k">val</span> <span class="pl-en">iter</span> <span class="pl-k">=</span> recordBuffer.iterator()

<span class="pl-k">while</span> (iter.hasNext) {
  <span class="pl-c"><span class="pl-c">//</span> print every row with values</span>
  println(iter.next)
}</pre>
  </div> 
  <p>Here is an example of using predicate to keep certain records:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.github.sadikovi.netflowlib.predicate.</span><span class="pl-v">FilterApi</span>
<span class="pl-k">val</span> <span class="pl-en">predicate</span> <span class="pl-k">=</span> <span class="pl-en">FilterApi</span>.and(
  <span class="pl-en">FilterApi</span>.eq(<span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_SRCPORT</span>, <span class="pl-c1">123</span>),
  <span class="pl-en">FilterApi</span>.eq(<span class="pl-en">NetFlowV5</span>.<span class="pl-en">FIELD_DSTPORT</span>, <span class="pl-c1">456</span>)
)

...
<span class="pl-k">val</span> <span class="pl-en">recordBuffer</span> <span class="pl-k">=</span> reader.prepareRecordBuffer(fields, predicate)</pre>
  </div> 
 </article>
</div>