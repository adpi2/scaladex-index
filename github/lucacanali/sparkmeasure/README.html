<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/lucacanali/sparkmeasure#sparkmeasure" aria-hidden="true" class="anchor" id="user-content-sparkmeasure" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>sparkMeasure</h1> 
  <p><strong>sparkMeasure is a tool for performance investigations of Apache Spark workloads.</strong><br> It simplifies the collection and analysis of Spark performance metrics. It is intended also as proof-of-concept code on how to use Spark listeners for custom metrics collection.</p> 
  <ul> 
   <li> <p>Developed and tested for Spark 2.1.0 and 2.1.1, 2.2.0</p> 
    <ul> 
     <li>Latest development version 0.12-SNAPSHOT, last modified July 2017</li> 
    </ul> </li> 
   <li> <p>Created and maintained by: <a href="mailto:Luca.Canali@cern.ch" target="_blank">Luca.Canali@cern.ch</a></p> 
    <ul> 
     <li>Additional credits to: Viktor Khristenko</li> 
    </ul> </li> 
   <li> <p><a href="https://mvnrepository.com/artifact/ch.cern.sparkmeasure" target="_blank">https://mvnrepository.com/artifact/ch.cern.sparkmeasure</a> - sparkMeasure on Maven Central</p> </li> 
   <li> <p><a href="http://db-blog.web.cern.ch/blog/luca-canali/2017-03-measuring-apache-spark-workload-metrics-performance-troubleshooting" target="_blank">Link to the accompanying blog post</a></p> </li> 
  </ul> 
  <p><strong>Where sparkMeasure can be useful:</strong></p> 
  <ul> 
   <li>Performance investigations: Measure and analyze performance interactively from spark-shell (Scala), pyspark (Python) or Jupyter notebooks</li> 
   <li>Inside your code: add instrumentation calls in your code to use sparkMeasure custom Listeners and/or use the classes StageMetrics/TaskMetrics and related APIs for collecting, analyzing and optionally saving metrics data</li> 
   <li>Instrument code that you cannot change: use sparkMeasure in the "Flight Recorde"r mode, this records the performance metrics automatically and saves data for later processing</li> 
  </ul> 
  <p><strong>Main concepts underlying sparkMeasure:</strong></p> 
  <ul> 
   <li>The tool is based on the Spark Listener interface, that is used as source for Spark workload metrics data.</li> 
   <li>Metrics are collected at the granularity or stage and task (configurable)</li> 
   <li>Metrics are flattened and collected into a ListBuffer of a case class.</li> 
   <li>Data is then transformed into a Spark DataFrame for analysis.</li> 
   <li>Data can be saved for offline analysis</li> 
  </ul> 
  <p><strong>How to use:</strong> use sbt to compile are package the jar, or use the package on Maven Central. Example:</p> 
  <div class="highlight highlight-source-scala">
   <pre>bin<span class="pl-k">/</span>spark<span class="pl-k">-</span>shell <span class="pl-k">--</span>packages ch.cern.sparkmeasure<span class="pl-k">:</span>spark<span class="pl-k">-</span>measure_2<span class="pl-c1">.11</span><span class="pl-k">:</span><span class="pl-c1">0.11</span></pre>
  </div> 
  <p>or use the jar as in :</p> 
  <div class="highlight highlight-source-scala">
   <pre>spark<span class="pl-k">-</span>submit<span class="pl-k">/</span>pyspark<span class="pl-k">/</span>spark<span class="pl-k">-</span>shell <span class="pl-k">--</span>jars spark<span class="pl-k">-</span>measure_2<span class="pl-c1">.11</span><span class="pl-k">-</span><span class="pl-c1">0.12</span><span class="pl-k">-</span><span class="pl-en">SNAPSHOT</span>.jar</pre>
  </div> 
  <p><strong>Examples</strong></p> 
  <ol> 
   <li>Measure metrics at the Stage level (example in Scala):</li> 
  </ol> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">stageMetrics</span> <span class="pl-k">=</span> ch.cern.sparkmeasure.<span class="pl-en">StageMetrics</span>(spark) 
stageMetrics.runAndMeasure(spark.sql(<span class="pl-s"><span class="pl-pds">"</span>select count(*) from range(1000) cross join range(1000) cross join range(1000)<span class="pl-pds">"</span></span>).show)</pre>
  </div> 
  <ol start="2"> 
   <li>This is an alternative way to collect and print metrics (Scala):</li> 
  </ol> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">stageMetrics</span> <span class="pl-k">=</span> ch.cern.sparkmeasure.<span class="pl-en">StageMetrics</span>(spark) 
stageMetrics.begin()

...execute one or more <span class="pl-en">Spark</span> jobs...

stageMetrics.end()
stageMetrics.printReport()
stageMetrics.printAccumulables</pre>
  </div> 
  <ol start="3"> 
   <li>Print additional accumulables metrics (including SQL metrics) collected at stage-level, Scala:</li> 
  </ol> 
  <div class="highlight highlight-source-scala">
   <pre>stageMetrics.printAccumulables()</pre>
  </div> 
  <ol start="4"> 
   <li>Collect and report Task metrics, Scala:</li> 
  </ol> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">taskMetrics</span> <span class="pl-k">=</span> ch.cern.sparkmeasure.<span class="pl-en">TaskMetrics</span>(spark)
taskMetrics.runAndMeasure(spark.sql(<span class="pl-s"><span class="pl-pds">"</span>select count(*) from range(1000) cross join range(1000) cross join range(1000)<span class="pl-pds">"</span></span>).show)</pre>
  </div> 
  <ol start="5"> 
   <li>How to collect stage metrics, example in Python:</li> 
  </ol> 
  <div class="highlight highlight-source-python">
   <pre>stageMetrics <span class="pl-k">=</span> sc._jvm.ch.cern.sparkmeasure.StageMetrics(spark._jsparkSession)
stageMetrics.begin()
spark.sql(<span class="pl-s"><span class="pl-pds">"</span>select count(*) from range(1000) cross join range(1000) cross join range(1000)<span class="pl-pds">"</span></span>).show()
stageMetrics.end()
stageMetrics.printReport()
stageMetrics.printAccumulables()</pre>
  </div> 
  <ol start="6"> 
   <li>How to collect task metrics, example in Python:</li> 
  </ol> 
  <div class="highlight highlight-source-python">
   <pre>taskMetrics <span class="pl-k">=</span> sc._jvm.ch.cern.sparkmeasure.TaskMetrics(spark._jsparkSession, <span class="pl-c1">False</span>)
taskMetrics.begin()
spark.sql(<span class="pl-s"><span class="pl-pds">"</span>select count(*) from range(1000) cross join range(1000) cross join range(1000)<span class="pl-pds">"</span></span>).show()
taskMetrics.end()
taskMetrics.printReport()

<span class="pl-c"><span class="pl-c">#</span> As an alternative to using begin() and end(), you can run the following:</span>
df <span class="pl-k">=</span> taskMetrics.createTaskMetricsDF(<span class="pl-s"><span class="pl-pds">"</span>PerfTaskMetrics<span class="pl-pds">"</span></span>)
spark.sql(<span class="pl-s"><span class="pl-pds">"</span>select * from PerfTaskMetrics<span class="pl-pds">"</span></span>).show()
df.show()
taskMetrics.saveData(df, <span class="pl-s"><span class="pl-pds">"</span>taskmetrics_test1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>json<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p><strong>Flight Recorder mode</strong> This is for instrumenting Spark applications without touching their code. Just add an extra custom listener that will record the metrics of interest and save to a file at the end of the application.</p> 
  <ul> 
   <li>For recording stage metrics: <code>--conf spark.extraListeners=ch.cern.sparkmeasure.FlightRecorderStageMetrics</code></li> 
   <li>For recording task-level metrics: <code>--conf spark.extraListeners=ch.cern.sparkmeasure.FlightRecorderTaskMetrics</code></li> 
  </ul> 
  <p>To post-process the saved metrics you will need to deserialize objects saved by the flight mode. This is an example of how to do that using the supplied helper object sparkmeasure.Utils</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">m1</span> <span class="pl-k">=</span> ch.cern.sparkmeasure.<span class="pl-en">Utils</span>.readSerializedStageMetrics(<span class="pl-s"><span class="pl-pds">"</span>/tmp/stageMetrics.serialized<span class="pl-pds">"</span></span>)
m1.toDF.show</pre>
  </div> 
  <p><strong>Analysis of performance metrics:</strong><br> One of the key features of sparkMeasure is that it makes data easily accessible for analysis.<br> This is achieved by exporting the collected data into Spark DataFrames where they can be queries with Spark APIs and/or SQL. In addition the metrics can be used for plotting and other visualizations, for example using Jupyter notebooks.</p> 
  <p>Example of analysis of Task Metrics using a Jupyter notebook at: <a href="https://github.com/lucacanali/sparkmeasure/blob/master/examples/SparkTaskMetricsAnalysisExample.ipynb" target="_blank">SparkTaskMetricsAnalysisExample.ipynb</a></p> 
  <p>Additional example code:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> export task metrics collected by the Listener into a DataFrame and registers as a temporary view </span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> taskMetrics.createTaskMetricsDF(<span class="pl-s"><span class="pl-pds">"</span>PerfTaskMetrics<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> other option: read metrics previously saved on a json file</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> spark.read.json(<span class="pl-s"><span class="pl-pds">"</span>taskmetrics_test1<span class="pl-pds">"</span></span>)
df.createOrReplaceTempView(<span class="pl-s"><span class="pl-pds">"</span>PerfTaskMetrics<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> show the top 5 tasks by duration</span>
spark.sql(<span class="pl-s"><span class="pl-pds">"</span>select jobId, host, duration from PerfTaskMetrics order by duration desc limit 5<span class="pl-pds">"</span></span>).show()
<span class="pl-c"><span class="pl-c">//</span> show the available metrics</span>
spark.sql(<span class="pl-s"><span class="pl-pds">"</span>desc PerfTaskMetrics<span class="pl-pds">"</span></span>).show()</pre>
  </div> 
  <hr> 
  <p><strong>Additional info on Stage Metrics implementation:</strong></p> 
  <ul> 
   <li> <p>class StageInfoRecorderListener extends SparkListener</p> 
    <ul> 
     <li>Collects metrics at the end of each Stage</li> 
     <li>This is the main engine to collect metrics. Metrics are collected in a ListBuffer of case class StageVals for metrics generating from TaskMetrics and in a ListBuffer of accumulablesInfo for metrics generated from "accumulables".</li> 
    </ul> </li> 
   <li> <p>case class StageVals -&gt; used to collect and store "flatten" the stageinfo and TaskMetric info collected by the Listener. Metrics are aggregated per stage and include: executor run time, CPU time, shuffle read and write time, serialization and deserialization time, HDFS I/O metrics, etc</p> </li> 
   <li> <p>case class accumulablesInfo -&gt; used to collect and store the metrics of type "accumulables"</p> </li> 
   <li> <p>case class StageMetrics(sparkSession: SparkSession)</p> 
    <ul> 
     <li>Helper class to help in collecting and storing performance metrics. It provides wrapper methods to add the listener to the Spark Context (ListenerBus) and other other methods for analysis. When you instantiate this class you start collecting stage metrics data.</li> 
     <li>def begin() and def end() methods -&gt; use them at mark beginning and end of data collection if you plan to use printReport()</li> 
     <li>def createStageMetricsDF(nameTempView: String = "PerfStageMetrics"): DataFrame -&gt; converts the ListBuffer with stage metrics into a DataFrame and creates a temporary view, useful for data analytics</li> 
     <li>def createAccumulablesDF(nameTempView: String = "AccumulablesStageMetrics"): DataFrame -&gt; converts the accumulables aggregated at stage level in a ListBuffer into a DataFrame and temporary view</li> 
     <li>def printReport(): Unit -&gt; prints a report of the metrics in "PerfStageMetrics" between the timestamps: beginSnapshot and endSnapshot</li> 
     <li>def printAccumulables(): Unit -&gt; prints the accumulables metrics divided in 2 groups: internal metrics (which are basically the same as TaskMetrics) and the rest (typically metrics generated custom by parts of the SQL execution engine)</li> 
     <li>def runAndMeasure[T](f: =&gt; T): T -&gt; a handy extension to do 3 actions: runs the Spark workload, measure its metrics and print the report. You can see this as an extension of spark.time() command</li> 
     <li>def saveData(df: DataFrame, fileName: String, fileFormat: String = "json") -&gt; helper method to save metrics data collected in a DataFrame for later analysis/plotting</li> 
    </ul> </li> 
  </ul> 
  <p><strong>Additional info on Task Metrics:</strong></p> 
  <ul> 
   <li>case class TaskMetrics(sparkSession: SparkSession, gatherAccumulables: Boolean = false) 
    <ul> 
     <li>Collects metrics at the end of each Task</li> 
     <li>This is the main engine to collect metrics. Metrics are collected in a ListBuffer of case class TaskVals</li> 
     <li>optionally gathers accumulabels (with task metrics and SQL metrics per task if gatherAccumulables is set to true)</li> 
    </ul> </li> 
   <li>case class TaskVals -&gt; used to collect and store "flatten" TaskMetric info collected by the Listener. Metrics are collected per task and include:executor run time, CPU time, scheduler delay, shuffle read and write time, serialization and deserialization time, HDFS I/O metrics, etc read and write time, serializa and deserialization time, HDFS I/O metrics, etc</li> 
   <li>case class TaskMetrics(sparkSession: SparkSession 
    <ul> 
     <li>Helper class to help in collecting and storing performance metrics. It provides wrapper methods to add the listener to the Spark Context (ListenerBus) and other other methods for analysis. When you instantiate this class you start collecting task-level metrics data.</li> 
     <li>def begin() and def end() methods -&gt; use them at mark beginning and end of data collection if you plan to use printReport()</li> 
     <li>def printReport(): Unit -&gt; prints a report of the metrics in "TaskStageMetrics" between the timestamps: beginSnapshot and endSnapshot</li> 
     <li>def createTaskMetricsDF(nameTempView: String = "PerfTaskMetrics"): DataFrame -&gt; converts the ListBuffer with stage metrics into a DataFrame and creates a temporary view, useful for data analytics</li> 
     <li>def runAndMeasure[T](f: =&gt; T): T -&gt; a handy extension to do 3 actions: runs the Spark workload, measure its metrics and print the report. You can see this as an extension of spark.time() command</li> 
     <li>def saveData(df: DataFrame, fileName: String, fileFormat: String = "json") -&gt; helper method to save metrics data collected in a DataFrame for later analysis/plotting</li> 
     <li>def createAccumulablesDF(nameTempView: String = "AccumulablesTaskMetrics"): DataFrame -&gt; converts the accumulables aggregated at task level in a ListBuffer into a DataFrame and temporary view</li> 
     <li>def printAccumulables(): Unit -&gt; prints the accumulables metrics divided in 2 groups: internal metrics (which are basically the same as TaskMetrics) and the rest (typically metrics generated custom by parts of the SQL execution engine)</li> 
    </ul> </li> 
  </ul> 
  <p><strong>Additional info on Flight Recorder Mode:</strong></p> 
  <p>To use in flight recorder mode add one or both of the following to the spark-submit/spark-shell/pyspark command line:</p> 
  <ul> 
   <li>--conf spark.extraListeners=ch.cern.sparkmeasure.FlightRecorderStageMetrics</li> 
   <li>--conf class FlightRecorderTaskMetrics(conf: SparkConf) extends TaskInfoRecorderListener</li> 
  </ul> 
  <p>The flight recorder mode writes the collected metrics serializaed into a file in the driver's filesystem. Optionally add one or both of the following configuration parameters to determine the path of the output file</p> 
  <ul> 
   <li>--conf spark.executorEnv.stageMetricsFileName"= (default is "/tmp/stageMetrics.serialized")</li> 
   <li>--conf spark.executorEnv.taskMetricsFileName"= (default is "/tmp/taskMetrics.serialized")</li> 
  </ul> 
  <p><strong>Additional info on Utils:</strong></p> 
  <p>The object Utils contains some helper code for the sparkMeasure package</p> 
  <ul> 
   <li>The methods formatDuration and formatBytes are used for printing stage metrics reports</li> 
   <li>The methods readSerializedStageMetrics and readSerializedTaskMetrics are used to read data serialized into files by "flight recorder" mode</li> 
  </ul> 
  <p>Examples:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">taskVals</span> <span class="pl-k">=</span> ch.cern.sparkmeasure.<span class="pl-en">Utils</span>.readSerializedTaskMetrics(<span class="pl-s"><span class="pl-pds">"</span>&lt;file name&gt;<span class="pl-pds">"</span></span>)
<span class="pl-k">val</span> <span class="pl-en">taskMetricsDF</span> <span class="pl-k">=</span> taskVals.toDF

<span class="pl-k">val</span> <span class="pl-en">stageVals</span> <span class="pl-k">=</span> ch.cern.sparkmeasure.<span class="pl-en">Utils</span>.readSerializedStageMetrics(<span class="pl-s"><span class="pl-pds">"</span>&lt;file name&gt;<span class="pl-pds">"</span></span>)
<span class="pl-k">val</span> <span class="pl-en">stageMetricsDF</span> <span class="pl-k">=</span> stageVals.toDF</pre>
  </div> 
  <p><strong>Known issues and TODO list</strong></p> 
  <ul> 
   <li>gatherAccumulables=true for taskMetrics(sparkSession: SparkSession, gatherAccumulables: Boolean) currently only works only on Spark 2.1.x and breaks from Spark 2.2.1. This is a consequence of <a href="https://github.com/apache/spark/pull/17596" target="_blank">SPARK PR 17596</a>. Todo: restore the functionality of measuring task accumulables for Spark 2.2.x.</li> 
   <li>Task/stage failures and other errors are mostly not handled by the code in this version, this puts the effort on the user to validate the output. This needs to be fixed in a future version.</li> 
   <li>Following <a href="https://github.com/apache/spark/pull/18249/files" target="_blank">SPARK PR 18249</a> add support for the newly introduced remoteBytesReadToDisk Task Metric (I believe this is for Spark 2.3, to be checked).</li> 
   <li>Following <a href="https://github.com/apache/spark/pull/18162" target="_blank">SPARK PR 18162</a> TaskMetrics._updatedBlockStatuses is off by default, so maybe can be taken out of the list of metrics collected by sparkMetric</li> 
  </ul> 
 </article>
</div>