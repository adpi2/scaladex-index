<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <p><a href="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/logo.png" target="_blank"><img src="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/logo.png" width="471" height="126" style="max-width:100%;"></a></p> 
  <p>NeuroFlow is a library to design, train and evaluate Artificial Neural Networks. It is written in Scala, matrix and vector operations are performed with <a href="https://github.com/scalanlp/breeze" target="_blank">Breeze</a>.</p> 
  <h1><a href="https://github.com/zenecture/neuroflow#introduction" aria-hidden="true" class="anchor" id="user-content-introduction" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Introduction</h1> 
  <p>There are three modules:</p> 
  <ul> 
   <li>core: the building blocks to create neural network architectures</li> 
   <li>application: plugins, helpers, functionality related to various applications</li> 
   <li>playground: examples with resources</li> 
  </ul> 
  <h1><a href="https://github.com/zenecture/neuroflow#getting-started" aria-hidden="true" class="anchor" id="user-content-getting-started" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Getting Started</h1> 
  <p>To use NeuroFlow within your project, add these dependencies (Scala Version 2.12.x):</p> 
  <div class="highlight highlight-source-scala">
   <pre>libraryDependencies <span class="pl-k">++</span><span class="pl-k">=</span> <span class="pl-en">Seq</span>(
  <span class="pl-s"><span class="pl-pds">"</span>com.zenecture<span class="pl-pds">"</span></span>   <span class="pl-k">%%</span>   <span class="pl-s"><span class="pl-pds">"</span>neuroflow-core<span class="pl-pds">"</span></span>          <span class="pl-k">%</span>   <span class="pl-s"><span class="pl-pds">"</span>1.2.5<span class="pl-pds">"</span></span>,
  <span class="pl-s"><span class="pl-pds">"</span>com.zenecture<span class="pl-pds">"</span></span>   <span class="pl-k">%%</span>   <span class="pl-s"><span class="pl-pds">"</span>neuroflow-application<span class="pl-pds">"</span></span>   <span class="pl-k">%</span>   <span class="pl-s"><span class="pl-pds">"</span>1.2.5<span class="pl-pds">"</span></span>
)

resolvers <span class="pl-k">++</span><span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-s"><span class="pl-pds">"</span>Sonatype Releases<span class="pl-pds">"</span></span> at <span class="pl-s"><span class="pl-pds">"</span>https://oss.sonatype.org/content/repositories/releases/<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>Seeing code examples is a good way to get started. You may have a look at the playground for some inspiration. If you want to use neural nets in your project, you can expect a journey full of fun and experiments.</p> 
  <h1><a href="https://github.com/zenecture/neuroflow#construction-of-a-net" aria-hidden="true" class="anchor" id="user-content-construction-of-a-net" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Construction of a Net</h1> 
  <p><a href="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/arch.png" target="_blank"><img src="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/arch.png" width="443" height="320" style="max-width:100%;"></a></p> 
  <p>Let's construct the fully connected feed-forward net (FFN) depicted above. We have to import everything we need:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">neuroflow.application.plugin.Notation.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">neuroflow.core.Activator.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">neuroflow.core.WeightProvider.Double.FFN.</span><span class="pl-v">randomWeights</span>
<span class="pl-k">import</span> <span class="pl-v">neuroflow.core.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">neuroflow.nets.cpu.DenseNetwork.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">shapeless.</span><span class="pl-v">_</span></pre>
  </div> 
  <p>This gives a fully connected <code>DenseNetwork</code>, which is initialized with random weights by <code>WeightProvider</code>. We import all <code>Activator</code> functions so we can place a <code>Sigmoid</code> on the layers:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> (g, h) <span class="pl-k">=</span> (<span class="pl-en">Sigmoid</span>, <span class="pl-en">Sigmoid</span>)
<span class="pl-k">val</span> <span class="pl-en">net</span> <span class="pl-k">=</span> <span class="pl-en">Network</span>(<span class="pl-en">Input</span>(<span class="pl-c1">2</span>) <span class="pl-k">::</span> <span class="pl-en">Dense</span>(<span class="pl-c1">3</span>, g) <span class="pl-k">::</span> <span class="pl-en">Output</span>(<span class="pl-c1">1</span>, h) <span class="pl-k">::</span> <span class="pl-en">HNil</span>)</pre>
  </div> 
  <p>In NeuroFlow, network architectures are expressed as <a href="https://github.com/milessabin/shapeless" target="_blank">HLists</a>. They give type-safety and a humble ability to compose groups of layers. For instance, a little deeper net, with some rates and rules defined through a <code>Settings</code> instance, could look like this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> (e, f) <span class="pl-k">=</span> (<span class="pl-en">Linear</span>, <span class="pl-en">Sigmoid</span>)

<span class="pl-k">val</span> <span class="pl-en">bottleNeck</span> <span class="pl-k">=</span>
  <span class="pl-en">Input</span>  (<span class="pl-c1">50</span>)               <span class="pl-k">::</span>
  <span class="pl-en">Focus</span>  (<span class="pl-en">Dense</span>(<span class="pl-c1">10</span>, e))     <span class="pl-k">::</span> <span class="pl-en">HNil</span>

<span class="pl-k">val</span> <span class="pl-en">fullies</span>    <span class="pl-k">=</span>
  <span class="pl-en">Dense</span>  (<span class="pl-c1">20</span>,  f)           <span class="pl-k">::</span>
  <span class="pl-en">Dense</span>  (<span class="pl-c1">30</span>,  f)           <span class="pl-k">::</span>
  <span class="pl-en">Dense</span>  (<span class="pl-c1">40</span>,  f)           <span class="pl-k">::</span>
  <span class="pl-en">Dense</span>  (<span class="pl-c1">420</span>, f)           <span class="pl-k">::</span>
  <span class="pl-en">Dense</span>  (<span class="pl-c1">40</span>,  f)           <span class="pl-k">::</span>
  <span class="pl-en">Dense</span>  (<span class="pl-c1">30</span>,  f)           <span class="pl-k">::</span> 
  <span class="pl-en">Output</span> (<span class="pl-c1">20</span>,  f)           <span class="pl-k">::</span> <span class="pl-en">HNil</span>

<span class="pl-k">val</span> <span class="pl-en">deeperNet</span> <span class="pl-k">=</span> <span class="pl-en">Network</span>(
  bottleNeck <span class="pl-k">:::</span> fullies, 
  <span class="pl-en">Settings</span>[<span class="pl-k">Double</span>](
    lossFunction <span class="pl-k">=</span> <span class="pl-en">SquaredMeanError</span>(), precision <span class="pl-k">=</span> <span class="pl-c1">1E-5</span>, iterations <span class="pl-k">=</span> <span class="pl-c1">250</span>, 
    learningRate { <span class="pl-k">case</span> (iter, _) <span class="pl-k">if</span> iter <span class="pl-k">&lt;</span> <span class="pl-c1">100</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1E-4</span> <span class="pl-k">case</span> (_, _) <span class="pl-k">=&gt;</span> <span class="pl-c1">1E-5</span> },
    regularization <span class="pl-k">=</span> <span class="pl-en">Some</span>(<span class="pl-en">KeepBest</span>), batchSize <span class="pl-k">=</span> <span class="pl-en">Some</span>(<span class="pl-c1">8</span>), parallelism <span class="pl-k">=</span> <span class="pl-en">Some</span>(<span class="pl-c1">8</span>)
  )
)</pre>
  </div> 
  <p>The learning rate is a partial function from iteration and old learning rate to new learning rate for gradient descent. The <code>batchSize</code> defines how many samples are presented per weight update and <code>parallelism</code> sets the thread pool size, since a batch can be processed in parallel. Another important aspect is the numerical type of the net, which is set by explicitly annotating the type <code>Double</code> on the settings instance. For instance, on the GPU, you might want to work with <code>Float</code> (single) instead of <code>Double</code> precision. Have a look at the <code>Settings</code> class for the full list of options.</p> 
  <p>Be aware that a network must start with one <code>In</code>-typed layer and end with one <code>Out</code>-typed layer. If a network doesn't follow this rule, it won't compile.</p> 
  <h1><a href="https://github.com/zenecture/neuroflow#training" aria-hidden="true" class="anchor" id="user-content-training" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training</h1> 
  <p>We want to map from a two-dimensional vector <code>x</code> to a one-dimensional vector <code>y</code> with our architecture. There are many functions out there of this kind; here we use the XOR-Function. It is linearily not separable, so we can check whether our net can capture this non-linearity.</p> 
  <p>Let's train our <code>net</code> with the <code>train</code> method. In NeuroFlow, you work with Breeze vectors and matrices (<code>DenseMatrix[V]</code>, <code>DenseVector[V]</code>). To define the training data we use the built-in vector notation:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">xs</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">0.0</span>), <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>), <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">0.0</span>), <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>))
<span class="pl-k">val</span> <span class="pl-en">ys</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">0.0</span>), <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">1.0</span>), <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">1.0</span>), <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">0.0</span>))

<span class="pl-c"><span class="pl-c">/*</span></span>
<span class="pl-c">  It's the XOR-Function :-).</span>
<span class="pl-c">  Or: the net learns to add binary digits modulo 2.</span>
<span class="pl-c"><span class="pl-c">*/</span></span>

net.train(xs, ys)</pre>
  </div> 
  <p>For our XOR-feed-forward net, the loss function is defined as follows:</p> 
  <pre><code>E(W) = Σ1/2(t - net(x))²
</code></pre> 
  <p>Where <code>W</code> are the weights, <code>t</code> is the target and <code>net(x)</code> the prediction. The sum <code>Σ</code> is taken over all samples and the square <code>²</code> gives a convex functional form, which is convenient for gradient descent.</p> 
  <p>The training progress will appear on console so we can track it. If you want to visualize the loss function, you can pipe the values to a <code>file</code> like this:</p> 
  <div class="highlight highlight-source-scala">
   <pre>  <span class="pl-en">Settings</span>(
    lossFuncOutput <span class="pl-k">=</span> <span class="pl-en">Some</span>(
      <span class="pl-en">LossFuncOutput</span>(
        file <span class="pl-k">=</span> <span class="pl-en">Some</span>(<span class="pl-s"><span class="pl-pds">"</span>~/NF/lossFunc.txt<span class="pl-pds">"</span></span>), 
        action <span class="pl-k">=</span> <span class="pl-en">Some</span>(loss <span class="pl-k">=&gt;</span> sendToDashboard(loss))
      )
    )
  )</pre>
  </div> 
  <p>This way we can use beloved gnuplot to plot the loss during training:</p> 
  <div class="highlight highlight-source-shell">
   <pre>gnuplot<span class="pl-k">&gt;</span> <span class="pl-c1">set</span> style line 1 lc rgb <span class="pl-s"><span class="pl-pds">'</span>#0060ad<span class="pl-pds">'</span></span> lt 1 lw 1 pt 7 ps 0.5 
gnuplot<span class="pl-k">&gt;</span> plot <span class="pl-s"><span class="pl-pds">'</span>~/NF/lossFunc.txt<span class="pl-pds">'</span></span> with linespoints ls 1</pre>
  </div> 
  <p><a href="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/errgraph3.png" target="_blank"><img src="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/errgraph3.png" width="448" height="321" style="max-width:100%;"></a></p> 
  <p>If you want to be more flexible, e.g. piping the loss over the wire to a real-time dashboard, you can provide a function closure <code>action</code> of type <code>Double =&gt; Unit</code> which gets executed in the background after each training epoch, using the respective loss as input.</p> 
  <p>After work is done, the trained net can be evaluated like a regular function:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">x</span> <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-k">&gt;</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>)
<span class="pl-k">val</span> <span class="pl-en">result</span> <span class="pl-k">=</span> net(x)
<span class="pl-c"><span class="pl-c">//</span> result: Vector(0.980237270455592)</span></pre>
  </div> 
  <p>The resulting vector has dimension = 1, as specified for the XOR-example.</p> 
  <h1><a href="https://github.com/zenecture/neuroflow#distributed-training" aria-hidden="true" class="anchor" id="user-content-distributed-training" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Distributed Training</h1> 
  <p>Let's consider this fully connected FFN:</p> 
  <pre><code>Layout: [1200 In, 210 Dense (R), 210 Dense (R), 210 Dense (R), 1200 Out (R)]
Number of Weights: 592.200 (≈ 4,51813 MB)
</code></pre> 
  <p>On the JVM, a <code>Double</code> takes 8 bytes, meaning the derivative of this network requires roughly 4,5 MB per sample. Training with, let's say, 1 million samples would require ≈ 4,5 TB of RAM for vanilla gradient descent. Luckily, the loss function <code>Σ1/2(t - net(x))²</code> is parallelizable with respect to the sum operator. So, if a single machine offering this amount of memory is not available, we can spread the load across several machines instead of batching it.</p> 
  <p><a href="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/distributedtraining.png" target="_blank"><img src="https://raw.githubusercontent.com/zenecture/zenecture-docs/master/neuroflow/distributedtraining.png" width="800" height="555" style="max-width:100%;"></a></p> 
  <p>Distributed gradient descent broadcasts the respective weight updates between the training epochs to all nodes. In our example, the overhead is 2*4,5=9 MB network traffic per node and iteration, while gaining computational parallelism.</p> 
  <p><em>However, note that on-line or mini-batch training can have much faster convergence (depending on the level of redundancy within the data) than a full distributed batch, even when using several machines.</em></p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">neuroflow.nets.distributed.DenseNetwork.</span><span class="pl-v">_</span>

<span class="pl-k">object</span> <span class="pl-en">Coordinator</span> <span class="pl-k">extends</span> <span class="pl-e">App</span> {

  <span class="pl-k">val</span> <span class="pl-en">nodes</span> <span class="pl-k">=</span> <span class="pl-en">Set</span>(<span class="pl-en">Node</span>(<span class="pl-s"><span class="pl-pds">"</span>localhost<span class="pl-pds">"</span></span>, <span class="pl-c1">2553</span>) <span class="pl-c"><span class="pl-c">/*</span> ... <span class="pl-c">*/</span></span>)

  <span class="pl-k">def</span> <span class="pl-en">coordinator</span> <span class="pl-k">=</span> {
    <span class="pl-k">val</span> <span class="pl-en">f</span> <span class="pl-k">=</span> <span class="pl-en">ReLU</span>
    <span class="pl-k">val</span> <span class="pl-en">net</span> <span class="pl-k">=</span>
      <span class="pl-en">Network</span>(
        <span class="pl-en">Input</span> (<span class="pl-c1">1200</span>) <span class="pl-k">::</span> <span class="pl-en">Dense</span>(<span class="pl-c1">210</span>, f) <span class="pl-k">::</span> <span class="pl-en">Dense</span>(<span class="pl-c1">210</span>, f) <span class="pl-k">::</span> <span class="pl-en">Dense</span>(<span class="pl-c1">210</span>, f) <span class="pl-k">::</span> <span class="pl-en">Output</span>(<span class="pl-c1">1200</span>, f) <span class="pl-k">::</span> <span class="pl-en">HNil</span>,
        <span class="pl-en">Settings</span>[<span class="pl-k">Double</span>](
          coordinator  <span class="pl-k">=</span> <span class="pl-en">Node</span>(<span class="pl-s"><span class="pl-pds">"</span>localhost<span class="pl-pds">"</span></span>, <span class="pl-c1">2552</span>),
          transport    <span class="pl-k">=</span> <span class="pl-en">Transport</span>(messageGroupSize <span class="pl-k">=</span> <span class="pl-c1">100000</span>, frameSize <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>128 MiB<span class="pl-pds">"</span></span>)
        )
      )
    net.train(nodes)
  }

}</pre>
  </div> 
  <p>The network is defined in the <code>Coordinator</code>. The <code>train</code> method will trigger training for all <code>nodes</code>.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">neuroflow.nets.distributed.</span><span class="pl-v">DenseExecutor</span>

<span class="pl-k">object</span> <span class="pl-en">Executor</span> <span class="pl-k">extends</span> <span class="pl-e">App</span> {

  <span class="pl-k">val</span> (xs, ys) <span class="pl-k">=</span>  (<span class="pl-k">???</span>, <span class="pl-k">???</span>) <span class="pl-c"><span class="pl-c">//</span> Local Training Data</span>
  <span class="pl-en">DenseExecutor</span>(<span class="pl-en">Node</span>(<span class="pl-s"><span class="pl-pds">"</span>localhost<span class="pl-pds">"</span></span>, <span class="pl-c1">2553</span>), xs, ys)

}</pre>
  </div> 
  <p>The <code>Executor</code>, a single node, loads the local data source, boots the networking subsystem and listens for incoming jobs.</p> 
  <h1><a href="https://github.com/zenecture/neuroflow#io" aria-hidden="true" class="anchor" id="user-content-io" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>IO</h1> 
  <p>Using <code>neuroflow.application.plugin.IO</code> we can store the weights represented as JSON strings. Look at this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">file</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>/path/to/net.nf<span class="pl-pds">"</span></span>
<span class="pl-k">implicit</span> <span class="pl-k">val</span> <span class="pl-en">weightProvider</span> <span class="pl-k">=</span> <span class="pl-en">IO</span>.<span class="pl-en">File</span>.readDouble(file)
<span class="pl-k">val</span> <span class="pl-en">net</span> <span class="pl-k">=</span> <span class="pl-en">Network</span>(layers)
<span class="pl-c"><span class="pl-c">//</span> ... do work.</span>
<span class="pl-en">IO</span>.<span class="pl-en">File</span>.write(net.weights, file)</pre>
  </div> 
  <p>Here, <code>IO.File.read</code> will yield an implicit <code>WeightProvider</code> from file to construct a net. The weights will be saved to the same file with <code>IO.File.write</code>. If the desired target is a database, simply use <code>IO.Json.write</code> instead and save it as a raw JSON string. However, all important types extend <code>Serializable</code>, so feel free to work with the bytes on your own.</p> 
 </article>
</div>