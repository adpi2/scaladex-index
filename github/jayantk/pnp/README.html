<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/jayantk/pnp#probabilistic-neural-programs" aria-hidden="true" class="anchor" id="user-content-probabilistic-neural-programs" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Probabilistic Neural Programs</h1> 
  <p>Probabilistic Neural Programming (PNP) is a Scala library for expressing, training and running inference in neural network models that <strong>include discrete choices</strong>. The enhanced expressivity of PNP is useful for structured prediction, reinforcement learning, and latent variable models.</p> 
  <p>Probabilistic neural programs have several advantages over computation graph libraries for neural networks, such as TensorFlow:</p> 
  <ul> 
   <li><strong>Probabilistic inference</strong> is implemented within the library. For example, running a beam search to (approximately) generate the highest-scoring output sequence of a sequence-to-sequence model takes 1 line of code in PNP.</li> 
   <li><strong>Additional training algorithms</strong> that require running inference during training are part of the library. This includes learning-to-search algorithms, such as LaSO, reinforcement learning, and training latent variable models.</li> 
   <li><strong>Computation graphs</strong> are a subset of probabilistic neural programs. We use <a href="https://github.com/clab/dynet" target="_blank">DyNet</a> to express neural networks, which provides a rich set of operations and efficient training.</li> 
  </ul> 
  <h2><a href="https://github.com/jayantk/pnp#installation" aria-hidden="true" class="anchor" id="user-content-installation" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Installation</h2> 
  <p>This library depends on DyNet with the <a href="https://github.com/allenai/dynet/tree/master/swig" target="_blank">Scala DyNet bindings</a>. See the link for build instructions. After building this library, run the following commands from the <code>pnp</code> root directory:</p> 
  <pre><code>cd lib
ln -s &lt;PATH_TO_DYNET&gt;/build/contrib/swig/dynet_swigJNI_scala.jar .
ln -s &lt;PATH_TO_DYNET&gt;/build/contrib/swig/dynet_swigJNI_dylib.jar .
</code></pre> 
  <p>That's it! Verify that your installation works by running <code>sbt test</code> in the root directory.</p> 
  <h2><a href="https://github.com/jayantk/pnp#usage" aria-hidden="true" class="anchor" id="user-content-usage" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage</h2> 
  <p>This section describes how to use probabilistic neural programs to define and train a model. The typical usage has three steps:</p> 
  <ol> 
   <li><strong>Define a model.</strong> Models are implemented by writing a function that takes your problem input and outputs <code>Pnp[X]</code> objects. The probabilistic neural program type <code>Pnp[X]</code> represents a function from neural network parameters to probability distributions over values of type <code>X</code>. Each program describes a (possibly infinite) space of executions, each of which returns a value of type <code>X</code>.</li> 
   <li><strong>Train.</strong> Training is performed by passing a list of examples to a <code>Trainer</code>, where each example consists of a <code>Pnp[X]</code> object and a label. Labels are implemented as functions that assign costs to program executions or as conditional distributions over correct executions. Many training algorithms can be used, from loglikelihood to learning-to-search algorithms.</li> 
   <li><strong>Run the model.</strong> A model can be run on a new input by constructing the appropriate <code>Pnp[X]</code> object, then running inference on this object with trained parameters.</li> 
  </ol> 
  <p>These steps are illustrated in detail for a sequence-to-sequence model in <a href="https://github.com/jayantk/pnp/blob/master/src/main/scala/org/allenai/pnp/examples/Seq2Seq.scala" target="_blank">Seq2Seq2.scala</a>. For a more complex example, run the <a href="https://github.com/jayantk/pnp/blob/master/experiments/geoquery/scripts/example.sh" target="_blank">GeoQuery semantic parsing experiment</a>.</p> 
  <h2><a href="https://github.com/jayantk/pnp#defining-probabilistic-neural-programs" aria-hidden="true" class="anchor" id="user-content-defining-probabilistic-neural-programs" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Defining Probabilistic Neural Programs</h2> 
  <p>Probabilistic neural programs are specified by writing the forward computation of a neural network, using the <code>choose</code> operation to represent discrete choices. Roughly, we can write:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">pnp</span> <span class="pl-k">=</span> <span class="pl-k">for</span> {
  scores1 <span class="pl-k">&lt;</span><span class="pl-k">-</span> ... some neural net operations ...
  <span class="pl-c"><span class="pl-c">//</span> Make a discrete choice</span>
  x1 <span class="pl-k">&lt;</span><span class="pl-k">-</span> choose(values, scores1)
  scores2 <span class="pl-k">&lt;</span><span class="pl-k">-</span> ... more neural net operations, may depend on x1 ...
  ...
  xn <span class="pl-k">&lt;</span><span class="pl-k">-</span> choose(values, scoresn)
} <span class="pl-k">yield</span> {
  xn
}</pre>
  </div> 
  <p><code>pnp</code> then represents a function that takes some neural network parameters and returns a distribution over possible values of <code>xn</code> (which in turn depends on the values of intermediate choices). We evaluate <code>pnp</code> by running inference, which simultaneously runs the forward pass of the network and performs probabilistic inference:</p> 
  <div class="highlight highlight-source-scala">
   <pre>nnParams <span class="pl-k">=</span> ... 
<span class="pl-k">val</span> <span class="pl-en">dist</span> <span class="pl-k">=</span> pnp.beamSearch(<span class="pl-c1">10</span>, nnParams)</pre>
  </div> 
  <h3><a href="https://github.com/jayantk/pnp#choose" aria-hidden="true" class="anchor" id="user-content-choose" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Choose</h3> 
  <p>The <code>choose</code> operator defines a distribution over a list of values:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">flip</span><span class="pl-k">:</span> <span class="pl-en">Pnp</span>[<span class="pl-k">Boolean</span>] <span class="pl-k">=</span> choose(<span class="pl-en">Array</span>(<span class="pl-c1">true</span>, <span class="pl-c1">false</span>), <span class="pl-en">Array</span>(<span class="pl-c1">0.5</span>, <span class="pl-c1">0.5</span>))</pre>
  </div> 
  <p>This snippet creates a probability distribution that returns either true or false with 50% probability. <code>flip</code> has type <code>Pnp[Boolean]</code>, which represents a function from neural network parameters to probability distributions over values of type <code>Boolean</code>. (In this case it's just a probability distribution since we haven't referenced any parameters.) Note that <code>flip</code> is not a draw from the distribution, rather, <em>it is the distribution itself</em>. The probability of each choice can be given to <code>choose</code> either in an explicit list (as above) or via an <code>Expression</code> of a neural network.</p> 
  <p>We compose distributions using <code>for {...} yield {...}</code>:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">twoFlips</span><span class="pl-k">:</span> <span class="pl-en">Pnp</span>[<span class="pl-k">Boolean</span>] <span class="pl-k">=</span> <span class="pl-k">for</span> {
  x <span class="pl-k">&lt;</span><span class="pl-k">-</span> flip
  y <span class="pl-k">&lt;</span><span class="pl-k">-</span> flip
} <span class="pl-k">yield</span> {
  x <span class="pl-k">&amp;&amp;</span> y
}</pre>
  </div> 
  <p>This program returns <code>true</code> if two independent draws from <code>flip</code> both return <code>true</code>. The notation <code>x &lt;- flip</code> can be thought of as drawing a value from <code>flip</code> and assigning it to <code>x</code>. However, we can only use the value within the for/yield block to construct another probability distribution. We can now run inference on this object:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">marginals3</span> <span class="pl-k">=</span> twoFlips.beamSearch(<span class="pl-c1">5</span>)
println(marginals3.marginals().getProbabilityMap)</pre>
  </div> 
  <p>This prints out the expected probabilities:</p> 
  <pre><code>{false=0.75, true=0.25}
</code></pre> 
  <h3><a href="https://github.com/jayantk/pnp#neural-networks" aria-hidden="true" class="anchor" id="user-content-neural-networks" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Neural Networks</h3> 
  <p>Probabilistic neural programs have access to an underlying computation graph that is used to define neural networks:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">def</span> <span class="pl-en">mlp</span>(<span class="pl-v">x</span>: <span class="pl-en">FloatVector</span>)<span class="pl-k">:</span> <span class="pl-en">Pnp</span>[<span class="pl-k">Boolean</span>] <span class="pl-k">=</span> {
  <span class="pl-k">for</span> {
    <span class="pl-c"><span class="pl-c">//</span> Get the computation graph</span>
    cg <span class="pl-k">&lt;</span><span class="pl-k">-</span> computationGraph()

    <span class="pl-c"><span class="pl-c">//</span> Get the parameters of a multilayer perceptron by name.</span>
    <span class="pl-c"><span class="pl-c">//</span> The dimensionalities and values of these parameters are </span>
    <span class="pl-c"><span class="pl-c">//</span> defined in a PnpModel that is passed to inference.</span>
    weights1 <span class="pl-k">&lt;</span><span class="pl-k">-</span> param(<span class="pl-s"><span class="pl-pds">"</span>layer1Weights<span class="pl-pds">"</span></span>)
    bias1 <span class="pl-k">&lt;</span><span class="pl-k">-</span> param(<span class="pl-s"><span class="pl-pds">"</span>layer1Bias<span class="pl-pds">"</span></span>)
    weights2 <span class="pl-k">&lt;</span><span class="pl-k">-</span> param(<span class="pl-s"><span class="pl-pds">"</span>layer2Weights<span class="pl-pds">"</span></span>)

    <span class="pl-c"><span class="pl-c">//</span> Input the feature vector to the computation graph and</span>
    <span class="pl-c"><span class="pl-c">//</span> run the multilayer perceptron to produce scores.</span>
    inputExpression <span class="pl-k">=</span> input(cg.cg, <span class="pl-en">Seq</span>(<span class="pl-en">FEATURE_VECTOR_DIM</span>), x)
    scores <span class="pl-k">=</span> weights2 <span class="pl-k">*</span> tanh((weights1 <span class="pl-k">*</span> inputExpression) <span class="pl-k">+</span> bias1)

     <span class="pl-c"><span class="pl-c">//</span> Choose a label given the scores. Scores is expected to</span>
     <span class="pl-c"><span class="pl-c">//</span> be a 2-element vector, where the first element is the score</span>
     <span class="pl-c"><span class="pl-c">//</span> of true, etc.</span>
     y <span class="pl-k">&lt;</span><span class="pl-k">-</span> choose(<span class="pl-en">Array</span>(<span class="pl-c1">true</span>, <span class="pl-c1">false</span>), scores)
  } <span class="pl-k">yield</span> {
    y
  }
}</pre>
  </div> 
  <p>We can then evaluate the network on an example:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">model</span> <span class="pl-k">=</span> <span class="pl-en">PnpModel</span>.init(<span class="pl-c1">true</span>)
<span class="pl-c"><span class="pl-c">//</span> Initialize the network parameters. The values are</span>
<span class="pl-c"><span class="pl-c">//</span> chosen randomly.</span>
model.addParameter(<span class="pl-s"><span class="pl-pds">"</span>layer1Weights<span class="pl-pds">"</span></span>, <span class="pl-en">Seq</span>(<span class="pl-en">HIDDEN_DIM</span>, <span class="pl-en">FEATURE_VECTOR_DIM</span>))
model.addParameter(<span class="pl-s"><span class="pl-pds">"</span>layer1Bias<span class="pl-pds">"</span></span>, <span class="pl-en">Seq</span>(<span class="pl-en">HIDDEN_DIM</span>))
model.addParameter(<span class="pl-s"><span class="pl-pds">"</span>layer2Weights<span class="pl-pds">"</span></span>, <span class="pl-en">Seq</span>(<span class="pl-c1">2</span>, <span class="pl-en">HIDDEN_DIM</span>))

<span class="pl-c"><span class="pl-c">//</span> Run the multilayer perceptron on featureVector</span>
<span class="pl-k">val</span> <span class="pl-en">featureVector</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">FloatVector</span>(<span class="pl-en">Seq</span>(<span class="pl-c1">1.0f</span>, <span class="pl-c1">2.0f</span>, <span class="pl-c1">3.0f</span>))
<span class="pl-k">val</span> <span class="pl-en">dist</span> <span class="pl-k">=</span> mlp(featureVector)
<span class="pl-k">val</span> <span class="pl-en">marginals</span> <span class="pl-k">=</span> dist.beamSearch(<span class="pl-c1">2</span>, model)
 
<span class="pl-k">for</span> (x <span class="pl-k">&lt;</span><span class="pl-k">-</span> marginals.executions) {
  println(x)
}</pre>
  </div> 
  <p>This prints something like:</p> 
  <pre><code>[Execution true -0.4261836111545563]
[Execution false -1.058420181274414]
</code></pre> 
  <p>Each execution has a single value that is an output of our program and a score derived from the neural network computation. In this case, the scores are log probabilities, but the scores may have different semantics depending on the way the model is defined and its parameters are trained.</p> 
  <p>Pnp uses Dynet as the underlying neural network library, which provides a rich set of operations (e.g., LSTMs). See the <a href="http://dynet.readthedocs.io/en/latest/operations.html" target="_blank">Dynet documentation</a> for details, along with the documentation for <a href="https://github.com/allenai/dynet/tree/master/swig" target="_blank">Dynet Scala bindings</a>.</p> 
  <p>TODO: document usage of RNNBuilders, which have to be used statelessly.</p> 
  <h3><a href="https://github.com/jayantk/pnp#defining-richer-models" aria-hidden="true" class="anchor" id="user-content-defining-richer-models" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Defining Richer Models</h3> 
  <p>Probabilistic neural programs can be easily composed to construct richer models using <code>for {...} yield {...}</code>. For example, we can define a CRF sequence tagger using the multilayer perceptron above:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">def</span> <span class="pl-en">sequenceTag</span>(<span class="pl-v">xs</span>: <span class="pl-en">Seq</span>[<span class="pl-en">FloatVector</span>])<span class="pl-k">:</span> <span class="pl-en">Pnp</span>[<span class="pl-en">List</span>[<span class="pl-k">Boolean</span>]] <span class="pl-k">=</span> {
  xs.foldLeft(<span class="pl-en">Pnp</span>.value(<span class="pl-en">List</span>[<span class="pl-k">Boolean</span>]()))((x, y) <span class="pl-k">=&gt;</span> <span class="pl-k">for</span> {
    cur <span class="pl-k">&lt;</span><span class="pl-k">-</span> mlp(y)
    rest <span class="pl-k">&lt;</span><span class="pl-k">-</span> x

    cg <span class="pl-k">&lt;</span><span class="pl-k">-</span> computationGraph()
    _ <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-k">if</span> (rest.length <span class="pl-k">&gt;</span> <span class="pl-c1">0</span>) {
      <span class="pl-c"><span class="pl-c">//</span> Add a factor to the model that scores adjacent labels</span>
      <span class="pl-c"><span class="pl-c">//</span> in the sequence. Here, labelNn runs a neural network</span>
      <span class="pl-c"><span class="pl-c">//</span> whose inputs are cur and the next label, and whose output</span>
      <span class="pl-c"><span class="pl-c">//</span> is a 1-element vector containing the score.</span>
      score(labelNn(cur, rest.head, cg.cg))
    } <span class="pl-k">else</span> {
      value(())
    }
  } <span class="pl-k">yield</span> {
    cur <span class="pl-k">::</span> rest
  })
}</pre>
  </div> 
  <p>We can now run this model on a sequence of feature vectors in the same way as the multilayer perceptron:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> Same model as before, but make it globally normalized </span>
<span class="pl-c"><span class="pl-c">//</span> and add some more parameters for labelNn</span>
model.locallyNormalized <span class="pl-k">=</span> <span class="pl-c1">false</span>
model.addLookupParameter(<span class="pl-s"><span class="pl-pds">"</span>left<span class="pl-pds">"</span></span>, <span class="pl-c1">2</span>, <span class="pl-en">Seq</span>(<span class="pl-en">LABEL_DIM</span>))
model.addLookupParameter(<span class="pl-s"><span class="pl-pds">"</span>right<span class="pl-pds">"</span></span>, <span class="pl-c1">2</span>, <span class="pl-en">Seq</span>(<span class="pl-en">LABEL_DIM</span>))

<span class="pl-k">val</span> <span class="pl-en">featureVectors</span> <span class="pl-k">=</span> <span class="pl-en">Seq</span>(<span class="pl-k">new</span> <span class="pl-en">FloatVector</span>(...), <span class="pl-k">new</span> <span class="pl-en">FloatVector</span>(...), <span class="pl-k">new</span> <span class="pl-en">FloatVector</span>(...))
<span class="pl-k">val</span> <span class="pl-en">dist</span> <span class="pl-k">=</span> sequenceTag(featureVectors)
<span class="pl-k">val</span> <span class="pl-en">marginals</span> <span class="pl-k">=</span> dist.beamSearch(<span class="pl-c1">5</span>, model)
<span class="pl-k">for</span> (x <span class="pl-k">&lt;</span><span class="pl-k">-</span> marginals.executions) {
  println(x)
}</pre>
  </div> 
  <p>This prints something like:</p> 
  <pre><code>[Execution List(true, true, true) 5.28779661655426]
[Execution List(false, true, true) 1.7529568672180176]
[Execution List(true, true, false) 1.4970757961273193]
[Execution List(true, false, false) -0.007531404495239258]
[Execution List(true, false, true) -0.42748916149139404]
</code></pre> 
  <h2><a href="https://github.com/jayantk/pnp#training" aria-hidden="true" class="anchor" id="user-content-training" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Training</h2> 
  <p>TODO</p> 
  <h2><a href="https://github.com/jayantk/pnp#inference" aria-hidden="true" class="anchor" id="user-content-inference" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Inference</h2> 
  <p>TODO</p> 
 </article>
</div>