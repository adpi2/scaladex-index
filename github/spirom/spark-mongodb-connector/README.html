<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-nsmc-a-native-mongodb-connector-for-apache-spark" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#nsmc-a-native-mongodb-connector-for-apache-spark" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>NSMC: A Native MongoDB Connector for Apache Spark</h1> 
  <p>This is a native connector for reading and writing MongoDB collections directly from Apache Spark. In Spark, the data from MongoDB is represented as an RDD[MongoDBObject]. Starting with Release 0.5.0, you can also use NSMC through Spark SQL by registering a MongpDB collection as a temporary table.</p> 
  <p>Fore more details about this project, check the following blog posts:</p> 
  <p><a href="http://www.river-of-bytes.com/2015/01/nsmc-native-mongodb-connector-for.html" target="_blank">NSMC: A Native MongoDB Connector for Apache Spark</a></p> 
  <p><a href="http://www.river-of-bytes.com/2015/02/spark-sql-integration-for-mongodb.html" target="_blank">Spark SQL Integration for MongoDB</a></p> 
  <h1><a id="user-content-current-limitations" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#current-limitations" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Current Limitations</h1> 
  <ul> 
   <li>No Java or Python API bindings</li> 
   <li>Can only read (no updates)</li> 
   <li>Only tested with the following configurations:</li> 
   <li>MongoDB: 2.6</li> 
   <li>Scala: 2.10</li> 
   <li>Spark: 1.4.0</li> 
   <li>Casbah 2.7</li> 
   <li>Not tested with MongoDB's hash-based partitioning</li> 
  </ul> 
  <h1><a id="user-content-other-warnings" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#other-warnings" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Other Warnings</h1> 
  <ul> 
   <li>The APIs will probably change several times before an official release</li> 
  </ul> 
  <h1><a id="user-content-release-status" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#release-status" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Release Status</h1> 
  <p>A pre-release alpha version (0.5.3) is available.</p> 
  <h1><a id="user-content-licensing" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#licensing" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Licensing</h1> 
  <p>See the top-level LICENSE file</p> 
  <h1><a id="user-content-getting-started" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#getting-started" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Getting Started</h1> 
  <p>Perhaps the <strong>easiest way to get started</strong> is to use the <a href="https://github.com/spirom/spark-mongodb-examples" target="_blank">companion example project</a>. It contains an appropriately configured <strong>sbt</strong> project, together with sample code for using NSMC from both core Spark and Spark SQL. Be sure to check out that project's README to to make sure you're using the right branch for the NSMC and Apache Spark versions you want to use.</p> 
  <h1><a id="user-content-sbt-configuration" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#sbt-configuration" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a><strong>sbt</strong> configuration</h1> 
  <p>Add the following to your build.sbt for the latest <em>stable</em> release:</p> 
  <pre><code>scalaVersion := "2.10.4" // any 2.10 is OK -- support for 2.11 coming soon

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.4.0" % "provided"

libraryDependencies += "org.apache.spark" %% "spark-sql" % "1.4.0" % "provided"

libraryDependencies += "com.github.spirom" %% "spark-mongodb-connector" % "0.5.3"
</code></pre> 
  <h1><a id="user-content-configuration" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#configuration" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Configuration</h1> 
  <table> 
   <thead> 
    <tr> 
     <th>Setting</th> 
     <th>Meaning</th> 
     <th>Units</th> 
     <th>Default</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>spark.nsmc.connection.host</td> 
     <td>MongoDB host or IP address</td> 
     <td></td> 
     <td>localhost</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.connection.port</td> 
     <td>MongoDB port</td> 
     <td></td> 
     <td>27017</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.user</td> 
     <td>MongoDB user name</td> 
     <td></td> 
     <td>no authentication</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.password</td> 
     <td>MongoDB password</td> 
     <td></td> 
     <td>no authentication</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.split.indexed.collections</td> 
     <td>Should indexed collections be partitioned using MongoDB's [internal] splitVector command?</td> 
     <td>boolean</td> 
     <td>false</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.split.chunk.size</td> 
     <td>Maximum chunk size, in megabytes, passed to MongoDB's splitVector command, if used.</td> 
     <td>MB</td> 
     <td>4</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.partition.on.shard.chunks</td> 
     <td>Should collections that are already sharded in MongoDB retain this as their partitioning in Spark? If not, the entire collection will be read as a single Spark partition.</td> 
     <td>boolean</td> 
     <td>false</td> 
    </tr> 
    <tr> 
     <td>spark.nsmc.direct.to.shards</td> 
     <td>If sharding of collections is being observed, should the mongos server be bypassed? (Don't do this unless you understand MongoDB really well, or you may obtain incorrect results -- if MongoDB is rebalancing the shards when your query executes.)</td> 
     <td>boolean</td> 
     <td>false</td> 
    </tr>
   </tbody>
  </table> 
  <h1><a id="user-content-usage-from-core-spark" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#usage-from-core-spark" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage from core Spark</h1> 
  <p>In your code, you need to add:</p> 
  <pre><code>import nsmc._

import com.mongodb.DBObject
</code></pre> 
  <p>Then to actually read from a collection:</p> 
  <pre><code>val conf = new SparkConf()
    .setAppName("My MongoApp").setMaster("local[4]")
    .set("spark.nsmc.connection.host", "myMongoHost")
    .set("spark.nsmc.connection.port", "myMongoPort")
    .set("spark.nsmc.user", "yourUsernameHere")
    .set("spark.nsmc.password", "yourPasswordHere")
val sc = new SparkContext(conf)
val data: MongoRDD[DBObject] =
    sc.mongoCollection[DBObject]("myDB", "myCollection")
</code></pre> 
  <h1><a id="user-content-usage-from-spark-sql" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#usage-from-spark-sql" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage from Spark SQL</h1> 
  <p>Register a temporary table as follows</p> 
  <pre><code>sqlContext.sql(
    s"""
    |CREATE TEMPORARY TABLE dataTable
    |USING nsmc.sql.MongoRelationProvider
    |OPTIONS (db 'myDB', collection 'myCollection')
  """.stripMargin)
</code></pre> 
  <p>Query the temporary table as follows</p> 
  <pre><code>val data = sqlContext.sql("SELECT * FROM dataTable")
</code></pre> 
  <h1><a id="user-content-summary-of-releases" class="anchor" href="https://github.com/spirom/spark-mongodb-connector#summary-of-releases" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Summary of releases</h1> 
  <table> 
   <thead> 
    <tr> 
     <th>NSMC Release</th> 
     <th>Status</th> 
     <th>Apache Spark Release</th> 
     <th>Scala Version</th> 
     <th>Features Added</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>0.5.3</td> 
     <td>Alpha</td> 
     <td>1.4.0</td> 
     <td>2.10</td> 
     <td>Spark 1.4.0</td> 
    </tr> 
    <tr> 
     <td>0.5.2</td> 
     <td>Alpha</td> 
     <td>1.3.0</td> 
     <td>2.10</td> 
     <td>Configuration parameters are prefixed with "spark." to enable use of various Spark tools like the Thrift server and <strong>spark-submit</strong>.</td> 
    </tr> 
    <tr> 
     <td>0.5.1</td> 
     <td>Alpha</td> 
     <td>1.3.0</td> 
     <td>2.10</td> 
     <td>Apache Spark dependency is marked as "provided", making it easier to create assemblies that include NSMC.</td> 
    </tr> 
    <tr> 
     <td>0.5.0</td> 
     <td>Alpha</td> 
     <td>1.3.0</td> 
     <td>2.10</td> 
     <td>Spark 1.3.0</td> 
    </tr> 
    <tr> 
     <td>0.4.1</td> 
     <td>Alpha</td> 
     <td>1.2.0</td> 
     <td>2.10</td> 
     <td>Filter and Projection push-down for Spark SQL</td> 
    </tr> 
    <tr> 
     <td>0.4.0</td> 
     <td>Alpha</td> 
     <td>1.2.0</td> 
     <td>2.10</td> 
     <td>Spark 1.2.0 and Spark SQL Support</td> 
    </tr> 
    <tr> 
     <td>0.3.0</td> 
     <td>Alpha</td> 
     <td>1.1.0</td> 
     <td>2.10</td> 
     <td>Initial Release</td> 
    </tr>
   </tbody>
  </table> 
 </article>
</div>