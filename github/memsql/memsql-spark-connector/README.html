<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-memsql-spark-20-connector" class="anchor" href="https://github.com/memsql/memsql-spark-connector#memsql-spark-20-connector" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>MemSQL Spark 2.0 Connector</h1> 
  <p>The MemSQL 2.0 Spark connector enables users to load data from MemSQL tables into Spark Dataframes, and write Spark Dataframes to MemSQL tables.</p> 
  <h2><a id="user-content-requirements" class="anchor" href="https://github.com/memsql/memsql-spark-connector#requirements" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Requirements</h2> 
  <p>This library requires Spark 2.0+ and has been primarily tested against Spark version 2.0.2. For support with Spark 1.x, please check the <a href="https://github.com/memsql/memsql-spark-connector/tree/1.3.3" target="_blank">1.x branch</a>.</p> 
  <h2><a id="user-content-documentation" class="anchor" href="https://github.com/memsql/memsql-spark-connector#documentation" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Documentation</h2> 
  <p>You can find Scala documentation for everything exposed in this repo here: <a href="http://memsql.github.io/memsql-spark-connector" target="_blank">memsql.github.io/memsql-spark-connector</a></p> 
  <h2><a id="user-content-installation" class="anchor" href="https://github.com/memsql/memsql-spark-connector#installation" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Installation</h2> 
  <p>Inside a project definition you can depend on the MemSQL Connector using sbt:</p> 
  <pre><code>libraryDependencies  += "com.memsql" %% "memsql-connector" % "2.0.2"
</code></pre> 
  <p>or Maven:</p> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;<span class="pl-ent">dependency</span>&gt;
    &lt;<span class="pl-ent">groupId</span>&gt;com.memsql&lt;/<span class="pl-ent">groupId</span>&gt;
    &lt;<span class="pl-ent">artifactId</span>&gt;memsql-connector_2.11&lt;/<span class="pl-ent">artifactId</span>&gt;
    &lt;<span class="pl-ent">version</span>&gt;2.0.2&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre>
  </div> 
  <h2><a id="user-content-usage" class="anchor" href="https://github.com/memsql/memsql-spark-connector#usage" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage</h2> 
  <p>MemSQL Spark Connector leverages Spark SQL's Data Sources API. The connection to MemSQL relies on the following Spark configuration settings.</p> 
  <table> 
   <thead> 
    <tr> 
     <th>Setting name</th> 
     <th>Default value if not specified</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>spark.memsql.host</td> 
     <td>localhost</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.port</td> 
     <td>3306</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.user</td> 
     <td>root</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.password</td> 
     <td>None</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.defaultDatabase</td> 
     <td>None</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.defaultSaveMode</td> 
     <td>"error" (see description below)</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.disablePartitionPushdown</td> 
     <td>false</td> 
    </tr> 
    <tr> 
     <td>spark.memsql.defaultCreateMode</td> 
     <td>DatabaseAndTable</td> 
    </tr>
   </tbody>
  </table> 
  <p><code>defaultCreateMode</code> specifies whether the connector will create the database and/or table if it doesn't already exist, when saving data to MemSQL. The possible values are <code>DatabaseAndTable</code>, <code>Table</code>, and <code>Skip</code>. The user will need the corresponding create permissions if the value is not <code>Skip</code>.</p> 
  <p>Note that all MemSQL credentials have to be the same on all nodes to take advantage of partition pushdown, which queries leaves directly.</p> 
  <h3><a id="user-content-loading-data-from-memsql" class="anchor" href="https://github.com/memsql/memsql-spark-connector#loading-data-from-memsql" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Loading data from MemSQL</h3> 
  <p>The following example creates a Dataframe from the table "illinois" in the database "customers". To use the library, pass in "com.memsql.spark.connector" as the <code>format</code> parameter so Spark will call the MemSQL Spark Connector code. The option <code>path</code> is the full path of the table using the syntax <code>$database_name</code>.<code>$table_name</code>. If there is only a table name, the connector will look for the table in the default database set in the configuration.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.spark.</span><span class="pl-v">SparkConf</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">conf</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SparkConf</span>()
	.setAppName(<span class="pl-s"><span class="pl-pds">"</span>MemSQL Spark Connector Example<span class="pl-pds">"</span></span>)
	.set(<span class="pl-s"><span class="pl-pds">"</span>spark.memsql.host<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>10.0.0.190<span class="pl-pds">"</span></span>)
	.set(<span class="pl-s"><span class="pl-pds">"</span>spark.memsql.password<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>foobar<span class="pl-pds">"</span></span>)
	.set(<span class="pl-s"><span class="pl-pds">"</span>spark.memsql.defaultDatabase<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>customers<span class="pl-pds">"</span></span>)
<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().config(conf).getOrCreate()

<span class="pl-k">val</span> <span class="pl-en">customersFromIllinois</span> <span class="pl-k">=</span> spark
	.read
	.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>)
	.options(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>path<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-s"><span class="pl-pds">"</span>customers.illinois<span class="pl-pds">"</span></span>)))
	.load()
<span class="pl-c"><span class="pl-c">//</span> customersFromIllinois is now a Spark DataFrame which represents the specified MemSQL table</span>
<span class="pl-c"><span class="pl-c">//</span> and can be queried using Spark DataFrame query functions</span>

<span class="pl-c"><span class="pl-c">//</span> count the number of rows</span>
println(s<span class="pl-s"><span class="pl-pds">"</span>The number of customers from Illinois is ${customersFromIllinois.count()}<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> print out the DataFrame</span>
customersFromIllinois.show()</pre>
  </div> 
  <p>Instead of specifying a MemSQL table as the <code>path</code> in the options, the user can opt to create a DataFrame from a SQL query with the option <code>query</code>. This can minimize the amount of data transferred from MemSQL to Spark, and push down distributed computations to MemSQL instead of Spark. For best performance, either specify the database name using the option <code>database</code>, OR make sure a default database is set in the Spark configuration. Either setting enables the connector to query the MemSQL leaf nodes directly, instead of going through the master aggregator.</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.spark.</span><span class="pl-v">SparkConf</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">conf</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SparkConf</span>()
	.setAppName(<span class="pl-s"><span class="pl-pds">"</span>MemSQL Spark Connector Example<span class="pl-pds">"</span></span>)
	.set(<span class="pl-s"><span class="pl-pds">"</span>spark.memsql.host<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>10.0.0.190<span class="pl-pds">"</span></span>)
	.set(<span class="pl-s"><span class="pl-pds">"</span>spark.memsql.password<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>foobar<span class="pl-pds">"</span></span>)
<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().config(conf).getOrCreate()

<span class="pl-k">val</span> <span class="pl-en">customersFromIllinois</span> <span class="pl-k">=</span> spark
	.read
	.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>)
	.options(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>query<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-s"><span class="pl-pds">"</span>select age_group, count(*) from customers.illinois where number_of_orders &gt; 3 GROUP BY age_group<span class="pl-pds">"</span></span>),
				 <span class="pl-s"><span class="pl-pds">"</span>database<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>customers<span class="pl-pds">"</span></span>))
	.load()

customersFromIllinois.show()
<span class="pl-c"><span class="pl-c">//</span> +-----------+---------+</span>
<span class="pl-c"><span class="pl-c">//</span> | age_group | count(*)|</span>
<span class="pl-c"><span class="pl-c">//</span> +-----------+---------+</span>
<span class="pl-c"><span class="pl-c">//</span> |  13-18    |   128   |</span>
<span class="pl-c"><span class="pl-c">//</span> |  19-25    |   150   |</span>
<span class="pl-c"><span class="pl-c">//</span> |  26+      |   140   |</span>
<span class="pl-c"><span class="pl-c">//</span> +-----------+---------+</span></pre>
  </div> 
  <h3><a id="user-content-saving-data-to-memsql" class="anchor" href="https://github.com/memsql/memsql-spark-connector#saving-data-to-memsql" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Saving data to MemSQL</h3> 
  <p>Similarly, use Spark SQL's Data Sources API to save a DataFrame to MemSQL. To save a DataFrame in the MemSQL table "students":</p> 
  <div class="highlight highlight-source-scala">
   <pre>...

<span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.parallelize(<span class="pl-en">Array</span>(<span class="pl-en">Row</span>(<span class="pl-s"><span class="pl-pds">"</span>John Smith<span class="pl-pds">"</span></span>, <span class="pl-c1">12</span>), <span class="pl-en">Row</span>(<span class="pl-s"><span class="pl-pds">"</span>Jane Doe<span class="pl-pds">"</span></span>, <span class="pl-c1">13</span>)))
<span class="pl-k">val</span> <span class="pl-en">schema</span> <span class="pl-k">=</span> <span class="pl-en">StructType</span>(<span class="pl-en">Seq</span>(<span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Name<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, <span class="pl-c1">false</span>),
                            <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Age<span class="pl-pds">"</span></span>, <span class="pl-en">IntegerType</span>, <span class="pl-c1">false</span>)))
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.createDataFrame(rdd, schema)
df
	.write
	.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>)
	.mode(<span class="pl-s"><span class="pl-pds">"</span>error<span class="pl-pds">"</span></span>)
	.save(<span class="pl-s"><span class="pl-pds">"</span>people.students<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>The <code>mode</code> specifies how to handle duplicate keys when the MemSQL table has a primary key. The default, if unspecified, is "error", which means that if a row with the same primary key already exists in MemSQL's people.students table, an error is to be thrown. Other save modes:</p> 
  <table> 
   <thead> 
    <tr> 
     <th>Save mode string</th> 
     <th>Description</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>"error"</td> 
     <td>MemSQL will error when encountering a record with duplicate keys</td> 
    </tr> 
    <tr> 
     <td>"ignore"</td> 
     <td>MemSQL will ignore records with duplicate keys and, without rolling back, continue inserting records with unique keys.</td> 
    </tr> 
    <tr> 
     <td>"overwrite"</td> 
     <td>MemSQL will replace the existing record with the new record</td> 
    </tr>
   </tbody>
  </table> 
  <p>Other MemSQL write settings can be specified using <code>.option(...)</code> or <code>.options(...)</code>. To perform a dry run of the previous example:</p> 
  <div class="highlight highlight-source-scala">
   <pre>df
	.write
	.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>)
	.mode(<span class="pl-s"><span class="pl-pds">"</span>error<span class="pl-pds">"</span></span>)
	.option(<span class="pl-s"><span class="pl-pds">"</span>dryRun<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>true<span class="pl-pds">"</span></span>)
	.save(<span class="pl-s"><span class="pl-pds">"</span>people.students<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <table> 
   <thead> 
    <tr> 
     <th>Option name</th> 
     <th>Description</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>writeToMaster</td> 
     <td>Force this write to be sent to the master aggregator</td> 
    </tr> 
    <tr> 
     <td>dryRun</td> 
     <td>Don't actually perform the write (this will still create the database and table if they don't exist)</td> 
    </tr> 
    <tr> 
     <td>saveMode</td> 
     <td>See Spark configuration settings</td> 
    </tr> 
    <tr> 
     <td>createMode</td> 
     <td>See Spark configuration settings</td> 
    </tr> 
    <tr> 
     <td>insertBatchSize</td> 
     <td>See Spark configuration settings</td> 
    </tr> 
    <tr> 
     <td>loadDataCompression</td> 
     <td>See Spark configuration settings</td> 
    </tr>
   </tbody>
  </table> 
  <p>The second interface to save data to MemSQL is via the <code>saveToMemSQL</code> implicit function on a DataFrame you wish to save:</p> 
  <div class="highlight highlight-source-scala">
   <pre>...

<span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.parallelize(<span class="pl-en">Array</span>(<span class="pl-en">Row</span>(<span class="pl-s"><span class="pl-pds">"</span>John Smith<span class="pl-pds">"</span></span>, <span class="pl-c1">12</span>), <span class="pl-en">Row</span>(<span class="pl-s"><span class="pl-pds">"</span>Jane Doe<span class="pl-pds">"</span></span>, <span class="pl-c1">13</span>)))
<span class="pl-k">val</span> <span class="pl-en">schema</span> <span class="pl-k">=</span> <span class="pl-en">StructType</span>(<span class="pl-en">Seq</span>(<span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Name<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, <span class="pl-c1">false</span>),
                            <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>Age<span class="pl-pds">"</span></span>, <span class="pl-en">IntegerType</span>, <span class="pl-c1">false</span>)))
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.createDataFrame(rdd, schema)
df.saveToMemSQL(<span class="pl-s"><span class="pl-pds">"</span>people.students<span class="pl-pds">"</span></span>)
      <span class="pl-c"><span class="pl-c">//</span> The database name can be omitted if "spark.memsql.defaultDatabase" is set</span>
      <span class="pl-c"><span class="pl-c">//</span> in the Spark configuration df.sqlContext.sparkContext.getConf.getAll</span></pre>
  </div> 
  <p>A call to <code>saveToMemSQL</code> can take three forms:</p> 
  <div class="highlight highlight-source-scala">
   <pre># <span class="pl-en">Table</span> only
df.saveToMemSQL(<span class="pl-s"><span class="pl-pds">"</span>tbl<span class="pl-pds">"</span></span>)

# <span class="pl-en">Database</span> and table
df.saveToMemSQL(<span class="pl-s"><span class="pl-pds">"</span>db<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>tbl<span class="pl-pds">"</span></span>)

# <span class="pl-en">Database</span>, table, and options
<span class="pl-k">val</span> <span class="pl-en">saveConf</span> <span class="pl-k">=</span> <span class="pl-en">SaveToMemSQLConf</span>(ss.memSQLConf, params<span class="pl-k">=</span><span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>dryRun<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>true<span class="pl-pds">"</span></span>))
df.saveToMemSQL(<span class="pl-en">TableIdentifier</span>(<span class="pl-s"><span class="pl-pds">"</span>db<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>tbl<span class="pl-pds">"</span></span>), saveConf)</pre>
  </div> 
  <p>Any options not specified in <code>saveConf</code> will default to those in the <code>MemSQLConf</code>.</p> 
  <h2><a id="user-content-types" class="anchor" href="https://github.com/memsql/memsql-spark-connector#types" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Types</h2> 
  <p>When saving a Dataframe from Spark to MemSQL, the SparkType of each Dataframe column will be converted to the following MemSQL type:</p> 
  <table> 
   <thead> 
    <tr> 
     <th>SparkType</th> 
     <th>MemSQL Type</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>ShortType</td> 
     <td>SMALLINT</td> 
    </tr> 
    <tr> 
     <td>FloatType</td> 
     <td>FLOAT</td> 
    </tr> 
    <tr> 
     <td>DoubleType</td> 
     <td>DOUBLE</td> 
    </tr> 
    <tr> 
     <td>LongType</td> 
     <td>BIGINT</td> 
    </tr> 
    <tr> 
     <td>IntegerType</td> 
     <td>INT</td> 
    </tr> 
    <tr> 
     <td>BooleanType</td> 
     <td>BOOLEAN</td> 
    </tr> 
    <tr> 
     <td>StringType</td> 
     <td>TEXT</td> 
    </tr> 
    <tr> 
     <td>BinaryType</td> 
     <td>BLOB</td> 
    </tr> 
    <tr> 
     <td>DecimalType</td> 
     <td>DECIMAL</td> 
    </tr> 
    <tr> 
     <td>TimeStampType</td> 
     <td>TIMESTAMP</td> 
    </tr> 
    <tr> 
     <td>DateType</td> 
     <td>DATE</td> 
    </tr>
   </tbody>
  </table> 
  <p>When reading a MemSQL table as a Spark Dataframe, the MemSQL column type will be converted to the following SparkType:</p> 
  <table> 
   <thead> 
    <tr> 
     <th>MemSQL Type</th> 
     <th>SparkType</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>TINYINT, SMALLINT</td> 
     <td>ShortType</td> 
    </tr> 
    <tr> 
     <td>INTEGER</td> 
     <td>IntegerType</td> 
    </tr> 
    <tr> 
     <td>BIGINT (signed)</td> 
     <td>LongType</td> 
    </tr> 
    <tr> 
     <td>DOUBLE, NUMERIC</td> 
     <td>DoubleType</td> 
    </tr> 
    <tr> 
     <td>REAL</td> 
     <td>FloatType</td> 
    </tr> 
    <tr> 
     <td>DECIMAL</td> 
     <td>DecimalType</td> 
    </tr> 
    <tr> 
     <td>TIMESTAMP</td> 
     <td>TimestampType</td> 
    </tr> 
    <tr> 
     <td>DATE</td> 
     <td>DateType</td> 
    </tr> 
    <tr> 
     <td>TIME</td> 
     <td>StringType</td> 
    </tr> 
    <tr> 
     <td>CHAR, VARCHAR</td> 
     <td>StringType</td> 
    </tr> 
    <tr> 
     <td>BIT, BLOG, BINARY</td> 
     <td>BinaryType</td> 
    </tr>
   </tbody>
  </table> 
  <p>MemSQL Spark 2.0 Connector does not support GeoSpatial or JSON MemSQL types since Spark 2.0 has currently disabled user defined types (see <a href="https://issues.apache.org/jira/browse/SPARK-14155" target="_blank">JIRA issue</a>). These types, when read, will become BinaryType.</p> 
  <h2><a id="user-content-changes-from-memsql-spark-1x-connector" class="anchor" href="https://github.com/memsql/memsql-spark-connector#changes-from-memsql-spark-1x-connector" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Changes from MemSQL Spark 1.X Connector</h2> 
  <p>While the MemSQL Spark 1.X Connector relied on Spark SQL experimental developer APIs, the MemSQL Spark 2.0 Connector uses only the official and stable APIs for loading data from an external data source documented <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.sources.package" target="_blank">here</a>. In certain cases, we can "push down" distributed computations to MemSQL. This means that instead of having Spark perform a a transformation (eg. filter, join, etc) on the data it retrieved from MemSQL, you can let MemSQL do the operation on the data and pass the result to Spark. The MemSQL Spark 2.0 Connector supports column and filter pushdown; if you would like to push down joins or aggregates, consider explicitly including it in the user-specified <code>query</code> option. E.g. instead of</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">people</span> <span class="pl-k">=</span> spark.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>).options(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>path<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-s"><span class="pl-pds">"</span>db.people<span class="pl-pds">"</span></span>))).load()
<span class="pl-k">val</span> <span class="pl-en">department</span> <span class="pl-k">=</span> spark.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>).options(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>path<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-s"><span class="pl-pds">"</span>db.department<span class="pl-pds">"</span></span>))).load()
<span class="pl-k">val</span> <span class="pl-en">result</span> <span class="pl-k">=</span> people.join(department, people(<span class="pl-s"><span class="pl-pds">"</span>deptId<span class="pl-pds">"</span></span>) <span class="pl-k">===</span> department(<span class="pl-s"><span class="pl-pds">"</span>id<span class="pl-pds">"</span></span>))</pre>
  </div> 
  <p>Do:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">result</span> <span class="pl-k">=</span> spark
	.read
	.format(<span class="pl-s"><span class="pl-pds">"</span>com.memsql.spark.connector<span class="pl-pds">"</span></span>)
	.options(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>query<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-s"><span class="pl-pds">"</span>select * from people join department on people.deptId = department.id<span class="pl-pds">"</span></span>)))
	.load()</pre>
  </div> 
  <h2><a id="user-content-building-and-testing" class="anchor" href="https://github.com/memsql/memsql-spark-connector#building-and-testing" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Building and Testing</h2> 
  <p>You can use SBT to compile the library</p> 
  <pre><code>sbt compile
</code></pre> 
  <p>All unit tests can be run via sbt. They will also run at build time automatically.</p> 
  <pre><code>sbt test
</code></pre> 
 </article>
</div>