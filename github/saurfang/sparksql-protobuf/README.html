<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/saurfang/sparksql-protobuf#sparksql-protobuf" aria-hidden="true" class="anchor" id="user-content-sparksql-protobuf" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>sparksql-protobuf</h1> 
  <p>This library provides utilities to work with Protobuf objects in SparkSQL. It provides a way to read parquet file written by SparkSQL back as an RDD of compatible protobuf object. It can also converts RDD of protobuf objects into DataFrame.</p> 
  <p><a href="https://travis-ci.org/saurfang/sparksql-protobuf" target="_blank"><img src="https://camo.githubusercontent.com/6d1dc25293acfdf0c3fbe8a328fa613b9b2f0734/68747470733a2f2f7472617669732d63692e6f72672f7361757266616e672f737061726b73716c2d70726f746f6275662e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/saurfang/sparksql-protobuf.svg?branch=master" style="max-width:100%;"></a> <a href="https://codecov.io/github/saurfang/sparksql-protobuf?branch=master" target="_blank"><img src="https://camo.githubusercontent.com/6435e045137d3d00d5abaaadaaa76371a4415677/68747470733a2f2f636f6465636f762e696f2f6769746875622f7361757266616e672f737061726b73716c2d70726f746f6275662f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="https://codecov.io/github/saurfang/sparksql-protobuf/coverage.svg?branch=master" style="max-width:100%;"></a></p> 
  <p>For sbt 0.13.6+</p> 
  <div class="highlight highlight-source-scala">
   <pre>resolvers <span class="pl-k">+</span><span class="pl-k">=</span> <span class="pl-en">Resolver</span>.jcenterRepo

libraryDependencies <span class="pl-k">++</span><span class="pl-k">=</span> <span class="pl-en">Seq</span>(
    <span class="pl-s"><span class="pl-pds">"</span>com.github.saurfang<span class="pl-pds">"</span></span> <span class="pl-k">%%</span> <span class="pl-s"><span class="pl-pds">"</span>sparksql-protobuf<span class="pl-pds">"</span></span> <span class="pl-k">%</span> <span class="pl-s"><span class="pl-pds">"</span>0.1.2<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>org.apache.parquet<span class="pl-pds">"</span></span> <span class="pl-k">%</span> <span class="pl-s"><span class="pl-pds">"</span>parquet-protobuf<span class="pl-pds">"</span></span> <span class="pl-k">%</span> <span class="pl-s"><span class="pl-pds">"</span>1.8.1<span class="pl-pds">"</span></span>
)</pre>
  </div> 
  <h2><a href="https://github.com/saurfang/sparksql-protobuf#motivation" aria-hidden="true" class="anchor" id="user-content-motivation" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Motivation</h2> 
  <p>SparkSQL is very powerful and easy to use. However it has a few limitations and schema is only detected during runtime makes developers a lot less confident that they will get things right at first time. <em>Static</em> typing helps a lot! This is where protobuf comes in:</p> 
  <ol> 
   <li>Protobuf defines nested data structure easily</li> 
   <li>It doesn't constraint you to the 22 fields limit in case class (no longer true once we upgrade to 2.11+)</li> 
   <li>It is language agnostic and generates code that gives you native objects hence you get all the benefit of type checking and code completion unlike operating <code>Row</code> in Spark/SparkSQL</li> 
  </ol> 
  <h2><a href="https://github.com/saurfang/sparksql-protobuf#features" aria-hidden="true" class="anchor" id="user-content-features" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Features</h2> 
  <h3><a href="https://github.com/saurfang/sparksql-protobuf#read-parquet-file-as-rddprotobuf" aria-hidden="true" class="anchor" id="user-content-read-parquet-file-as-rddprotobuf" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Read Parquet file as <code>RDD[Protobuf]</code></h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">personsPB</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">ProtoParquetRDD</span>(sc, <span class="pl-s"><span class="pl-pds">"</span>persons.parquet<span class="pl-pds">"</span></span>, <span class="pl-c1">classOf</span>[<span class="pl-en">Person</span>])</pre>
  </div> 
  <p>where we need <code>SparkContext</code>, parquet path and protobuf class.</p> 
  <p>This converts the existing workflow:</p> 
  <ol> 
   <li>Ingest raw data as DataFrame with nested data structure</li> 
   <li>Create awkward runtime type checking udfs</li> 
   <li>Transform raw DataFrame using above udfs into a tabular DataFrame for data analytics</li> 
  </ol> 
  <p>to</p> 
  <ol> 
   <li>Ingest raw data as DataFrame with nested data structure and persist as Parquet file</li> 
   <li>Read Parquet file back as <code>RDD[Protobuf]</code></li> 
   <li>Perform any data transformation and extraction by working with compile typesafe Protobuf getters</li> 
   <li>Create a DataFrame out of the above transformation and perform additional downstream data analytics on the tabular DataFrame</li> 
  </ol> 
  <h3><a href="https://github.com/saurfang/sparksql-protobuf#infer-sparksql-schema-from-protobuf-definition" aria-hidden="true" class="anchor" id="user-content-infer-sparksql-schema-from-protobuf-definition" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Infer SparkSQL Schema from Protobuf Definition</h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">personSchema</span> <span class="pl-k">=</span> <span class="pl-en">ProtoReflection</span>.schemaFor[<span class="pl-en">Person</span>].dataType.<span class="pl-c1">asInstanceOf</span>[<span class="pl-en">StructType</span>]</pre>
  </div> 
  <h3><a href="https://github.com/saurfang/sparksql-protobuf#convert-rddprotobuf-to-dataframe" aria-hidden="true" class="anchor" id="user-content-convert-rddprotobuf-to-dataframe" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Convert <code>RDD[Protobuf]</code> to <code>DataFrame</code></h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.github.saurfang.parquet.proto.spark.sql.</span><span class="pl-v">_</span>
<span class="pl-k">val</span> <span class="pl-en">personsDF</span> <span class="pl-k">=</span> sqlContext.createDataFrame(protoPersons)</pre>
  </div> 
  <p><em>For more information, please see test cases.</em></p> 
  <h2><a href="https://github.com/saurfang/sparksql-protobuf#under-the-hood" aria-hidden="true" class="anchor" id="user-content-under-the-hood" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Under the hood</h2> 
  <ol> 
   <li><code>ProtoMessageConverter</code> has been improved to read from <a href="https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists" target="_blank">LIST specification</a> according to latest parquet documentation. This implementation should be backwards compatible and is able to read repeated fields generated by writers like SparkSQL.</li> 
   <li><code>ProtoMessageParquetInputFormat</code> helps the above process by correctly returning the built protobuf object as value.</li> 
   <li><code>ProtoParquetRDD</code> abstract the Hadoop input format and returns an RDD of your protobuf objects from parquet files directly.</li> 
   <li><code>ProtoReflection</code> infers SparkSQL schema from any Protobuf message class.</li> 
   <li><code>ProtoRDDConversions</code> converts Protobuf objects into SparkSQL rows.</li> 
  </ol> 
  <h2><a href="https://github.com/saurfang/sparksql-protobuf#related-work" aria-hidden="true" class="anchor" id="user-content-related-work" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Related Work</h2> 
  <p><a href="https://github.com/twitter/elephant-bird" target="_blank">Elephant Bird</a></p> 
  <p><a href="https://issues.apache.org/jira/browse/SPARK-9999" target="_blank">Spark-9999</a></p> 
 </article>
</div>