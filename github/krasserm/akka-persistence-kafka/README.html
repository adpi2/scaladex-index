<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/krasserm/akka-persistence-kafka#kafka-plugins-for-akka-persistence" aria-hidden="true" class="anchor" id="user-content-kafka-plugins-for-akka-persistence" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Kafka Plugins for Akka Persistence</h1> 
  <p>Replicated <a href="http://doc.akka.io/docs/akka/2.3.11/scala/persistence.html" target="_blank">Akka Persistence</a> journal and snapshot store backed by <a href="http://kafka.apache.org/" target="_blank">Apache Kafka</a>.</p> 
  <p><a href="https://travis-ci.org/krasserm/akka-persistence-kafka" target="_blank"><img src="https://camo.githubusercontent.com/01dd422d85cd8a5108e08de199b5376fe2acbde1/68747470733a2f2f7472617669732d63692e6f72672f6b7261737365726d2f616b6b612d70657273697374656e63652d6b61666b612e7376673f6272616e63683d747261766973" alt="Build Status" data-canonical-src="https://travis-ci.org/krasserm/akka-persistence-kafka.svg?branch=travis" style="max-width:100%;"></a></p> 
  <h2><a href="https://github.com/krasserm/akka-persistence-kafka#dependency" aria-hidden="true" class="anchor" id="user-content-dependency" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Dependency</h2> 
  <p>To include the Kafka plugins into your <code>sbt</code> project, add the following lines to your <code>build.sbt</code> file:</p> 
  <pre><code>resolvers += "krasserm at bintray" at "http://dl.bintray.com/krasserm/maven"

libraryDependencies += "com.github.krasserm" %% "akka-persistence-kafka" % “0.4”
</code></pre> 
  <p>This version of <code>akka-persistence-kafka</code> depends on Kafka 0.8.2.1, Akka 2.3.11 and is cross-built against Scala 2.10.4 and 2.11.6. A complete list of released versions is <a href="https://github.com/krasserm/akka-persistence-kafka/wiki/Releases" target="_blank">here</a>.</p> 
  <h2><a href="https://github.com/krasserm/akka-persistence-kafka#usage-hints" aria-hidden="true" class="anchor" id="user-content-usage-hints" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage hints</h2> 
  <p>Kafka does not permanently store log entries but rather deletes them after a configurable <em>retention time</em> which defaults to 7 days in Kafka 0.8.x. Therefore, applications need to take snapshots of their persistent actors at intervals that are smaller than the configured retention time (for example, every 3 days). This ensures that persistent actors can always be recovered successfully.</p> 
  <p>Alternatively, the retention time can be set to a maximum value so that Kafka will never delete old entries. In this case, all events written by a single persistent actor must fit on a single node. This is a limitation of the current implementation which may be removed in later versions. However, this limitation is likely not relevant when running Kafka with default (or comparable) retention times and taking snapshots.</p> 
  <p>The latest snapshot of a persistent actor is never deleted if <a href="http://kafka.apache.org/documentation.html#compaction" target="_blank">log compaction</a> is enabled. See also section <a href="https://github.com/krasserm/akka-persistence-kafka#configuration-hints" target="_blank">Configuration hints</a> for details how to properly configure Kafka for being used with the storage plugins.</p> 
  <h2><a href="https://github.com/krasserm/akka-persistence-kafka#journal-plugin" aria-hidden="true" class="anchor" id="user-content-journal-plugin" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Journal plugin</h2> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#activation" aria-hidden="true" class="anchor" id="user-content-activation" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Activation</h3> 
  <p>To activate the journal plugin, add the following line to <code>application.conf</code>:</p> 
  <pre><code>akka.persistence.journal.plugin = "kafka-journal"
</code></pre> 
  <p>This will run the journal plugin with default settings and connect to a Zookeeper instance running on <code>localhost:2181</code>. The Zookeeper connect string can be customized with the <code>kafka-journal.zookeeper.connect</code> configuration key (see also section <a href="https://github.com/krasserm/akka-persistence-kafka#kafka-cluster" target="_blank">Kafka cluster</a>). Recommended Kafka broker configurations are given in section <a href="https://github.com/krasserm/akka-persistence-kafka#configuration-hints" target="_blank">Configuration hints</a>.</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#use-cases" aria-hidden="true" class="anchor" id="user-content-use-cases" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Use cases</h3> 
  <ul> 
   <li>Akka Persistence <a href="http://doc.akka.io/docs/akka/2.3.9/scala/persistence.html#journal-plugin-api" target="_blank">journal plugin</a> (obvious).</li> 
   <li>Event publishing to <a href="https://github.com/krasserm/akka-persistence-kafka#user-defined-topics" target="_blank">user-defined topics</a>.</li> 
   <li>Event consumption from user-defined topics by <a href="https://github.com/krasserm/akka-persistence-kafka#external-consumers" target="_blank">external consumers</a>.</li> 
  </ul> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#journal-topics" aria-hidden="true" class="anchor" id="user-content-journal-topics" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Journal topics</h3> 
  <p>For each persistent actor, the plugin creates a Kafka topic where the topic name equals the actor's <code>persistenceId</code> (only if it contains alphanumeric, <code>.</code>, <code>-</code> or <code>_</code> characters, otherwise, all other characters are replaced by <code>_</code>). Events published to these topics are serialized <code>akka.persistence.PersistentRepr</code> objects (see <a href="http://doc.akka.io/docs/akka/2.3.9/scala/persistence.html#journal-plugin-api" target="_blank">journal plugin API</a>). Serialization of <code>PersistentRepr</code> objects can be <a href="http://doc.akka.io/docs/akka/2.3.11/scala/persistence.html#custom-serialization" target="_blank">customized</a>. Journal topics are mainly intended for internal use (for recovery of persistent actors) but can also be <a href="https://github.com/krasserm/akka-persistence-kafka#external-consumers" target="_blank">consumed externally</a>.</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#user-defined-topics" aria-hidden="true" class="anchor" id="user-content-user-defined-topics" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>User-defined topics</h3> 
  <p>The journal plugin can also publish events to user-defined topics. By default, all events generated by all persistent actors are published to a single <code>events</code> topic. This topic is intended for <a href="https://github.com/krasserm/akka-persistence-kafka#external-consumers" target="_blank">external consumption</a> only. Events published to user-defined topics are serialized <code>Event</code> objects</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">akka.persistence.kafka</span>

<span class="pl-c"><span class="pl-c">/**</span></span>
<span class="pl-c"> * Event published to user-defined topics.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * <span class="pl-k">@param</span> <span class="pl-v">persistenceId</span> Id of the persistent actor that generates event `data`.</span>
<span class="pl-c"> * <span class="pl-k">@param</span> <span class="pl-v">sequenceNr</span> Sequence number of the event.</span>
<span class="pl-c"> * <span class="pl-k">@param</span> <span class="pl-v">data</span> Event data generated by a persistent actor.</span>
<span class="pl-c"> <span class="pl-c">*/</span></span>
<span class="pl-k">case</span> <span class="pl-k">class</span> <span class="pl-en">Event</span>(<span class="pl-v">persistenceId</span>: <span class="pl-k">String</span>, <span class="pl-v">sequenceNr</span>: <span class="pl-k">Long</span>, <span class="pl-v">data</span>: <span class="pl-en">Any</span>)</pre>
  </div> 
  <p>where <code>data</code> is the actual event written by a persistent actor (by calling <code>persist</code> or <code>persistAsync</code>), <code>sequenceNr</code> is the event's sequence number and <code>persistenceId</code> the id of the persistent actor. <code>Event</code> objects are serialized with a <a href="https://github.com/google/protobuf" target="_blank">protobuf</a> serializer and event <code>data</code> serialization can be customized with a <a href="http://doc.akka.io/docs/akka/2.3.11/scala/persistence.html#custom-serialization" target="_blank">user-defined serializer</a> in the same way as for <a href="https://github.com/krasserm/akka-persistence-kafka#journal-topics" target="_blank">journal topics</a>. Custom serializer configurations always apply to both, journal topics and user-defined topics.</p> 
  <p>For publishing events to user-defined topics the journal plugin uses an <code>EventTopicMapper</code>:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">akka.persistence.kafka</span>

<span class="pl-c"><span class="pl-c">/**</span></span>
<span class="pl-c"> * Defines a mapping of events to user-defined topics.</span>
<span class="pl-c"> <span class="pl-c">*/</span></span>
<span class="pl-k">trait</span> <span class="pl-en">EventTopicMapper</span> {
<span class="pl-c">  <span class="pl-c">/**</span></span>
<span class="pl-c">   * Maps an event to zero or more topics.</span>
<span class="pl-c">   *</span>
<span class="pl-c">   * <span class="pl-k">@param</span> <span class="pl-v">event</span> event to be mapped.</span>
<span class="pl-c">   * <span class="pl-k">@return</span> a sequence of topic names.</span>
<span class="pl-c">   <span class="pl-c">*/</span></span>
  <span class="pl-k">def</span> <span class="pl-en">topicsFor</span>(<span class="pl-v">event</span>: <span class="pl-en">Event</span>)<span class="pl-k">:</span> immutable.<span class="pl-en">Seq</span>[<span class="pl-k">String</span>]
}</pre>
  </div> 
  <p>The default mapper is <code>DefaultEventTopicMapper</code> which maps all events to the <code>events</code> topic. It is configured in the <a href="https://github.com/krasserm/akka-persistence-kafka#reference-configuration" target="_blank">reference configuration</a> as follows:</p> 
  <pre><code>kafka-journal.event.producer.topic.mapper.class = "akka.persistence.kafka.DefaultEventTopicMapper"
</code></pre> 
  <p>To customize the mapping of events to user-defined topics, applications can implement and configure a custom <code>EventTopicMapper</code>. For example, in order to publish</p> 
  <ul> 
   <li>events from persistent actor <code>a</code> to topics <code>topic-a-1</code> and <code>topic-a-2</code> and</li> 
   <li>events from persistent actor <code>b</code> to topic <code>topic-b</code></li> 
  </ul> 
  <p>and to turn of publishing of events from all other actors, one would implement the following <code>ExampleEventTopicMapper</code></p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">package</span> <span class="pl-en">akka.persistence.kafka.example</span>

<span class="pl-k">class</span> <span class="pl-en">ExampleEventTopicMapper</span> <span class="pl-k">extends</span> <span class="pl-e">EventTopicMapper</span> {
  <span class="pl-k">def</span> <span class="pl-en">topicsFor</span>(<span class="pl-v">event</span>: <span class="pl-en">Event</span>)<span class="pl-k">:</span> <span class="pl-en">Seq</span>[<span class="pl-k">String</span>] <span class="pl-k">=</span> event.persistenceId <span class="pl-k">match</span> {
    <span class="pl-k">case</span> <span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-en">List</span>(<span class="pl-s"><span class="pl-pds">"</span>topic-a-1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>topic-a-2<span class="pl-pds">"</span></span>)
    <span class="pl-k">case</span> <span class="pl-s"><span class="pl-pds">"</span>b<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-en">List</span>(<span class="pl-s"><span class="pl-pds">"</span>topic-b<span class="pl-pds">"</span></span>)
    <span class="pl-k">case</span> _   <span class="pl-k">=&gt;</span> <span class="pl-c1">Nil</span>
  }</pre>
  </div> 
  <p>and configure it in <code>application.conf</code>:</p> 
  <pre><code>kafka-journal.event.producer.topic.mapper.class = "akka.persistence.kafka.example.ExampleEventTopicMapper"
</code></pre> 
  <p>To turn off publishing events to user-defined topics, the <code>EmptyEventTopicMapper</code> should be configured.</p> 
  <pre><code>kafka-journal.event.producer.topic.mapper.class = "akka.persistence.kafka.EmptyEventTopicMapper"
</code></pre> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#external-consumers" aria-hidden="true" class="anchor" id="user-content-external-consumers" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>External consumers</h3> 
  <p>The following example shows how to consume <code>Event</code>s from a user-defined topic with name <code>topic-a-2</code> (see <a href="https://github.com/krasserm/akka-persistence-kafka#user-defined-topics" target="_blank">previous</a> example) using Kafka's <a href="http://kafka.apache.org/documentation.html#highlevelconsumerapi" target="_blank">high-level consumer API</a>:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">java.util.</span><span class="pl-v">Properties</span>

<span class="pl-k">import</span> <span class="pl-v">akka.persistence.kafka.</span>{<span class="pl-v">EventDecoder</span>, <span class="pl-v">Event</span>}

<span class="pl-k">import</span> <span class="pl-v">kafka.consumer.</span>{<span class="pl-v">Consumer</span>, <span class="pl-v">ConsumerConfig</span>}
<span class="pl-k">import</span> <span class="pl-v">kafka.serializer.</span><span class="pl-v">StringDecoder</span>

<span class="pl-k">val</span> <span class="pl-en">props</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">Properties</span>()
props.put(<span class="pl-s"><span class="pl-pds">"</span>group.id<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>consumer-1<span class="pl-pds">"</span></span>)
props.put(<span class="pl-s"><span class="pl-pds">"</span>zookeeper.connect<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>localhost:2181<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">//</span> ...</span>

<span class="pl-k">val</span> <span class="pl-en">system</span> <span class="pl-k">=</span> <span class="pl-en">ActorSystem</span>(<span class="pl-s"><span class="pl-pds">"</span>consumer<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">consConn</span> <span class="pl-k">=</span> <span class="pl-en">Consumer</span>.create(<span class="pl-k">new</span> <span class="pl-en">ConsumerConfig</span>(props))
<span class="pl-k">val</span> <span class="pl-en">streams</span> <span class="pl-k">=</span> consConn.createMessageStreams(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>topic-a-2<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">1</span>),
  keyDecoder <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">StringDecoder,</span> valueDecoder <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">EventDecoder</span>(system))

streams(<span class="pl-s"><span class="pl-pds">"</span>topic-a-2<span class="pl-pds">"</span></span>)(<span class="pl-c1">0</span>).foreach { mm <span class="pl-k">=&gt;</span>
  <span class="pl-k">val</span> <span class="pl-en">event</span><span class="pl-k">:</span> <span class="pl-en">Event</span> <span class="pl-k">=</span> mm.message
  println(s<span class="pl-s"><span class="pl-pds">"</span>consumed ${event}<span class="pl-pds">"</span></span>)
}</pre>
  </div> 
  <p>Applications may also consume serialized <code>PersistentRepr</code> objects from journal topics and deserialize them with Akka's serialization extension:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">java.util.</span><span class="pl-v">Properties</span>

<span class="pl-k">import</span> <span class="pl-v">akka.actor.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">akka.persistence.</span><span class="pl-v">PersistentRepr</span>
<span class="pl-k">import</span> <span class="pl-v">akka.serialization.</span><span class="pl-v">SerializationExtension</span>

<span class="pl-k">import</span> <span class="pl-v">com.typesafe.config.</span><span class="pl-v">ConfigFactory</span>

<span class="pl-k">import</span> <span class="pl-v">kafka.consumer.</span>{<span class="pl-v">Consumer</span>, <span class="pl-v">ConsumerConfig</span>}
<span class="pl-k">import</span> <span class="pl-v">kafka.serializer.</span>{<span class="pl-v">DefaultDecoder</span>, <span class="pl-v">StringDecoder</span>}

<span class="pl-k">val</span> <span class="pl-en">props</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">Properties</span>()
props.put(<span class="pl-s"><span class="pl-pds">"</span>group.id<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>consumer-2<span class="pl-pds">"</span></span>)
props.put(<span class="pl-s"><span class="pl-pds">"</span>zookeeper.connect<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>localhost:2181<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">//</span> ...</span>

<span class="pl-k">val</span> <span class="pl-en">system</span> <span class="pl-k">=</span> <span class="pl-en">ActorSystem</span>(<span class="pl-s"><span class="pl-pds">"</span>example<span class="pl-pds">"</span></span>)
<span class="pl-k">val</span> <span class="pl-en">extension</span> <span class="pl-k">=</span> <span class="pl-en">SerializationExtension</span>(system)

<span class="pl-k">val</span> <span class="pl-en">consConn</span> <span class="pl-k">=</span> <span class="pl-en">Consumer</span>.create(<span class="pl-k">new</span> <span class="pl-en">ConsumerConfig</span>(props))
<span class="pl-k">val</span> <span class="pl-en">streams</span> <span class="pl-k">=</span> consConn.createMessageStreams(<span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">1</span>),
  keyDecoder <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">StringDecoder,</span> valueDecoder <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">DefaultDecoder)</span>

streams(<span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span>)(<span class="pl-c1">0</span>).foreach { mm <span class="pl-k">=&gt;</span>
  <span class="pl-k">val</span> <span class="pl-en">persistent</span><span class="pl-k">:</span> <span class="pl-en">PersistentRepr</span> <span class="pl-k">=</span> extension.deserialize(mm.message, <span class="pl-c1">classOf</span>[<span class="pl-en">PersistentRepr</span>]).get
  println(s<span class="pl-s"><span class="pl-pds">"</span>consumed ${persistent}<span class="pl-pds">"</span></span>)
}</pre>
  </div> 
  <p>There are many other libraries that can be used to consume (event) streams from Kafka topics, such as <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank">Spark Streaming</a>, to mention only one example.</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#implementation-notes" aria-hidden="true" class="anchor" id="user-content-implementation-notes" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Implementation notes</h3> 
  <ul> 
   <li>During initialization, the journal plugin fetches cluster metadata from Zookeeper which may take up to a few seconds.</li> 
   <li>The journal plugin always writes <code>PersistentRepr</code> entries to partition 0 of journal topics. This ensures that all events written by a single persistent actor are stored in correct order. Later versions of the plugin may switch to a higher partition after having written a configurable number of events to the current partition.</li> 
   <li>The journal plugin distributes <code>Event</code> entries to all available partitions of user-defined topics. The partition key is the event's <code>persistenceId</code> so that a partial ordering of events is preserved when consuming events from user-defined topics. In other words, events written by a single persistent actor are always consumed in correct order but the relative ordering of events from different persistent actors is not defined.</li> 
  </ul> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#current-limitations" aria-hidden="true" class="anchor" id="user-content-current-limitations" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Current limitations</h3> 
  <ul> 
   <li>The journal plugin does not support features that have been deprecated in Akka 2.3.4 (channels and single event deletions).</li> 
   <li>Range deletions are not persistent (which may not be relevant for applications that configure Kafka with reasonably small retention times).</li> 
  </ul> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#example-source-code" aria-hidden="true" class="anchor" id="user-content-example-source-code" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Example source code</h3> 
  <p>The complete source code of all examples from previous sections is in <a href="https://github.com/krasserm/akka-persistence-kafka/blob/master/src/test/scala/akka/persistence/kafka/example/Example.scala" target="_blank">Example.scala</a>, the corresponding configuration in <a href="https://github.com/krasserm/akka-persistence-kafka/blob/master/src/test/resources/example.conf" target="_blank">example.conf</a>.</p> 
  <h2><a href="https://github.com/krasserm/akka-persistence-kafka#snapshot-store-plugin" aria-hidden="true" class="anchor" id="user-content-snapshot-store-plugin" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Snapshot store plugin</h2> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#activation-1" aria-hidden="true" class="anchor" id="user-content-activation-1" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Activation</h3> 
  <p>To activate the snapshot store plugin, add the following line to <code>application.conf</code>:</p> 
  <pre><code>akka.persistence.snapshot-store.plugin = "kafka-snapshot-store"
</code></pre> 
  <p>This will run the snapshot store plugin with default settings and connect to a Zookeeper instance running on <code>localhost:2181</code>. The Zookeeper connect string can be customized with the <code>kafka-snapshot-store.zookeeper.connect</code> configuration key (see also section <a href="https://github.com/krasserm/akka-persistence-kafka#kafka-cluster" target="_blank">Kafka cluster</a>). Recommended Kafka broker configurations are given in section <a href="https://github.com/krasserm/akka-persistence-kafka#configuration-hints" target="_blank">Configuration hints</a>.</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#snapshot-topics" aria-hidden="true" class="anchor" id="user-content-snapshot-topics" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Snapshot topics</h3> 
  <p>For each persistent actor, the plugin creates a Kafka topic where the topic name equals the actor's <code>persistenceId</code>, prefixed by the value of the <code>kafka-snapshot-store.prefix</code> configuration key which defaults to <code>snapshot-</code>. For example, if an actor's <code>persistenceId</code> is <code>example</code>, its snapshots are published to topic <code>snapshot-example</code>. For persistent views, the <code>viewId</code> is taken instead of the <code>persistenceId</code>.</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#implementation-notes-1" aria-hidden="true" class="anchor" id="user-content-implementation-notes-1" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Implementation notes</h3> 
  <ul> 
   <li>During initialization, the journal plugin fetches cluster metadata from Zookeeper which may take up to a few seconds.</li> 
   <li>The journal plugin always writes snapshots to partition 0 of snapshot topics.</li> 
  </ul> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#current-limitations-1" aria-hidden="true" class="anchor" id="user-content-current-limitations-1" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Current limitations</h3> 
  <ul> 
   <li>Deletions are not persistent (which may not be relevant for applications that configure Kafka with reasonably small retention times).</li> 
  </ul> 
  <h2><a href="https://github.com/krasserm/akka-persistence-kafka#kafka" aria-hidden="true" class="anchor" id="user-content-kafka" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Kafka</h2> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#kafka-cluster" aria-hidden="true" class="anchor" id="user-content-kafka-cluster" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Kafka cluster</h3> 
  <p>To connect to an existing Kafka cluster, an application must set a value for the <code>kafka-journal.zookeeper.connect</code> key in its <code>application.conf</code>:</p> 
  <pre><code>kafka-journal.zookeeper.connect = "&lt;host1&gt;:&lt;port1&gt;,&lt;host2&gt;:&lt;port2&gt;,..."
</code></pre> 
  <p>If you want to run a Kafka cluster on a single node, you may find <a href="http://www.michael-noll.com/blog/2013/03/13/running-a-multi-broker-apache-kafka-cluster-on-a-single-node/" target="_blank">this article</a> useful.</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#test-server" aria-hidden="true" class="anchor" id="user-content-test-server" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Test server</h3> 
  <p>To use the test server, the following additional dependencies must be added to <code>build.sbt</code>:</p> 
  <pre><code>libraryDependencies ++= Seq(
  "com.github.krasserm" %% "akka-persistence-kafka" % "0.4" % "test" classifier "tests",
  "org.apache.curator" % "curator-test" % "2.7.1" % "test"
)
</code></pre> 
  <p>This makes the <code>TestServer</code> class available which can be used to start a single Kafka and Zookeeper instance:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">akka.persistence.kafka.server.</span><span class="pl-v">TestServer</span>

<span class="pl-c"><span class="pl-c">//</span> start a local Kafka and Zookeeper instance</span>
<span class="pl-k">val</span> <span class="pl-en">server</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">TestServer</span>() 

<span class="pl-c"><span class="pl-c">//</span> use the local instance</span>
<span class="pl-c"><span class="pl-c">//</span> ...</span>

<span class="pl-c"><span class="pl-c">//</span> and stop it</span>
server.stop()</pre>
  </div> 
  <p>The <code>TestServer</code> configuration can be customized with the <code>test-server.*</code> configuration keys (see <a href="https://github.com/krasserm/akka-persistence-kafka#reference-configuration" target="_blank">reference configuration</a> for details).</p> 
  <h3><a href="https://github.com/krasserm/akka-persistence-kafka#configuration-hints" aria-hidden="true" class="anchor" id="user-content-configuration-hints" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Configuration hints</h3> 
  <p>The following <a href="http://kafka.apache.org/documentation.html#brokerconfigs" target="_blank">broker configurations</a> are recommended for being used with the storage plugins:</p> 
  <ul> 
   <li><code>num.partitions</code> should be set to <code>1</code> by default because the plugins only write to partition 0 of <a href="https://github.com/krasserm/akka-persistence-kafka#journal-topics" target="_blank">journal topics</a> and <a href="https://github.com/krasserm/akka-persistence-kafka#snapshot-topics" target="_blank">snapshot topics</a>. If a higher number of partitions is needed for <a href="https://github.com/krasserm/akka-persistence-kafka#user-defined-topics" target="_blank">user-defined topics</a> (e.g. for scalability or throughput reasons) then this should be configured manually with the <code>kafka-topics</code> command line tool.</li> 
   <li><code>default.replication.factor</code> should be set to at least <code>2</code> for high-availability of topics created by the plugins.</li> 
   <li><code>message.max.bytes</code> and <code>replica.fetch.max.bytes</code> should be set to a value that is larger than the largest snapshot size. The default value is <code>1024 * 1024</code> which may be large enough for journal entries but likely to small for snapshots. When changing these settings make sure to also set <code>kafka-snapshot-store.consumer.fetch.message.max.bytes</code> and <code>kafka-journal.consumer.fetch.message.max.bytes</code> to this value.</li> 
   <li><code>log.cleanup.policy</code> must be set to <code>"compact"</code> otherwise the most recent snapshot may be deleted if the retention time is exceeded and complete state recovery of persistent actors is not possible any more.</li> 
  </ul> 
  <p>See also section <a href="https://github.com/krasserm/akka-persistence-kafka#usage-hints" target="_blank">Usage hints</a>.</p> 
  <h2><a href="https://github.com/krasserm/akka-persistence-kafka#reference-configuration" aria-hidden="true" class="anchor" id="user-content-reference-configuration" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Reference configuration</h2> 
  <pre><code>kafka-journal {

  # FQCN of the Kafka journal plugin
  class = "akka.persistence.kafka.journal.KafkaJournal"

  # Dispatcher for the plugin actor
  plugin-dispatcher = "kafka-journal.default-dispatcher"

  # Number of concurrent writers (should be &lt;= number of available threads in
  # dispatcher).
  write-concurrency = 8

  # The partition to use when publishing to and consuming from journal topics.
  partition = 0

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }

  consumer {
    # -------------------------------------------------------------------
    # Simple consumer configuration (used for message replay and reading
    # metadata).
    #
    # See http://kafka.apache.org/documentation.html#consumerconfigs
    # See http://kafka.apache.org/documentation.html#simpleconsumerapi
    # -------------------------------------------------------------------

    socket.timeout.ms = 30000

    socket.receive.buffer.bytes = 65536

    fetch.message.max.bytes = 1048576
  }

  producer {
    # -------------------------------------------------------------------
    # PersistentRepr producer (to journal topics) configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    #
    # The metadata.broker.list property is set dynamically by the journal.
    # No need to set it here.
    # -------------------------------------------------------------------

    request.required.acks = 1

    # DO NOT CHANGE!
    producer.type = "sync"

    # DO NOT CHANGE!
    partitioner.class = "akka.persistence.kafka.StickyPartitioner"

    # DO NOT CHANGE!
    key.serializer.class = "kafka.serializer.StringEncoder"

    # Increase if hundreds of topics are created during initialization.
    message.send.max.retries = 5

    # Increase if hundreds of topics are created during initialization.
    retry.backoff.ms = 100

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  event.producer {
    # -------------------------------------------------------------------
    # Event producer (to user-defined topics) configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    # -------------------------------------------------------------------

    producer.type = "sync"

    request.required.acks = 0

    topic.mapper.class = "akka.persistence.kafka.DefaultEventTopicMapper"

    key.serializer.class = "kafka.serializer.StringEncoder"

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  zookeeper {
    # -------------------------------------------------------------------
    # Zookeeper client configuration
    # -------------------------------------------------------------------

    connect = "localhost:2181"

    session.timeout.ms = 6000

    connection.timeout.ms = 6000

    sync.time.ms = 2000
  }
}

kafka-snapshot-store {

  # FQCN of the Kafka snapshot store plugin
  class = "akka.persistence.kafka.snapshot.KafkaSnapshotStore"

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "kafka-snapshot-store.default-dispatcher"

  # The partition to use when publishing to and consuming from snapshot topics.
  partition = 0

  # Topic name prefix (which prepended to persistenceId)
  prefix = "snapshot-"

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }

  consumer {
    # -------------------------------------------------------------------
    # Simple consumer configuration (used for loading snapshots and
    # reading metadata).
    #
    # See http://kafka.apache.org/documentation.html#consumerconfigs
    # See http://kafka.apache.org/documentation.html#simpleconsumerapi
    # -------------------------------------------------------------------

    socket.timeout.ms = 30000

    socket.receive.buffer.bytes = 65536

    fetch.message.max.bytes = 1048576
  }

  producer {
    # -------------------------------------------------------------------
    # Snapshot producer configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    #
    # The metadata.broker.list property is set dynamically by the journal.
    # No need to set it here.
    # -------------------------------------------------------------------

    request.required.acks = 1

    producer.type = "sync"

    # DO NOT CHANGE!
    partitioner.class = "akka.persistence.kafka.StickyPartitioner"

    # DO NOT CHANGE!
    key.serializer.class = "kafka.serializer.StringEncoder"

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  zookeeper {
    # -------------------------------------------------------------------
    # Zookeeper client configuration
    # -------------------------------------------------------------------

    connect = "localhost:2181"

    session.timeout.ms = 6000

    connection.timeout.ms = 6000

    sync.time.ms = 2000
  }
}

test-server {
  # -------------------------------------------------------------------
  # Test Kafka and Zookeeper server configuration.
  #
  # See http://kafka.apache.org/documentation.html#brokerconfigs
  # -------------------------------------------------------------------

  zookeeper {

    port = 2181

    dir = "data/zookeeper"
  }

  kafka {

    broker.id = 1

    port = 6667

    num.partitions = 2

    log.cleanup.policy = "compact"

    log.dirs = data/kafka

    log.index.size.max.bytes = 1024
  }
}

akka {
  actor {
    serializers {
      kafka-snapshot = "akka.persistence.kafka.snapshot.KafkaSnapshotSerializer"
    }

    serialization-bindings {
      "akka.persistence.kafka.snapshot.KafkaSnapshot" = kafka-snapshot
    }
  }
}    
</code></pre> 
 </article>
</div>