<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-utils" class="anchor" href="https://github.com/bigdatagenomics/utils#utils" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>utils</h1> 
  <p>General (non-omics) code used across BDG products. Apache 2 licensed.</p> 
  <p><a href="https://coveralls.io/github/bigdatagenomics/utils?branch=master" target="_blank"><img src="https://camo.githubusercontent.com/619d8ce26e260930d3bd5dd6b2ab15273ac4e18c/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6269676461746167656e6f6d6963732f7574696c732f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/bigdatagenomics/utils/badge.svg?branch=master" style="max-width:100%;"></a></p> 
  <h1><a id="user-content-usage" class="anchor" href="https://github.com/bigdatagenomics/utils#usage" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage</h1> 
  <h2><a id="user-content-instrumentation" class="anchor" href="https://github.com/bigdatagenomics/utils#instrumentation" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Instrumentation</h2> 
  <p>The bdg-utils project contains functionality for instrumenting Spark operations, as well as Scala function calls. Function calls can be recorded both in the Spark Driver, and in the Spark Workers.</p> 
  <p>The following is an example of some instrumentation output from the <a href="https://github.com/bigdatagenomics/adam" target="_blank">ADAM Project</a>:</p> 
  <pre><code>Timings
+------------------------------------------------------------+--------------+----------------+-------------+----------+----------------+----------------+----------------+
|                           Metric                           | Worker Total |  Driver Total  | Driver Only |  Count   |      Mean      |      Min       |      Max       |
+------------------------------------------------------------+--------------+----------------+-------------+----------+----------------+----------------+----------------+
| └─ Base Quality Recalibration                              |            - | 2 mins 36 secs |   644.45 ms |        1 | 2 mins 36 secs | 2 mins 36 secs | 2 mins 36 secs |
|     ├─ map at DecadentRead.scala:50                        |            - |        6.72 ms |           - |        1 |        6.72 ms |        6.72 ms |        6.72 ms |
|     │   └─ function call                                   |   21.45 secs |              - |           - |   747308 |       28.71 µs |              0 |      183.24 ms |
|     ├─ filter at BaseQualityRecalibration.scala:71         |            - |       20.09 ms |           - |        1 |       20.09 ms |       20.09 ms |       20.09 ms |
|     │   └─ function call                                   |    170.74 ms |              - |           - |   373654 |         456 ns |              0 |        1.37 ms |
|     ├─ flatMap at BaseQualityRecalibration.scala:71        |            - |        9.88 ms |           - |        1 |        9.88 ms |        9.88 ms |        9.88 ms |
|     │   └─ function call                                   |   46.17 secs |              - |           - |   363277 |      127.08 µs |          69 µs |       54.42 ms |
|     ├─ map at BaseQualityRecalibration.scala:82            |            - |        5.37 ms |           - |        1 |        5.37 ms |        5.37 ms |        5.37 ms |
|     │   └─ function call                                   |    6.58 secs |              - |           - | 33940138 |         193 ns |              0 |       44.63 ms |
|     ├─ aggregate at BaseQualityRecalibration.scala:83      |            - | 2 mins 35 secs |           - |        1 | 2 mins 35 secs | 2 mins 35 secs | 2 mins 35 secs |
|     │   └─ function call                                   |   34.49 secs |              - |           - | 33940139 |        1.02 µs |              0 |      510.07 ms |
|     │       └─ Observation Accumulator: seq                |   26.91 secs |              - |           - | 33940138 |         792 ns |              0 |      510.07 ms |
|     └─ map at BaseQualityRecalibration.scala:95            |            - |       71.76 ms |           - |        1 |       71.76 ms |       71.76 ms |       71.76 ms |
|         └─ function call                                   |   57.08 secs |              - |           - |   373654 |      152.76 µs |          89 µs |       39.02 ms |
|             └─ Recalibrate Read                            |   56.85 secs |              - |           - |   373654 |      152.14 µs |          88 µs |       39.01 ms |
|                 └─ Compute Quality Score                   |   51.48 secs |              - |           - |   373654 |      137.76 µs |          75 µs |        34.4 ms |
|                     └─ Get Extra Values                    |   19.01 secs |              - |           - |   373654 |       50.88 µs |          22 µs |       33.31 ms |
| └─ Save File In ADAM Format                                |            - | 2 mins 33 secs |     89.8 ms |        1 | 2 mins 33 secs | 2 mins 33 secs | 2 mins 33 secs |
|     ├─ map at ADAMRDDFunctions.scala:73                    |            - |       30.45 ms |           - |        1 |       30.45 ms |       30.45 ms |       30.45 ms |
|     │   └─ function call                                   |     126.9 ms |              - |           - |   373654 |         339 ns |              0 |          65 µs |
|     └─ saveAsNewAPIHadoopFile at ADAMRDDFunctions.scala:75 |            - | 2 mins 33 secs |           - |        1 | 2 mins 33 secs | 2 mins 33 secs | 2 mins 33 secs |
|         └─ Write ADAM Record                               |   12.22 secs |              - |           - |   373654 |       32.71 µs |              0 |      359.63 ms |
+------------------------------------------------------------+--------------+----------------+-------------+----------+----------------+----------------+----------------+

Spark Operations
+----------+-----------------------------------------------------+---------------+----------------+----------------+----------+
| Sequence |                      Operation                      | Is New Stage? | Stage Duration |  Driver Total  | Stage ID |
+----------+-----------------------------------------------------+---------------+----------------+----------------+----------+
| 1        | map at DecadentRead.scala:50                        | false         |              - |        6.72 ms | -        |
| 2        | filter at BaseQualityRecalibration.scala:71         | false         |              - |       20.09 ms | -        |
| 3        | flatMap at BaseQualityRecalibration.scala:71        | false         |              - |        9.88 ms | -        |
| 4        | map at BaseQualityRecalibration.scala:82            | false         |              - |        5.37 ms | -        |
| 5        | aggregate at BaseQualityRecalibration.scala:83      | true          | 2 mins 35 secs | 2 mins 35 secs | 1        |
| 6        | map at BaseQualityRecalibration.scala:95            | false         |              - |       71.76 ms | -        |
| 7        | map at ADAMRDDFunctions.scala:73                    | false         |              - |       30.45 ms | -        |
| 8        | saveAsNewAPIHadoopFile at ADAMRDDFunctions.scala:75 | true          | 2 mins 32 secs | 2 mins 33 secs | 2        |
+----------+-----------------------------------------------------+---------------+----------------+----------------+----------+
</code></pre> 
  <h3><a id="user-content-basic-usage" class="anchor" href="https://github.com/bigdatagenomics/utils#basic-usage" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Basic Usage</h3> 
  <p>First, initialize the <code>Metrics</code> object and create a Spark listener:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.bdgenomics.utils.instrumentation.</span><span class="pl-v">Metrics</span>
<span class="pl-k">import</span> <span class="pl-v">org.bdgenomics.utils.instrumentation.</span>{<span class="pl-v">RecordedMetrics</span>, <span class="pl-v">MetricsListener</span>}
<span class="pl-en">Metrics</span>.initialize(sparkContext)
<span class="pl-k">val</span> <span class="pl-en">metricsListener</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">MetricsListener</span>(<span class="pl-k">new</span> <span class="pl-en">RecordedMetrics</span>())
sparkContext.addSparkListener(metricsListener)</pre>
  </div> 
  <p>Metrics collection is turned on by calling the <code>Metrics.initialize</code> method. Calling the <code>initialize</code> method also resets any previously-recorded metrics, so it is advisable to call it at the start of every Spark job. It is currently also necessary to create a Spark listener and register this in the Spark context (though this requirement may be removed in future versions).</p> 
  <p>Then, to instrument a Spark RDD called <code>rdd</code>:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.spark.rdd.MetricsContext.</span><span class="pl-v">_</span>
<span class="pl-k">val</span> <span class="pl-en">instrumentedRDD</span> <span class="pl-k">=</span> rdd.instrument()</pre>
  </div> 
  <p>When any operations are performed on <code>instrumentedRDD</code> the RDD operation will be instrumented, along with any functions that the operation uses. All subsequent RDD operations will be instrumented until the <code>unInstrument</code> method is called on an RDD. For example, consider the following code:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">array</span> <span class="pl-k">=</span> instrumentedRDD.map(_<span class="pl-k">+</span><span class="pl-c1">1</span>).keyBy(_<span class="pl-k">%</span><span class="pl-c1">2</span>).groupByKey().collect()
<span class="pl-k">val</span> <span class="pl-en">writer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">PrintWriter</span>(<span class="pl-k">new</span> <span class="pl-en">OutputStreamWriter</span>(<span class="pl-en">System</span>.out))
<span class="pl-en">Metrics</span>.print(writer, <span class="pl-en">Some</span>(metricsListener.metrics.sparkMetrics.stageTimes))
writer.close()</pre>
  </div> 
  <p>This will result in output like the following:</p> 
  <pre><code>Timings
+------------------------------------------+--------------+--------------+-------------+-------+----------+----------+----------+
|                  Metric                  | Worker Total | Driver Total | Driver Only | Count |   Mean   |   Min    |   Max    |
+------------------------------------------+--------------+--------------+-------------+-------+----------+----------+----------+
| └─ map at BdgUtilsTester.scala:26        |            - |     82.84 ms |           - |     1 | 82.84 ms | 82.84 ms | 82.84 ms |
|     └─ function call                     |       321 µs |            - |           - |     5 |  64.2 µs |     8 µs |   284 µs |
| └─ keyBy at BdgUtilsTester.scala:26      |            - |     11.52 ms |           - |     1 | 11.52 ms | 11.52 ms | 11.52 ms |
|     └─ function call                     |        63 µs |            - |           - |     5 |  12.6 µs |     7 µs |    34 µs |
| └─ groupByKey at BdgUtilsTester.scala:26 |            - |     41.63 ms |           - |     1 | 41.63 ms | 41.63 ms | 41.63 ms |
| └─ collect at BdgUtilsTester.scala:26    |            - |     354.3 ms |           - |     1 | 354.3 ms | 354.3 ms | 354.3 ms |
+------------------------------------------+--------------+--------------+-------------+-------+----------+----------+----------+

Spark Operations
+----------+---------------------------------------+---------------+----------------+--------------+----------+
| Sequence |               Operation               | Is New Stage? | Stage Duration | Driver Total | Stage ID |
+----------+---------------------------------------+---------------+----------------+--------------+----------+
| 1        | map at BdgUtilsTester.scala:26        | false         |              - |     82.84 ms | -        |
| 2        | keyBy at BdgUtilsTester.scala:26      | true          |         124 ms |     11.52 ms | 0        |
| 3        | groupByKey at BdgUtilsTester.scala:26 | false         |              - |     41.63 ms | -        |
| 4        | collect at BdgUtilsTester.scala:26    | true          |          53 ms |     354.3 ms | 1        |
+----------+---------------------------------------+---------------+----------------+--------------+----------+
</code></pre> 
  <h4><a id="user-content-timings-table" class="anchor" href="https://github.com/bigdatagenomics/utils#timings-table" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Timings Table</h4> 
  <p>The "Timings" table contains each Spark operation, as well as timings for each of the functions that the Spark operations use. This table has 3 columns representing the total time for a particular entry, as follows:</p> 
  <ul> 
   <li>The "Worker Total" column is the total time spent in the Spark worker nodes (in other words, within an RDD operation).</li> 
   <li>The "Driver Total" column is the total time spent in the Spark driver program (in other words, outside of any RDD operations), including time spent waiting for Spark operations to complete. Since Spark executes operations lazily, this column can be misleading. Typically the driver program will only wait for operations that return data back to the driver program. Therefore, to investigate the performance of the driver program itself, it is typically better to use the "Driver Only" column.</li> 
   <li>The "Driver Only" column is the total time spent in the driver program, <em>excluding</em> any time spent waiting for Spark operations to complete. Note that in the above example the "Driver Only" column is always empty, as nothing in the driver program has been instrumented. See "Instrumenting Function Calls" for details of how to instrument the driver program.</li> 
  </ul> 
  <p>For an explanation of the difference between the Spark driver program and worker nodes see the <a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">Spark Cluster Mode Overview</a>.</p> 
  <h4><a id="user-content-spark-operations-table" class="anchor" href="https://github.com/bigdatagenomics/utils#spark-operations-table" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark Operations Table</h4> 
  <p>The "Spark Operations" table contains more details about the Spark operations. The operations are displayed in the order in which they were executed, and are labelled with the location in the driver program from which they are called. In addition, the following columns are shown:</p> 
  <ul> 
   <li>The "Is New Stage?" column specifies whether a particular operation required the creation of a new "stage" in Spark. A stage is a group of operations that must be completed before subsequent operations on the same data set can proceed. (for example, a <a href="http://spark.apache.org/docs/latest/programming-guide.html#shuffle-operations" target="_blank">shuffle operation</a>).</li> 
   <li>The "Stage Duration" column specifies the total duration of a particular stage. That is, is represents the duration of a particular entry, plus <em>all of the preceeding entries until the end of the previous stage</em>, for a particular data set. It is not possible to obtain the time for individual operations within a stage - however it is possible to obtain timings for functions that are executed by these operations. See the description of the "Timings" table (above) for more information.</li> 
   <li>The "Driver Total" column is the total time spent waiting for an operation to complete in the Spark driver program. This matches the corresponding column in the "Timings" table. See the description of that table (above) for further details.</li> 
   <li>The "Stage ID" column is Spark's internal stage ID.</li> 
  </ul> 
  <p><strong>IMPORTANT</strong>: When using bdg-utils instrumentation it is not a good idea to import <code>SparkContext._</code>, as the implicit conversions in there may conflict with those required for instrumentation. Instead it is better to import <code>MetricsContext._</code> and import only the specific parts of <code>SparkContext</code> that are required for your application. Avoid importing <code>rddToPairRDDFunctions</code> and <code>rddToOrderedRDDFunctions</code> from <code>SparkContext</code> as they will conflict with the corresponding methods in <code>MetricsContext</code>.</p> 
  <h3><a id="user-content-instrumenting-function-calls" class="anchor" href="https://github.com/bigdatagenomics/utils#instrumenting-function-calls" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Instrumenting Function Calls</h3> 
  <p>As well as instrumenting top-level functions used Spark operations, it is possible to instrument nested function calls. For example, consider the following code:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">object</span> <span class="pl-en">MyTimers</span> <span class="pl-k">extends</span> <span class="pl-e">Metrics</span> {
  <span class="pl-k">val</span> <span class="pl-en">DriverFunctionTopLevel</span> <span class="pl-k">=</span> timer(<span class="pl-s"><span class="pl-pds">"</span>Driver Function Top Level<span class="pl-pds">"</span></span>)
  <span class="pl-k">val</span> <span class="pl-en">DriverFunctionNested</span> <span class="pl-k">=</span> timer(<span class="pl-s"><span class="pl-pds">"</span>Driver Function Nested<span class="pl-pds">"</span></span>)
  <span class="pl-k">val</span> <span class="pl-en">WorkerFunctionTopLevel</span> <span class="pl-k">=</span> timer(<span class="pl-s"><span class="pl-pds">"</span>Worker Function Top Level<span class="pl-pds">"</span></span>)
  <span class="pl-k">val</span> <span class="pl-en">WorkerFunctionNested</span> <span class="pl-k">=</span> timer(<span class="pl-s"><span class="pl-pds">"</span>Worker Function Nested<span class="pl-pds">"</span></span>)
}

<span class="pl-k">import</span> <span class="pl-v">MyTimers.</span><span class="pl-v">_</span>

<span class="pl-en">DriverFunctionTopLevel</span>.time {
  <span class="pl-en">DriverFunctionNested</span>.time {
    <span class="pl-k">val</span> <span class="pl-en">array</span> <span class="pl-k">=</span> instrumentedRDD.map(e <span class="pl-k">=&gt;</span> {
      <span class="pl-en">WorkerFunctionTopLevel</span>.time {
        <span class="pl-en">WorkerFunctionNested</span>.time {
          e<span class="pl-k">+</span><span class="pl-c1">1</span>
        }
      }
    }).collect()
  }
}

<span class="pl-k">val</span> <span class="pl-en">writer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">PrintWriter</span>(<span class="pl-k">new</span> <span class="pl-en">OutputStreamWriter</span>(<span class="pl-en">System</span>.out, <span class="pl-s"><span class="pl-pds">"</span>UTF-8<span class="pl-pds">"</span></span>))
<span class="pl-en">Metrics</span>.print(writer, <span class="pl-en">Some</span>(metricsListener.metrics.sparkMetrics.stageTimes))
writer.close()</pre>
  </div> 
  <p>This will result in output like the following:</p> 
  <pre><code>Timings
+-----------------------------------------------+--------------+--------------+-------------+-------+-----------+-----------+-----------+
|                    Metric                     | Worker Total | Driver Total | Driver Only | Count |   Mean    |    Min    |    Max    |
+-----------------------------------------------+--------------+--------------+-------------+-------+-----------+-----------+-----------+
| └─ Driver Function Top Level                  |            - |    604.27 ms |   168.11 ms |     1 | 604.27 ms | 604.27 ms | 604.27 ms |
|     └─ Driver Function Nested                 |            - |    497.71 ms |    61.55 ms |     1 | 497.71 ms | 497.71 ms | 497.71 ms |
|         ├─ map at NestedTester.scala:40       |            - |     75.39 ms |           - |     1 |  75.39 ms |  75.39 ms |  75.39 ms |
|         │   └─ function call                  |    173.29 ms |            - |           - |     5 |  34.66 ms |  30.84 ms |  35.71 ms |
|         │       └─ Worker Function Top Level  |     166.5 ms |            - |           - |     5 |   33.3 ms |  30.79 ms |  33.99 ms |
|         │           └─ Worker Function Nested |    103.54 ms |            - |           - |     5 |  20.71 ms |  20.21 ms |  20.83 ms |
|         └─ collect at NestedTester.scala:48   |            - |    360.78 ms |           - |     1 | 360.78 ms | 360.78 ms | 360.78 ms |
+-----------------------------------------------+--------------+--------------+-------------+-------+-----------+-----------+-----------+
</code></pre> 
  <p>We can see that a tree structure representing the nested function calls, along with their timings, is displayed.</p> 
  <p>Note that the "Worker Total" column is populated only for function calls within RDD operations, and the "Driver Total" column is populated for RDD operations themselves, and any operations outside them. The "Driver Only" column is populated just for function calls outside RDD operations. See the description of the "Timings" table above for further details on these columns.</p> 
  <h3><a id="user-content-instrumenting-file-saving-operations" class="anchor" href="https://github.com/bigdatagenomics/utils#instrumenting-file-saving-operations" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Instrumenting File Saving Operations</h3> 
  <p>It is possible to instrument file-saving operations in Spark by using a custom Hadoop <code>OutputFormat</code>. For example, consider the following code:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.spark.rdd.</span><span class="pl-v">InstrumentedOutputFormat</span>
<span class="pl-k">class</span> <span class="pl-en">InstrumentedAvroParquetOutputFormat</span> <span class="pl-k">extends</span> <span class="pl-e">InstrumentedOutputFormat</span>[<span class="pl-en">Void</span>, <span class="pl-en">IndexedRecord</span>] {
  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">outputFormatClass</span>()<span class="pl-k">:</span> <span class="pl-en">Class</span>[_ <span class="pl-k">&lt;</span><span class="pl-k">:</span> <span class="pl-en">NewOutputFormat</span>[<span class="pl-en">Void</span>, <span class="pl-en">IndexedRecord</span>]] <span class="pl-k">=</span> <span class="pl-c1">classOf</span>[<span class="pl-en">AvroParquetOutputFormat</span>]
  <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">timerName</span>()<span class="pl-k">:</span> <span class="pl-k">String</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Write Avro Record<span class="pl-pds">"</span></span>
}</pre>
  </div> 
  <p>This class extends the <code>InstrumentedOutputFormat</code> class to add instrumentation around the <code>AvroParquetOutputFormat</code> class. Every record written will be instrumented with the timer name returned from the <code>timerName</code> method.</p> 
  <p>After extending <code>InstrumentedOutputFormat</code> the regular <code>saveAs*HadoopFile</code> methods can be used on an instrumented RDD:</p> 
  <div class="highlight highlight-source-scala">
   <pre>instrumentedRDD.saveAsNewAPIHadoopFile(filePath,
  <span class="pl-c1">classOf</span>[java.lang.<span class="pl-en">Void</span>], manifest[<span class="pl-en">T</span>].runtimeClass.<span class="pl-c1">asInstanceOf</span>[<span class="pl-en">Class</span>[<span class="pl-en">T</span>]], <span class="pl-c1">classOf</span>[<span class="pl-en">InstrumentedAvroParquetOutputFormat</span>],
  <span class="pl-en">ContextUtil</span>.getConfiguration(job))</pre>
  </div> 
  <p>Note that the RDD must have be instrumented (see "Basic Usage" above).</p> 
  <p>Unfortunately it is not currently possible to instrument file reading, only writing.</p> 
  <h3><a id="user-content-additional-spark-statistics" class="anchor" href="https://github.com/bigdatagenomics/utils#additional-spark-statistics" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Additional Spark Statistics</h3> 
  <p>It is possible to get additional metrics about Spark tasks. For example, consider the following code:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.bdgenomics.utils.instrumentation.</span>{<span class="pl-v">RecordedMetrics</span>, <span class="pl-v">MetricsListener</span>}
<span class="pl-k">val</span> <span class="pl-en">metricsListener</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">MetricsListener</span>(<span class="pl-k">new</span> <span class="pl-en">RecordedMetrics</span>())
sparkContext.addSparkListener(metricsListener)

<span class="pl-k">val</span> <span class="pl-en">array</span> <span class="pl-k">=</span> instrumentedRDD.map(_<span class="pl-k">+</span><span class="pl-c1">1</span>).keyBy(_<span class="pl-k">%</span><span class="pl-c1">2</span>).groupByKey().collect()
<span class="pl-k">val</span> <span class="pl-en">writer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">PrintWriter</span>(<span class="pl-k">new</span> <span class="pl-en">OutputStreamWriter</span>(<span class="pl-en">System</span>.out, <span class="pl-s"><span class="pl-pds">"</span>UTF-8<span class="pl-pds">"</span></span>))
metricsListener.metrics.sparkMetrics.print(writer)
writer.close()</pre>
  </div> 
  <p>This will result in output similar to this:</p> 
  <pre><code>Task Timings
+-------------------------------+------------+-------+--------+-------+-------+
|            Metric             | Total Time | Count |  Mean  |  Min  |  Max  |
+-------------------------------+------------+-------+--------+-------+-------+
| Task Duration                 |     128 ms |     2 |  64 ms | 46 ms | 82 ms |
| Executor Run Time             |      86 ms |     2 |  43 ms | 42 ms | 44 ms |
| Executor Deserialization Time |      15 ms |     2 | 7.5 ms |  1 ms | 14 ms |
| Result Serialization Time     |       2 ms |     2 |   1 ms |     0 |  2 ms |
+-------------------------------+------------+-------+--------+-------+-------+

Task Timings By Host
+-------------------------------+-----------+------------+-------+--------+-------+-------+
|            Metric             |   Host    | Total Time | Count |  Mean  |  Min  |  Max  |
+-------------------------------+-----------+------------+-------+--------+-------+-------+
| Task Duration                 | localhost |     128 ms |     2 |  64 ms | 46 ms | 82 ms |
| Executor Run Time             | localhost |      86 ms |     2 |  43 ms | 42 ms | 44 ms |
| Executor Deserialization Time | localhost |      15 ms |     2 | 7.5 ms |  1 ms | 14 ms |
| Result Serialization Time     | localhost |       2 ms |     2 |   1 ms |     0 |  2 ms |
+-------------------------------+-----------+------------+-------+--------+-------+-------+

Task Timings By Stage
+-------------------------------+----------------------------+------------+-------+-------+-------+-------+
|            Metric             |      Stage ID &amp; Name       | Total Time | Count | Mean  |  Min  |  Max  |
+-------------------------------+----------------------------+------------+-------+-------+-------+-------+
| Task Duration                 | 1: keyBy at Ins.scala:30   |      82 ms |     1 | 82 ms | 82 ms | 82 ms |
| Task Duration                 | 0: collect at Ins.scala:30 |      46 ms |     1 | 46 ms | 46 ms | 46 ms |
| Executor Run Time             | 1: keyBy at Ins.scala:30   |      44 ms |     1 | 44 ms | 44 ms | 44 ms |
| Executor Run Time             | 0: collect at Ins.scala:30 |      42 ms |     1 | 42 ms | 42 ms | 42 ms |
| Executor Deserialization Time | 1: keyBy at Ins.scala:30   |      14 ms |     1 | 14 ms | 14 ms | 14 ms |
| Executor Deserialization Time | 0: collect at Ins.scala:30 |       1 ms |     1 |  1 ms |  1 ms |  1 ms |
| Result Serialization Time     | 0: collect at Ins.scala:30 |       2 ms |     1 |  2 ms |  2 ms |  2 ms |
| Result Serialization Time     | 1: keyBy at Ins.scala:30   |          0 |     1 |     0 |     0 |     0 |
+-------------------------------+----------------------------+------------+-------+-------+-------+-------+
</code></pre> 
  <p>The tables contain times for various parts of executing a Spark task, as well as the same timings broken down by host and Spark Stage.</p> 
  <h3><a id="user-content-performance" class="anchor" href="https://github.com/bigdatagenomics/utils#performance" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Performance</h3> 
  <p>The overhead of instrumenting a function call has been measured at around 120 nanoseconds on an Intel i7-3720QM processor. The overhead of calling the instrumentation code when no metrics are being recorded (the <code>Metrics.initialize</code> method has not be called) is negligible.</p> 
  <h3><a id="user-content-lifecycle-and-threading" class="anchor" href="https://github.com/bigdatagenomics/utils#lifecycle-and-threading" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Lifecycle and Threading</h3> 
  <p>Calling the <code>Metrics.initialize</code> method turns on metrics collection only for the <em>calling thread</em>. Therefore, if the Driver application is multi-threaded it is necessary to make this call in every thread that requires instrumentation.</p> 
  <p>Calling the <code>Metrics.initialize</code> method resets any previously-recorded Metrics, so it is strongly advised to call this at the start of each Spark job, otherwise metrics can "leak" between jobs.</p> 
  <p>If an application does not want to record metrics, it can simply avoid calling the <code>Metrics.initialize</code> method. This is useful for applications that want to avoid recording metrics in certain situations; it is not necessary to modify any code, just avoid calling the <code>initialize</code> method. Attempting to record metrics when the <code>initialize</code> method has not been called will not produce an error, and incurs negligible overhead. However, attempting to call the <code>Metrics.print</code> method will produce an error in this case.</p> 
  <p>If the application has previously turned on metrics collection, it can be turned off for a particular thread by calling the <code>Metrics.stopRecording</code> method. Calling <code>uninstrument</code> on an RDD is not enough to stop metrics collection, since metrics will still be collected in the Spark Driver. It is always necessary to call the <code>Metrics.stopRecording</code> method as well.</p> 
  <h1><a id="user-content-getting-in-touch" class="anchor" href="https://github.com/bigdatagenomics/utils#getting-in-touch" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Getting In Touch</h1> 
  <h2><a id="user-content-mailing-list" class="anchor" href="https://github.com/bigdatagenomics/utils#mailing-list" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Mailing List</h2> 
  <p>This project is maintained by the same developers as the <a href="https://www.github.com/bigdatagenomics/adam" target="_blank">ADAM project</a>. As such, <a href="https://groups.google.com/forum/#!forum/adam-developers" target="_blank">the ADAM mailing list</a> is a good way to sync up with other people who use the bdg-utils code, including the core developers. You can subscribe by sending an email to <code>adam-developers+subscribe@googlegroups.com</code> or just post using the <a href="https://groups.google.com/forum/#!forum/adam-developers" target="_blank">web forum page</a>.</p> 
  <h2><a id="user-content-irc-channel" class="anchor" href="https://github.com/bigdatagenomics/utils#irc-channel" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>IRC Channel</h2> 
  <p>A lot of the developers are hanging on the <a href="http://webchat.freenode.net/?channels=adamdev" target="_blank">#adamdev</a> freenode.net channel. Come join us and ask questions.</p> 
  <h1><a id="user-content-license" class="anchor" href="https://github.com/bigdatagenomics/utils#license" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>License</h1> 
  <p>bdg-utils is released under an <a href="https://github.com/bigdatagenomics/utils/blob/master/LICENSE.txt" target="_blank">Apache 2.0 license</a>.</p> 
 </article>
</div>