<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-spark-database-connector" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#spark-database-connector" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark Database Connector</h1> 
  <h2><a id="user-content-new-feature" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#new-feature" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>New Feature</h2> 
  <ul> 
   <li>List写入HBase支持Kerberos认证</li> 
   <li>升级HBase Client API为1.2.0版本</li> 
  </ul> 
  <p>隐藏处理各种数据库的连接细节，使用Scala API在Spark中简易地处理数据库连接的读写操作。</p> 
  <p>相关测试环境信息:</p> 
  <ul> 
   <li>Scala 2.11.8/2.10.5</li> 
   <li>Spark 1.6.0</li> 
   <li>HBase 0.98.4</li> 
   <li>Jdbc Driver 5.1.35</li> 
  </ul> 
  <p>目前支持的有:</p> 
  <ul> 
   <li>HBase</li> 
   <li>MySQL</li> 
  </ul> 
  <p>添加Maven引用:</p> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;<span class="pl-ent">dependency</span>&gt;
    &lt;<span class="pl-ent">groupId</span>&gt;info.xiaohei.www&lt;/<span class="pl-ent">groupId</span>&gt;
    &lt;<span class="pl-ent">artifactId</span>&gt;spark-database-connector_2.11&lt;/<span class="pl-ent">artifactId</span>&gt;
    &lt;<span class="pl-ent">version</span>&gt;1.0.0&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre>
  </div> 
  <p>Scala 2.10版本使用:</p> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;<span class="pl-ent">dependency</span>&gt;
    &lt;<span class="pl-ent">groupId</span>&gt;info.xiaohei.www&lt;/<span class="pl-ent">groupId</span>&gt;
    &lt;<span class="pl-ent">artifactId</span>&gt;spark-database-connector_2.10&lt;/<span class="pl-ent">artifactId</span>&gt;
    &lt;<span class="pl-ent">version</span>&gt;1.0.0&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre>
  </div> 
  <h2><a id="user-content-hbase" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>HBase</h2> 
  <h3><a id="user-content-设置hbase-host" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#设置hbase-host" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>设置HBase host</h3> 
  <p>通过以下三种任意方式设置HBase host地址</p> 
  <p><strong>1、在spark-submit中设置命令：</strong></p> 
  <div class="highlight highlight-source-shell">
   <pre>spark-submit --conf spark.hbase.host=your-hbase-host</pre>
  </div> 
  <p><strong>2、在Scala代码中设置：</strong></p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">sparkConf</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SparkConf</span>()
sparkConf.set(<span class="pl-s"><span class="pl-pds">"</span>spark.hbase.host<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>your-hbase-host<span class="pl-pds">"</span></span>)
<span class="pl-k">val</span> <span class="pl-en">sc</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SparkContext</span>(sparkConf)</pre>
  </div> 
  <p><strong>3、在JVM参数中设置：</strong></p> 
  <div class="highlight highlight-source-shell">
   <pre>java -Dspark.hbase.host=your-hbase-host -jar ....</pre>
  </div> 
  <p><strong>设置hbase-site.xml文件读取路径(可选)</strong></p> 
  <p>如果有读取hbase-site.xml文件的需求时,可以通过设置下面的选项进行指定:</p> 
  <div class="highlight highlight-source-shell">
   <pre>spark.hbase.config=your-hbase-config-path</pre>
  </div> 
  <p>设置该选项的方式同上 注意:需要将hbase-site.xml文件添加到当前项目可识别的resource路径中,否则将无法读取,使用默认配置</p> 
  <h3><a id="user-content-向hbase写入数据" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#向hbase写入数据" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>向HBase写入数据</h3> 
  <p><strong>导入隐式转换：</strong></p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">info.xiaohei.spark.connector.hbase.</span><span class="pl-v">_</span></pre>
  </div> 
  <h4><a id="user-content-spark-rdd写入hbase" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#spark-rdd写入hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark RDD写入HBase</h4> 
  <p>任何Spark RDD对象都能直接操作写入HBase，例如：</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">rdd</span> <span class="pl-k">=</span> sc.parallelize(<span class="pl-c1">1</span> to <span class="pl-c1">100</span>)
            .map(i <span class="pl-k">=&gt;</span> (s<span class="pl-s"><span class="pl-pds">"</span>rowkey-${i.toString}<span class="pl-pds">"</span></span>, s<span class="pl-s"><span class="pl-pds">"</span>column1-${i.toString}<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>column2<span class="pl-pds">"</span></span>))</pre>
  </div> 
  <p>这个RDD包含了100个三元组类型的数据，写入HBase时，第一个元素为rowkey，剩下的元素依次为各个列的值：</p> 
  <div class="highlight highlight-source-scala">
   <pre>rdd.toHBase(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
      .insert(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>col2<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>columnFamily<span class="pl-pds">"</span></span>)
      .save()</pre>
  </div> 
  <p>(1)使用RDD的toHBase函数传入要写入的表名<br> (2)insert函数传入要插入的各个列名<br> (3)inColumnFamily函数传入这些列所在的列族名<br> (4)最后save函数将该RDD保存在HBase中</p> 
  <p>如果col2和col1的列族不一样，可以在insert传入列名时单独指定：</p> 
  <div class="highlight highlight-source-scala">
   <pre>rdd.toHBase(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
      .insert(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>otherColumnFamily:col2<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>defaultColumnFamily<span class="pl-pds">"</span></span>)
      .save()</pre>
  </div> 
  <p>列族名和列名之间要用冒号(:)隔开，其他列需要指定列名时使用的方式一致</p> 
  <h4><a id="user-content-scala集合序列写入hbase" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#scala集合序列写入hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Scala集合/序列写入HBase</h4> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">dataList</span>  <span class="pl-k">=</span> <span class="pl-en">Seq</span>[(<span class="pl-k">String</span>, <span class="pl-k">String</span>)](
      (<span class="pl-s"><span class="pl-pds">"</span>00001475304346643896037<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>kgJkm0euSbe<span class="pl-pds">"</span></span>),
      (<span class="pl-s"><span class="pl-pds">"</span>00001475376619355219953<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>kiaR40qzI8o<span class="pl-pds">"</span></span>),
      (<span class="pl-s"><span class="pl-pds">"</span>00001475458728618943637<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>kgCoW0hgzXO<span class="pl-pds">"</span></span>),
      (<span class="pl-s"><span class="pl-pds">"</span>00001475838363931738019<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>kqiHu0WNJC0<span class="pl-pds">"</span></span>)

    )

<span class="pl-c"><span class="pl-c">//</span>创建隐式变量</span>
<span class="pl-k">implicit</span> <span class="pl-k">val</span> <span class="pl-en">hbaseConf</span> <span class="pl-k">=</span> <span class="pl-en">HBaseConf</span>.createConf(<span class="pl-s"><span class="pl-pds">"</span>hbase-host<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">//</span>如果实在spark程序操作可以通过以下的方式</span>
<span class="pl-k">implicit</span> <span class="pl-k">val</span> <span class="pl-en">hbaseConf</span> <span class="pl-k">=</span> <span class="pl-en">HBaseConf</span>.createFromSpark(sc)

dataList.toHBase(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
	.insert(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>col2<span class="pl-pds">"</span></span>)
	.inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>columnFamily<span class="pl-pds">"</span></span>)
	.save()</pre>
  </div> 
  <p>使用方式和RDD写入HBase的操作类似，<strong>注意,隐式变量不能在spark的foreachPartition等算子中定义</strong></p> 
  <p>以上的方式将使用HTable的put list批量将集合中的数据一次全部put到HBase中，如果写入HBase时想使用缓存区的方式，需要另外添加几个参数：</p> 
  <div class="highlight highlight-source-scala">
   <pre>dataList.toHBase(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>
      <span class="pl-c"><span class="pl-c">//</span>该参数指定写入时的autoFlush为false</span>
      , <span class="pl-en">Some</span>(<span class="pl-c1">false</span>, <span class="pl-c1">false</span>)
      <span class="pl-c"><span class="pl-c">//</span>该参数指定写入缓冲区的大小</span>
      , <span class="pl-en">Some</span>(<span class="pl-c1">5</span> <span class="pl-k">*</span> <span class="pl-c1">1024</span> <span class="pl-k">*</span> <span class="pl-c1">1024</span>))
      .insert(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>col2<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>columnFamily<span class="pl-pds">"</span></span>)
      .save()</pre>
  </div> 
  <p>使用该方式时，集合中的每个数据都会被put一次，但是关闭了自动刷写，所以只有当缓冲区满了之后才会批量向HBase写入</p> 
  <h4><a id="user-content-写入时为rowkey添加salt前缀" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#写入时为rowkey添加salt前缀" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>写入时为Rowkey添加salt前缀</h4> 
  <div class="highlight highlight-source-scala">
   <pre>rdd.toHBase(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
      .insert(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>otherColumnFamily:col2<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>defaultColumnFamily<span class="pl-pds">"</span></span>)
      <span class="pl-c"><span class="pl-c">//</span>添加salt</span>
      .withSalt(saltArray)
      .save()</pre>
  </div> 
  <p>saltArray是一个字符串数组,简单的例如0-9的字符串表示,由使用者自己定义</p> 
  <p>使用withSalt函数之后,在写入HBase时会为rowkey添加一个saltArray中的随机串,<strong>注意:为了更好的支持HBase部分键扫描(rowkey左对齐),数组中的所有元素长度都应该相等</strong></p> 
  <p>取随机串的方式有两种:</p> 
  <ul> 
   <li>1.计算当前的rowkey的hashCode的16进制表示并对saltArray的长度取余数,得到saltArray中的一个随机串作为salt前缀添加到rowkey</li> 
   <li>2.使用随机数生成器获得不超过saltArray长度的数字作为下标取数组中的值</li> 
  </ul> 
  <p>当前使用的是第一种方式</p> 
  <h3><a id="user-content-读取hbase数据" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#读取hbase数据" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>读取HBase数据</h3> 
  <p><strong>导入隐式转换：</strong></p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">info.xiaohei.spark.connector.hbase.</span><span class="pl-v">_</span></pre>
  </div> 
  <p>读取HBase的数据操作需要通过sc来进行：</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">hbaseRdd</span> <span class="pl-k">=</span> sc.fromHBase[(<span class="pl-k">String</span>, <span class="pl-k">String</span>, <span class="pl-k">String</span>)](<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
      .select(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>col2<span class="pl-pds">"</span></span>)
      .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>columnFamily<span class="pl-pds">"</span></span>)
      .withStartRow(<span class="pl-s"><span class="pl-pds">"</span>startRow<span class="pl-pds">"</span></span>)
      .withEndRow(<span class="pl-s"><span class="pl-pds">"</span>endRow<span class="pl-pds">"</span></span>)
      <span class="pl-c"><span class="pl-c">//</span>当rowkey中有随机的salt前缀时,将salt数组传入即可自动解析</span>
      <span class="pl-c"><span class="pl-c">//</span>得到的rowkey将会是原始的,不带salt前缀的</span>
      .withSalt(saltArray)</pre>
  </div> 
  <p>(1)使用sc的fromHBase函数传入要读取数据的表名，该函数需要指定读取数据的类型信息<br> (2)select函数传入要读取的各个列名<br> (3)inColumnFamily函数传入这些列所在的列族名<br> (4)withStartRow和withEndRow将设置rowkey的扫描范围，可选操作 (5)之后就可以在hbaseRdd上执行Spark RDD的各种算子操作</p> 
  <p>上面的例子中，fromHBase的泛型类型为三元组，但是select中只读取了两列值，因此，该三元组中第一个元素将是rowkey的值，其他元素按照列的顺序依次类推</p> 
  <p>当你不需要读取rowkey的值时，只需要将fromHBase的泛型类型改为二元组</p> 
  <p>即读取的列数为n，泛型类型为n元组时，列名和元组中的各个元素相对应 读取的列数为n，泛型类型为n+1元组时，元组的第一个元素为rowkey</p> 
  <p>当各个列位于不同列族时，设置列族的方式同写入HBase一致</p> 
  <h3><a id="user-content-sql-on-hbase" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#sql-on-hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SQL On HBase</h3> 
  <p>借助SQLContext的DataFrame接口，在组件中可以轻易实现SQL On HBase的功能。</p> 
  <p>上例中的hbaseRdd是从HBase中读取出来的数据，在此RDD的基础上进行转换操作：</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span>创建org.apache.spark.sql.Row类型的RDD</span>
<span class="pl-k">val</span> <span class="pl-en">rowRdd</span> <span class="pl-k">=</span> hbaseRdd.map(r <span class="pl-k">=&gt;</span> <span class="pl-en">Row</span>(r._1, r._2, r._3))
<span class="pl-k">val</span> <span class="pl-en">sqlContext</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SQLContext</span>(sc)
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.createDataFrame(
      rowRdd,
      <span class="pl-en">StructType</span>(<span class="pl-en">Array</span>(<span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>col1<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>), <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>col2<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>), <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>col3<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>)))
    )
df.show()

df.registerTempTable(<span class="pl-s"><span class="pl-pds">"</span>mytable<span class="pl-pds">"</span></span>)
sqlContext.sql(<span class="pl-s"><span class="pl-pds">"</span>select col1 from mytable<span class="pl-pds">"</span></span>).show()</pre>
  </div> 
  <h3><a id="user-content-使用case-class查询读取hbase的数据" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#使用case-class查询读取hbase的数据" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>使用case class查询/读取HBase的数据</h3> 
  <p>使用内置的隐式转换可以处理基本数据类型和元组数据,当有使用case class的需求时,需要额外做一些准备工作</p> 
  <p>定义如下的case class:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">case</span> <span class="pl-k">class</span> <span class="pl-en">MyClass</span>(<span class="pl-v">name</span>: <span class="pl-k">String</span>, <span class="pl-v">age</span>: <span class="pl-k">Int</span>)</pre>
  </div> 
  <p>如果想达到以下的效果:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">classRdd</span> <span class="pl-k">=</span> sc.fromHBase[<span class="pl-en">MyClass</span>](<span class="pl-s"><span class="pl-pds">"</span>tableName<span class="pl-pds">"</span></span>)
    .select(<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>age<span class="pl-pds">"</span></span>)
    .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>info<span class="pl-pds">"</span></span>)

classRdd.map{
    c <span class="pl-k">=&gt;</span>
        (c.name,c.age)
}</pre>
  </div> 
  <p>或者以下的效果:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span>classRdd的类型为RDD[MyClass]</span>
classRdd.toHBase(<span class="pl-s"><span class="pl-pds">"</span>tableName<span class="pl-pds">"</span></span>)
    .insert(<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>age<span class="pl-pds">"</span></span>)
    .inColumnFamily(<span class="pl-s"><span class="pl-pds">"</span>info<span class="pl-pds">"</span></span>)
    .save()</pre>
  </div> 
  <p>需要另外实现能够解析自定义case class的隐式方法:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myReaderConversion</span><span class="pl-k">:</span> <span class="pl-en">DataReader</span>[<span class="pl-en">MyClass</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">CustomDataReader</span>[(<span class="pl-k">String</span>, <span class="pl-k">Int</span>), <span class="pl-en">MyClass</span>] {
    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">convert</span>(<span class="pl-v">data</span>: (<span class="pl-k">String</span>, <span class="pl-k">Int</span>))<span class="pl-k">:</span> <span class="pl-en">MyClass</span> <span class="pl-k">=</span> <span class="pl-en">MyClass</span>(data._1, data._2)
  }

<span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myWriterConversion</span><span class="pl-k">:</span> <span class="pl-en">DataWriter</span>[<span class="pl-en">MyClass</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">CustomDataWriter</span>[<span class="pl-en">MyClass</span>, (<span class="pl-k">String</span>, <span class="pl-k">Int</span>)] {
    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">convert</span>(<span class="pl-v">data</span>: <span class="pl-en">MyClass</span>)<span class="pl-k">:</span> (<span class="pl-k">String</span>, <span class="pl-k">Int</span>) <span class="pl-k">=</span> (data.name, data.age)
  }</pre>
  </div> 
  <p>该隐式方法返回一个DataReader/DataWriter 重写CustomDataReader/CustomDataWriter中的convert方法 将case class转换为一个元组或者将元组转化为case class即可</p> 
  <h3><a id="user-content-写入带有kerberos认证的hbase" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#写入带有kerberos认证的hbase" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>写入带有Kerberos认证的HBase</h3> 
  <p>除了上述过程中写HBase需要的配置外,还需要指定以下三个配置:</p> 
  <ul> 
   <li>spark.hbase.krb.principal:认证的principal用户名</li> 
   <li>spark.hbase.krb.keytab:keytab文件路径(各个节点都存在且路径保持一致)</li> 
   <li>spark.hbase.config:hbase-site.xml文件路径</li> 
  </ul> 
  <p>写入HBase时将会使用提供给的krb信息进行认证</p> 
  <p>TODO:RDD的读写接口目前还未实现Kerberos认证</p> 
  <h2><a id="user-content-mysql" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#mysql" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>MySQL</h2> 
  <p>除了可以将RDD/集合写入HBase之外，还可以在普通的程序中进行MySQL的相关操作</p> 
  <h3><a id="user-content-在conf中设置相关信息" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#在conf中设置相关信息" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>在conf中设置相关信息</h3> 
  <p><strong>1、Spark程序中操作</strong></p> 
  <p>在SparkConf中设置以下的信息：</p> 
  <div class="highlight highlight-source-scala">
   <pre>sparkConf
  .set(<span class="pl-s"><span class="pl-pds">"</span>spark.mysql.host<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>your-host<span class="pl-pds">"</span></span>)
  .set(<span class="pl-s"><span class="pl-pds">"</span>spark.mysql.username<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>your-username<span class="pl-pds">"</span></span>)
  .set(<span class="pl-s"><span class="pl-pds">"</span>spark.mysql.password<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>your-passwd<span class="pl-pds">"</span></span>)
  .set(<span class="pl-s"><span class="pl-pds">"</span>spark.mysql.port<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>db-port<span class="pl-pds">"</span></span>)
  .set(<span class="pl-s"><span class="pl-pds">"</span>spark.mysql.db<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>database-name<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span>创建MySqlConf的隐式变量</span>
<span class="pl-k">implicit</span> <span class="pl-k">val</span> <span class="pl-en">mysqlConf</span> <span class="pl-k">=</span> <span class="pl-en">MysqlConf</span>.createFromSpark(sc)</pre>
  </div> 
  <p>关于这个隐式变量的说明：在RDD的foreachPartition或者mapPartitions等操作时，因为涉及到序列化的问题，默认的对MySqlConf的隐式转化操作会出现异常问题，所以需要显示的声明一下这个变量，其他不涉及网络序列化传输的操作可以省略这步</p> 
  <p>HBase小节中的设置属性的方法在这里也适用</p> 
  <p><strong>2、普通程序中操作</strong></p> 
  <p>创建MysqlConf，并设置相关属性：</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span>创建MySqlConf的隐式变量</span>
<span class="pl-k">implicit</span> <span class="pl-k">val</span> <span class="pl-en">mysqlConf</span> <span class="pl-k">=</span> <span class="pl-en">MysqlConf</span>.createConf(
      <span class="pl-s"><span class="pl-pds">"</span>your-host<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span>username<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span>password<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span>port<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span>db-name<span class="pl-pds">"</span></span>
    )
</pre>
  </div> 
  <p>在普通程序中操作时一定要显示声明MysqlConf这个隐式变量</p> 
  <h3><a id="user-content-写入mysql" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#写入mysql" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>写入MySQL</h3> 
  <p>导入隐式转换：</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">info.xiaohei.spark.connector.mysql.</span><span class="pl-v">_</span></pre>
  </div> 
  <p>之后任何Iterable类型的数据都可以直接写入MySQL中：</p> 
  <div class="highlight highlight-source-scala">
   <pre>list.toMysql(<span class="pl-s"><span class="pl-pds">"</span>table-name<span class="pl-pds">"</span></span>)
  <span class="pl-c"><span class="pl-c">//</span>插入的列名</span>
  .insert(<span class="pl-s"><span class="pl-pds">"</span>columns<span class="pl-pds">"</span></span>)
  <span class="pl-c"><span class="pl-c">//</span>where条件，如age=1</span>
  .where(<span class="pl-s"><span class="pl-pds">"</span>where-conditions<span class="pl-pds">"</span></span>)
  .save()</pre>
  </div> 
  <h3><a id="user-content-在spark程序中从mysql读取数据" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#在spark程序中从mysql读取数据" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>在Spark程序中从MySQL读取数据</h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">res</span> <span class="pl-k">=</span> sc.fromMysql[(<span class="pl-k">Int</span>,<span class="pl-k">String</span>,<span class="pl-k">Int</span>)](<span class="pl-s"><span class="pl-pds">"</span>table-name<span class="pl-pds">"</span></span>)
  .select(<span class="pl-s"><span class="pl-pds">"</span>id<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>age<span class="pl-pds">"</span></span>)
  .where(<span class="pl-s"><span class="pl-pds">"</span>where-conditions<span class="pl-pds">"</span></span>)
  .get</pre>
  </div> 
  <h3><a id="user-content-在普通程序中从mysql读取数据" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#在普通程序中从mysql读取数据" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>在普通程序中从MySQL读取数据</h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span>普通程序读取关系型数据库入口</span>
<span class="pl-k">val</span> <span class="pl-en">dbEntry</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">RelationalDbEntry</span>

<span class="pl-k">val</span> <span class="pl-en">res</span> <span class="pl-k">=</span> dbEntry.fromMysql[(<span class="pl-k">Int</span>,<span class="pl-k">String</span>,<span class="pl-k">Int</span>)](<span class="pl-s"><span class="pl-pds">"</span>table-name<span class="pl-pds">"</span></span>)
  .select(<span class="pl-s"><span class="pl-pds">"</span>id<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>age<span class="pl-pds">"</span></span>)
  .where(<span class="pl-s"><span class="pl-pds">"</span>where-conditions<span class="pl-pds">"</span></span>)
  .get</pre>
  </div> 
  <p>创建数据库入口之后的操作和spark中的流程一致</p> 
  <h3><a id="user-content-case-class解析" class="anchor" href="https://github.com/chubbyjiang/spark_db_connector#case-class解析" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>case class解析</h3> 
  <p>如果需要使用自定义的case class解析/写入MySQL,例如:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">case</span> <span class="pl-k">class</span> <span class="pl-en">Model</span>(<span class="pl-v">id</span>: <span class="pl-k">Int</span>, <span class="pl-v">name</span>: <span class="pl-k">String</span>, <span class="pl-v">age</span>: <span class="pl-k">Int</span>)</pre>
  </div> 
  <p>基本流程和hbase小节中差不多,定义隐式转换:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myExecutorConversion</span><span class="pl-k">:</span> <span class="pl-en">DataExecutor</span>[<span class="pl-en">Model</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">CustomDataExecutor</span>[<span class="pl-en">Model</span>, (<span class="pl-k">Int</span>, <span class="pl-k">String</span>, <span class="pl-k">Int</span>)]() {
    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">convert</span>(<span class="pl-v">data</span>: <span class="pl-en">Model</span>)<span class="pl-k">:</span> (<span class="pl-k">Int</span>, <span class="pl-k">String</span>, <span class="pl-k">Int</span>) <span class="pl-k">=</span> (data.id, data.name, data.age)
}

<span class="pl-k">implicit</span> <span class="pl-k">def</span> <span class="pl-en">myMapperConversion</span><span class="pl-k">:</span> <span class="pl-en">DataMapper</span>[<span class="pl-en">Model</span>] <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">CustomDataMapper</span>[(<span class="pl-k">Int</span>, <span class="pl-k">String</span>, <span class="pl-k">Int</span>), <span class="pl-en">Model</span>]() {
    <span class="pl-k">override</span> <span class="pl-k">def</span> <span class="pl-en">convert</span>(<span class="pl-v">data</span>: (<span class="pl-k">Int</span>, <span class="pl-k">String</span>, <span class="pl-k">Int</span>))<span class="pl-k">:</span> <span class="pl-en">Model</span> <span class="pl-k">=</span> <span class="pl-en">Model</span>(data._1, data._2, data._3)
 }</pre>
  </div> 
  <p>之后可以直接使用:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">entry</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">RelationalDbEntry</span>
<span class="pl-k">val</span> <span class="pl-en">res</span> <span class="pl-k">=</span> entry.fromMysql[<span class="pl-en">Model</span>](<span class="pl-s"><span class="pl-pds">"</span>test<span class="pl-pds">"</span></span>)
  .select(<span class="pl-s"><span class="pl-pds">"</span>id<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>age<span class="pl-pds">"</span></span>)
  .get
res.foreach(x <span class="pl-k">=&gt;</span> println(s<span class="pl-s"><span class="pl-pds">"</span>id:${x.id},name:${x.name},age:${x.age}<span class="pl-pds">"</span></span>))</pre>
  </div> 
 </article>
</div>