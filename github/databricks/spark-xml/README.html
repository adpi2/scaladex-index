<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-xml-data-source-for-apache-spark" class="anchor" href="https://github.com/databricks/spark-xml#xml-data-source-for-apache-spark" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>XML Data Source for Apache Spark</h1> 
  <p><a href="https://travis-ci.org/databricks/spark-xml" target="_blank"><img src="https://camo.githubusercontent.com/0fa97f60e8a6428f20d4aabb8620bf4946eca7d9/68747470733a2f2f7472617669732d63692e6f72672f64617461627269636b732f737061726b2d786d6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/databricks/spark-xml.svg?branch=master" style="max-width:100%;"></a> <a href="http://codecov.io/github/databricks/spark-xml?branch=master" target="_blank"><img src="https://camo.githubusercontent.com/4bf54e5166b76eba7d6f2e96978b69e1c73856a3/687474703a2f2f636f6465636f762e696f2f6769746875622f64617461627269636b732f737061726b2d786d6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/databricks/spark-xml/coverage.svg?branch=master" style="max-width:100%;"></a></p> 
  <ul> 
   <li> <p>A library for parsing and querying XML data with Apache Spark, for Spark SQL and DataFrames. The structure and test tools are mostly copied from <a href="https://github.com/databricks/spark-csv" target="_blank">CSV Data Source for Spark</a>.</p> </li> 
   <li> <p>This package supports to process format-free XML files in a distributed way, unlike JSON datasource in Spark restricts in-line JSON format.</p> </li> 
  </ul> 
  <h2><a id="user-content-requirements" class="anchor" href="https://github.com/databricks/spark-xml#requirements" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Requirements</h2> 
  <p>This library requires Spark 2.0+ for 0.4.x.</p> 
  <p>For version that works with Spark 1.x, please check for <a href="https://github.com/databricks/spark-xml/tree/branch-0.3" target="_blank">branch-0.3</a>.</p> 
  <h2><a id="user-content-linking" class="anchor" href="https://github.com/databricks/spark-xml#linking" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Linking</h2> 
  <p>You can link against this library in your program at the following coordinates:</p> 
  <h3><a id="user-content-scala-210" class="anchor" href="https://github.com/databricks/spark-xml#scala-210" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Scala 2.10</h3> 
  <pre><code>groupId: com.databricks
artifactId: spark-xml_2.10
version: 0.4.1
</code></pre> 
  <h3><a id="user-content-scala-211" class="anchor" href="https://github.com/databricks/spark-xml#scala-211" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Scala 2.11</h3> 
  <pre><code>groupId: com.databricks
artifactId: spark-xml_2.11
version: 0.4.1
</code></pre> 
  <h2><a id="user-content-using-with-spark-shell" class="anchor" href="https://github.com/databricks/spark-xml#using-with-spark-shell" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Using with Spark shell</h2> 
  <p>This package can be added to Spark using the <code>--packages</code> command line option. For example, to include it when starting the spark shell:</p> 
  <h3><a id="user-content-spark-compiled-with-scala-210" class="anchor" href="https://github.com/databricks/spark-xml#spark-compiled-with-scala-210" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark compiled with Scala 2.10</h3> 
  <pre><code>$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.10:0.4.1
</code></pre> 
  <h3><a id="user-content-spark-compiled-with-scala-211" class="anchor" href="https://github.com/databricks/spark-xml#spark-compiled-with-scala-211" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Spark compiled with Scala 2.11</h3> 
  <pre><code>$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.11:0.4.1
</code></pre> 
  <h2><a id="user-content-features" class="anchor" href="https://github.com/databricks/spark-xml#features" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Features</h2> 
  <p>This package allows reading XML files in local or distributed filesystem as <a href="https://spark.apache.org/docs/1.6.0/sql-programming-guide.html" target="_blank">Spark DataFrames</a>. When reading files the API accepts several options:</p> 
  <ul> 
   <li><code>path</code>: Location of files. Similar to Spark can accept standard Hadoop globbing expressions.</li> 
   <li><code>rowTag</code>: The row tag of your xml files to treat as a row. For example, in this xml <code>&lt;books&gt; &lt;book&gt;&lt;book&gt; ...&lt;/books&gt;</code>, the appropriate value would be <code>book</code>. Default is <code>ROW</code>. At the moment, rows containing self closing xml tags are not supported.</li> 
   <li><code>samplingRatio</code>: Sampling ratio for inferring schema (0.0 ~ 1). Default is 1. Possible types are <code>StructType</code>, <code>ArrayType</code>, <code>StringType</code>, <code>LongType</code>, <code>DoubleType</code>, <code>BooleanType</code>, <code>TimestampType</code> and <code>NullType</code>, unless user provides a schema for this.</li> 
   <li><code>excludeAttribute</code> : Whether you want to exclude attributes in elements or not. Default is false.</li> 
   <li><code>treatEmptyValuesAsNulls</code> : (DEPRECATED: use <code>nullValue</code> set to <code>""</code>) Whether you want to treat whitespaces as a null value. Default is false</li> 
   <li><code>mode</code>: The mode for dealing with corrupt records during parsing. Default is <code>PERMISSIVE</code>. 
    <ul> 
     <li><code>PERMISSIVE</code> : sets other fields to <code>null</code> when it meets a corrupted record, and puts the malformed string into a new field configured by <code>columnNameOfCorruptRecord</code>. When a schema is set by user, it sets <code>null</code> for extra fields.</li> 
     <li><code>DROPMALFORMED</code> : ignores the whole corrupted records.</li> 
     <li><code>FAILFAST</code> : throws an exception when it meets corrupted records.</li> 
    </ul> </li> 
   <li><code>columnNameOfCorruptRecord</code>: The name of new field where malformed strings are stored. Default is <code>_corrupt_record</code>.</li> 
   <li><code>attributePrefix</code>: The prefix for attributes so that we can differentiate attributes and elements. This will be the prefix for field names. Default is <code>_</code>.</li> 
   <li><code>valueTag</code>: The tag used for the value when there are attributes in the element having no child. Default is <code>_VALUE</code>.</li> 
   <li><code>charset</code>: Defaults to 'UTF-8' but can be set to other valid charset names</li> 
   <li><code>ignoreSurroundingSpaces</code>: Defines whether or not surrounding whitespaces from values being read should be skipped. Default is false.</li> 
  </ul> 
  <p>When writing files the API accepts several options:</p> 
  <ul> 
   <li><code>path</code>: Location to write files.</li> 
   <li><code>rowTag</code>: The row tag of your xml files to treat as a row. For example, in this xml <code>&lt;books&gt; &lt;book&gt;&lt;book&gt; ...&lt;/books&gt;</code>, the appropriate value would be <code>book</code>. Default is <code>ROW</code>.</li> 
   <li><code>rootTag</code>: The root tag of your xml files to treat as the root. For example, in this xml <code>&lt;books&gt; &lt;book&gt;&lt;book&gt; ...&lt;/books&gt;</code>, the appropriate value would be <code>books</code>. Default is <code>ROWS</code>.</li> 
   <li><code>nullValue</code>: The value to write <code>null</code> value. Default is string <code>null</code>. When this is <code>null</code>, it does not write attributes and elements for fields.</li> 
   <li><code>attributePrefix</code>: The prefix for attributes so that we can differentiating attributes and elements. This will be the prefix for field names. Default is <code>_</code>.</li> 
   <li><code>valueTag</code>: The tag used for the value when there are attributes in the element having no child. Default is <code>_VALUE</code>.</li> 
   <li><code>compression</code>: compression codec to use when saving to file. Should be the fully qualified name of a class implementing <code>org.apache.hadoop.io.compress.CompressionCodec</code> or one of case-insensitive shorten names (<code>bzip2</code>, <code>gzip</code>, <code>lz4</code>, and <code>snappy</code>). Defaults to no compression when a codec is not specified.</li> 
  </ul> 
  <p>Currently it supports the shortened name usage. You can use just <code>xml</code> instead of <code>com.databricks.spark.xml</code> from Spark 1.5.0+</p> 
  <h2><a id="user-content-structure-conversion" class="anchor" href="https://github.com/databricks/spark-xml#structure-conversion" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Structure Conversion</h2> 
  <p>Due to the structure differences between <code>DataFrame</code> and XML, there are some conversion rules from XML data to <code>DataFrame</code> and from <code>DataFrame</code> to XML data. Note that handling attributes can be disabled with the option <code>excludeAttribute</code>.</p> 
  <h3><a id="user-content-conversion-from-xml-to-dataframe" class="anchor" href="https://github.com/databricks/spark-xml#conversion-from-xml-to-dataframe" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Conversion from XML to <code>DataFrame</code></h3> 
  <ul> 
   <li> <p><strong>Attributes</strong>: Attributes are converted as fields with the heading prefix, <code>attributePrefix</code>.</p> 
    <div class="highlight highlight-text-xml">
     <pre>...
&lt;<span class="pl-ent">one</span> <span class="pl-e">myOneAttrib</span>=<span class="pl-s"><span class="pl-pds">"</span>AAAA<span class="pl-pds">"</span></span>&gt;
    &lt;<span class="pl-ent">two</span>&gt;two&lt;/<span class="pl-ent">two</span>&gt;
    &lt;<span class="pl-ent">three</span>&gt;three&lt;/<span class="pl-ent">three</span>&gt;
&lt;/<span class="pl-ent">one</span>&gt;
...</pre>
    </div> <p>produces a schema below:</p> <pre><code>root
 |-- _myOneAttrib: string (nullable = true)
 |-- two: string (nullable = true)
 |-- three: string (nullable = true)
</code></pre> </li> 
   <li> <p><strong>Value in an element that has no child elements but attributes</strong>: The value is put in a separate field, <code>valueTag</code>.</p> 
    <div class="highlight highlight-text-xml">
     <pre>...
&lt;<span class="pl-ent">one</span>&gt;
    &lt;<span class="pl-ent">two</span> <span class="pl-e">myTwoAttrib</span>=<span class="pl-s"><span class="pl-pds">"</span>BBBBB<span class="pl-pds">"</span></span>&gt;two&lt;/<span class="pl-ent">two</span>&gt;
    &lt;<span class="pl-ent">three</span>&gt;three&lt;/<span class="pl-ent">three</span>&gt;
&lt;/<span class="pl-ent">one</span>&gt;
...</pre>
    </div> <p>produces a schema below:</p> <pre><code>root
 |-- two: struct (nullable = true)
 |    |-- _VALUE: string (nullable = true)
 |    |-- _myTwoAttrib: string (nullable = true)
 |-- three: string (nullable = true)
</code></pre> </li> 
  </ul> 
  <h3><a id="user-content-conversion-from-dataframe-to-xml" class="anchor" href="https://github.com/databricks/spark-xml#conversion-from-dataframe-to-xml" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Conversion from <code>DataFrame</code> to XML</h3> 
  <ul> 
   <li> <p><strong>Element as an array in an array</strong>: Writing a XML file from <code>DataFrame</code> having a field <code>ArrayType</code> with its element as <code>ArrayType</code> would have an additional nested field for the element. This would not happen in reading and writing XML data but writing a <code>DataFrame</code> read from other sources. Therefore, roundtrip in reading and writing XML files has the same structure but writing a <code>DataFrame</code> read from other sources is possible to have a different structure.</p> <p><code>DataFrame</code> with a schema below:</p> <pre><code> |-- a: array (nullable = true)
 |    |-- element: array (containsNull = true)
 |    |    |-- element: string (containsNull = true)
</code></pre> <p>with data below:</p> <pre><code>+------------------------------------+
|                                   a|
+------------------------------------+
|[WrappedArray(aa), WrappedArray(bb)]|
+------------------------------------+
</code></pre> <p>produces a XML file below:</p> 
    <div class="highlight highlight-text-xml">
     <pre>...
&lt;<span class="pl-ent">a</span>&gt;
    &lt;<span class="pl-ent">item</span>&gt;aa&lt;/<span class="pl-ent">item</span>&gt;
&lt;/<span class="pl-ent">a</span>&gt;
&lt;<span class="pl-ent">a</span>&gt;
    &lt;<span class="pl-ent">item</span>&gt;bb&lt;/<span class="pl-ent">item</span>&gt;
&lt;/<span class="pl-ent">a</span>&gt;
...</pre>
    </div> </li> 
  </ul> 
  <h2><a id="user-content-examples" class="anchor" href="https://github.com/databricks/spark-xml#examples" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Examples</h2> 
  <p>These examples use a XML file available for download <a href="https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml" target="_blank">here</a>:</p> 
  <pre><code>$ wget https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml
</code></pre> 
  <h3><a id="user-content-sql-api" class="anchor" href="https://github.com/databricks/spark-xml#sql-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SQL API</h3> 
  <p>XML data source for Spark can infer data types:</p> 
  <div class="highlight highlight-source-sql">
   <pre><span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">books</span>
USING <span class="pl-c1">com</span>.<span class="pl-c1">databricks</span>.<span class="pl-c1">spark</span>.<span class="pl-c1">xml</span>
OPTIONS (<span class="pl-k">path</span> <span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>, rowTag <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>You can also specify column names and types in DDL. In this case, we do not infer schema.</p> 
  <div class="highlight highlight-source-sql">
   <pre><span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">books</span> (author string, description string, genre string, _id string, price double, publish_date string, title string)
USING <span class="pl-c1">com</span>.<span class="pl-c1">databricks</span>.<span class="pl-c1">spark</span>.<span class="pl-c1">xml</span>
OPTIONS (<span class="pl-k">path</span> <span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>, rowTag <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a id="user-content-scala-api" class="anchor" href="https://github.com/databricks/spark-xml#scala-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Scala API</h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SQLContext</span>

<span class="pl-k">val</span> <span class="pl-en">sqlContext</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SQLContext</span>(sc)
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .load(<span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">selectedData</span> <span class="pl-k">=</span> df.select(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>)
selectedData.write
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rootTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>books<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .save(<span class="pl-s"><span class="pl-pds">"</span>newbooks.xml<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>You can manually specify the schema when reading data:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SQLContext</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.types.</span>{<span class="pl-v">StructType</span>, <span class="pl-v">StructField</span>, <span class="pl-v">StringType</span>, <span class="pl-v">DoubleType</span>};

<span class="pl-k">val</span> <span class="pl-en">sqlContext</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SQLContext</span>(sc)
<span class="pl-k">val</span> <span class="pl-en">customSchema</span> <span class="pl-k">=</span> <span class="pl-en">StructType</span>(<span class="pl-en">Array</span>(
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>description<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>genre<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span> ,nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>price<span class="pl-pds">"</span></span>, <span class="pl-en">DoubleType</span>, nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>publish_date<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>),
    <span class="pl-en">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>title<span class="pl-pds">"</span></span>, <span class="pl-en">StringType</span>, nullable <span class="pl-k">=</span> <span class="pl-c1">true</span>)))


<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> sqlContext.read
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .schema(customSchema)
    .load(<span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">selectedData</span> <span class="pl-k">=</span> df.select(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>)
selectedData.write
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rootTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>books<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .save(<span class="pl-s"><span class="pl-pds">"</span>newbooks.xml<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a id="user-content-java-api" class="anchor" href="https://github.com/databricks/spark-xml#java-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Java API</h3> 
  <div class="highlight highlight-source-java">
   <pre><span class="pl-k">import</span> <span class="pl-smi">org.apache.spark.sql.SQLContext</span>

<span class="pl-smi">SQLContext</span> sqlContext <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">SQLContext</span>(sc);
<span class="pl-smi">DataFrame</span> df <span class="pl-k">=</span> sqlContext<span class="pl-k">.</span>read()
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .load(<span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>);

df<span class="pl-k">.</span>select(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>)<span class="pl-k">.</span>write()
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rootTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>books<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .save(<span class="pl-s"><span class="pl-pds">"</span>newbooks.xml<span class="pl-pds">"</span></span>);</pre>
  </div> 
  <p>You can manually specify schema:</p> 
  <div class="highlight highlight-source-java">
   <pre><span class="pl-k">import</span> <span class="pl-smi">org.apache.spark.sql.SQLContext</span>;
<span class="pl-k">import</span> <span class="pl-smi">org.apache.spark.sql.types.*</span>;

<span class="pl-smi">SQLContext</span> sqlContext <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">SQLContext</span>(sc);
<span class="pl-smi">StructType</span> customSchema <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">StructType</span>(<span class="pl-k">new</span> <span class="pl-smi">StructField</span>[] {
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">StringType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty()),
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">StringType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty()),
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>description<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">StringType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty()),
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>genre<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">StringType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty()),
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>price<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">DoubleType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty()),
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>publish_date<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">StringType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty()),
    <span class="pl-k">new</span> <span class="pl-smi">StructField</span>(<span class="pl-s"><span class="pl-pds">"</span>title<span class="pl-pds">"</span></span>, <span class="pl-smi">DataTypes</span><span class="pl-k">.</span><span class="pl-smi">StringType</span>, <span class="pl-c1">true</span>, <span class="pl-smi">Metadata</span><span class="pl-k">.</span>empty())
});

<span class="pl-smi">DataFrame</span> df <span class="pl-k">=</span> sqlContext<span class="pl-k">.</span>read()
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .schema(customSchema)
    .load(<span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>);

df<span class="pl-k">.</span>select(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>)<span class="pl-k">.</span>write()
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rootTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>books<span class="pl-pds">"</span></span>)
    .option(<span class="pl-s"><span class="pl-pds">"</span>rowTag<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
    .save(<span class="pl-s"><span class="pl-pds">"</span>newbooks.xml<span class="pl-pds">"</span></span>);</pre>
  </div> 
  <h3><a id="user-content-python-api" class="anchor" href="https://github.com/databricks/spark-xml#python-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Python API</h3> 
  <div class="highlight highlight-source-python">
   <pre><span class="pl-k">from</span> pyspark.sql <span class="pl-k">import</span> SQLContext
sqlContext <span class="pl-k">=</span> SQLContext(sc)

df <span class="pl-k">=</span> sqlContext.read.format(<span class="pl-s"><span class="pl-pds">'</span>com.databricks.spark.xml<span class="pl-pds">'</span></span>).options(<span class="pl-v">rowTag</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>book<span class="pl-pds">'</span></span>).load(<span class="pl-s"><span class="pl-pds">'</span>books.xml<span class="pl-pds">'</span></span>)
df.select(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>).write \
    .format(<span class="pl-s"><span class="pl-pds">'</span>com.databricks.spark.xml<span class="pl-pds">'</span></span>) \
    .options(<span class="pl-v">rowTag</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>book<span class="pl-pds">'</span></span>, <span class="pl-v">rootTag</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>books<span class="pl-pds">'</span></span>) \
    .save(<span class="pl-s"><span class="pl-pds">'</span>newbooks.xml<span class="pl-pds">'</span></span>)</pre>
  </div> 
  <p>You can manually specify schema:</p> 
  <div class="highlight highlight-source-python">
   <pre><span class="pl-k">from</span> pyspark.sql <span class="pl-k">import</span> SQLContext
<span class="pl-k">from</span> pyspark.sql.types <span class="pl-k">import</span> <span class="pl-k">*</span>

sqlContext <span class="pl-k">=</span> SQLContext(sc)
customSchema <span class="pl-k">=</span> StructType([ \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>, StringType(), <span class="pl-c1">True</span>), \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, StringType(), <span class="pl-c1">True</span>), \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>description<span class="pl-pds">"</span></span>, StringType(), <span class="pl-c1">True</span>), \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>genre<span class="pl-pds">"</span></span>, StringType(), <span class="pl-c1">True</span>), \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>price<span class="pl-pds">"</span></span>, DoubleType(), <span class="pl-c1">True</span>), \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>publish_date<span class="pl-pds">"</span></span>, StringType(), <span class="pl-c1">True</span>), \
    StructField(<span class="pl-s"><span class="pl-pds">"</span>title<span class="pl-pds">"</span></span>, StringType(), <span class="pl-c1">True</span>)])

df <span class="pl-k">=</span> sqlContext.read \
    .format(<span class="pl-s"><span class="pl-pds">'</span>com.databricks.spark.xml<span class="pl-pds">'</span></span>) \
    .options(<span class="pl-v">rowTag</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>book<span class="pl-pds">'</span></span>) \
    .load(<span class="pl-s"><span class="pl-pds">'</span>books.xml<span class="pl-pds">'</span></span>, <span class="pl-v">schema</span> <span class="pl-k">=</span> customSchema)

df.select(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>).write \
    .format(<span class="pl-s"><span class="pl-pds">'</span>com.databricks.spark.xml<span class="pl-pds">'</span></span>) \
    .options(<span class="pl-v">rowTag</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>book<span class="pl-pds">'</span></span>, <span class="pl-v">rootTag</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>books<span class="pl-pds">'</span></span>) \
    .save(<span class="pl-s"><span class="pl-pds">'</span>newbooks.xml<span class="pl-pds">'</span></span>)</pre>
  </div> 
  <h3><a id="user-content-r-api" class="anchor" href="https://github.com/databricks/spark-xml#r-api" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>R API</h3> 
  <p>Automatically infer schema (data types)</p> 
  <div class="highlight highlight-source-r">
   <pre>library(<span class="pl-smi">SparkR</span>)

Sys.setenv(<span class="pl-s"><span class="pl-pds">'</span>SPARKR_SUBMIT_ARGS<span class="pl-pds">'</span></span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>"--packages" "com.databricks:spark-xml_2.10:0.4.1" "sparkr-shell"<span class="pl-pds">'</span></span>)
<span class="pl-smi">sqlContext</span> <span class="pl-k">&lt;-</span> sparkRSQL.init(<span class="pl-smi">sc</span>)

<span class="pl-smi">df</span> <span class="pl-k">&lt;-</span> read.df(<span class="pl-smi">sqlContext</span>, <span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>, <span class="pl-v">source</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>, <span class="pl-v">rowTag</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> In this case, `rootTag` is set to "ROWS" and `rowTag` is set to "ROW".</span>
write.df(<span class="pl-smi">df</span>, <span class="pl-s"><span class="pl-pds">"</span>newbooks.csv<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>overwrite<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>You can manually specify schema:</p> 
  <div class="highlight highlight-source-r">
   <pre>library(<span class="pl-smi">SparkR</span>)

Sys.setenv(<span class="pl-s"><span class="pl-pds">'</span>SPARKR_SUBMIT_ARGS<span class="pl-pds">'</span></span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>"--packages" "com.databricks:spark-csv_2.10:0.4.1" "sparkr-shell"<span class="pl-pds">'</span></span>)
<span class="pl-smi">sqlContext</span> <span class="pl-k">&lt;-</span> sparkRSQL.init(<span class="pl-smi">sc</span>)
<span class="pl-smi">customSchema</span> <span class="pl-k">&lt;-</span> structType(
    structField(<span class="pl-s"><span class="pl-pds">"</span>_id<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>),
    structField(<span class="pl-s"><span class="pl-pds">"</span>author<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>),
    structField(<span class="pl-s"><span class="pl-pds">"</span>description<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>),
    structField(<span class="pl-s"><span class="pl-pds">"</span>genre<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>),
    structField(<span class="pl-s"><span class="pl-pds">"</span>price<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>double<span class="pl-pds">"</span></span>),
    structField(<span class="pl-s"><span class="pl-pds">"</span>publish_date<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>),
    structField(<span class="pl-s"><span class="pl-pds">"</span>title<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>))

<span class="pl-smi">df</span> <span class="pl-k">&lt;-</span> read.df(<span class="pl-smi">sqlContext</span>, <span class="pl-s"><span class="pl-pds">"</span>books.xml<span class="pl-pds">"</span></span>, <span class="pl-v">source</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>, <span class="pl-v">rowTag</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> In this case, `rootTag` is set to "ROWS" and `rowTag` is set to "ROW".</span>
write.df(<span class="pl-smi">df</span>, <span class="pl-s"><span class="pl-pds">"</span>newbooks.csv<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.xml<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>overwrite<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h2><a id="user-content-hadoop-inputformat" class="anchor" href="https://github.com/databricks/spark-xml#hadoop-inputformat" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Hadoop InputFormat</h2> 
  <p>The library contains a Hadoop input format for reading XML files by a start tag and an end tag. This is similar with <a href="https://github.com/apache/mahout/blob/9d14053c80a1244bdf7157ab02748a492ae9868a/integration/src/main/java/org/apache/mahout/text/wikipedia/XmlInputFormat.java" target="_blank">XmlInputFormat.java</a> in <a href="http://mahout.apache.org" target="_blank">Mahout</a> but supports to read compressed files, different encodings and read elements including attributes, which you may make direct use of as follows:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.databricks.spark.xml.</span><span class="pl-v">XmlInputFormat</span>

<span class="pl-c"><span class="pl-c">//</span> This will detect the tags including attributes</span>
sc.hadoopConfiguration.set(<span class="pl-en">XmlInputFormat</span>.<span class="pl-en">START_TAG_KEY</span>, <span class="pl-s"><span class="pl-pds">"</span>&lt;book&gt;<span class="pl-pds">"</span></span>)
sc.hadoopConfiguration.set(<span class="pl-en">XmlInputFormat</span>.<span class="pl-en">END_TAG_KEY</span>, <span class="pl-s"><span class="pl-pds">"</span>&lt;/book&gt;<span class="pl-pds">"</span></span>)
sc.hadoopConfiguration.set(<span class="pl-en">XmlInputFormat</span>.<span class="pl-en">ENCODING_KEY</span>, <span class="pl-s"><span class="pl-pds">"</span>utf-8<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">records</span> <span class="pl-k">=</span> sc.newAPIHadoopFile(
  path,
  <span class="pl-c1">classOf</span>[<span class="pl-en">XmlInputFormat</span>],
  <span class="pl-c1">classOf</span>[<span class="pl-en">LongWritable</span>],
  <span class="pl-c1">classOf</span>[<span class="pl-en">Text</span>])</pre>
  </div> 
  <h2><a id="user-content-building-from-source" class="anchor" href="https://github.com/databricks/spark-xml#building-from-source" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Building From Source</h2> 
  <p>This library is built with <a href="http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html" target="_blank">SBT</a>, which is automatically downloaded by the included shell script. To build a JAR file simply run <code>sbt/sbt package</code> from the project root. The build configuration includes support for both Scala 2.10 and 2.11.</p> 
  <h2><a id="user-content-acknowledgements" class="anchor" href="https://github.com/databricks/spark-xml#acknowledgements" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Acknowledgements</h2> 
  <p>This project was initially created by <a href="https://github.com/HyukjinKwon" target="_blank">HyukjinKwon</a> and donated to <a href="https://databricks.com" target="_blank">Databricks</a>.</p> 
 </article>
</div>