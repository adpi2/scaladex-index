<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a href="https://github.com/databricks/spark-avro#avro-data-source-for-apache-spark" aria-hidden="true" class="anchor" id="user-content-avro-data-source-for-apache-spark" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Avro Data Source for Apache Spark</h1> 
  <p>A library for reading and writing Avro data from <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">Spark SQL</a>.</p> 
  <p><a href="https://travis-ci.org/databricks/spark-avro" target="_blank"><img src="https://camo.githubusercontent.com/e16b3d4d12238eacf7b164d688df36d7387c78af/68747470733a2f2f7472617669732d63692e6f72672f64617461627269636b732f737061726b2d6176726f2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/databricks/spark-avro.svg?branch=master" style="max-width:100%;"></a> <a href="http://codecov.io/github/databricks/spark-avro?branch=master" target="_blank"><img src="https://camo.githubusercontent.com/2234295164125041e345b4888382b60c560b0525/687474703a2f2f636f6465636f762e696f2f6769746875622f64617461627269636b732f737061726b2d6176726f2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/databricks/spark-avro/coverage.svg?branch=master" style="max-width:100%;"></a></p> 
  <h2><a href="https://github.com/databricks/spark-avro#requirements" aria-hidden="true" class="anchor" id="user-content-requirements" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Requirements</h2> 
  <p>This documentation is for version 3.2.0 of this library, which supports Spark 2.0+. For documentation on earlier versions of this library, see the links below.</p> 
  <p>This library has different versions for Spark 1.2, 1.3, 1.4 through 1.6, and 2.0+:</p> 
  <table> 
   <thead> 
    <tr> 
     <th>Spark Version</th> 
     <th>Compatible version of Avro Data Source for Spark</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td><code>1.2</code></td> 
     <td><code>0.2.0</code></td> 
    </tr> 
    <tr> 
     <td><code>1.3</code></td> 
     <td><a href="https://github.com/databricks/spark-avro/tree/v1.0.0" target="_blank"><code>1.0.0</code></a></td> 
    </tr> 
    <tr> 
     <td><code>1.4</code>-<code>1.6</code></td> 
     <td><a href="https://github.com/databricks/spark-avro/tree/v2.0.1" target="_blank"><code>2.0.1</code></a></td> 
    </tr> 
    <tr> 
     <td><code>2.0+</code></td> 
     <td><code>3.2.0</code> (this version)</td> 
    </tr>
   </tbody>
  </table> 
  <h2><a href="https://github.com/databricks/spark-avro#linking" aria-hidden="true" class="anchor" id="user-content-linking" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Linking</h2> 
  <p>This library is cross-published for Scala 2.11, so 2.11 users should replace 2.10 with 2.11 in the commands listed below.</p> 
  <p>You can link against this library in your program at the following coordinates:</p> 
  <p><strong>Using SBT:</strong></p> 
  <pre><code>libraryDependencies += "com.databricks" %% "spark-avro" % "3.2.0"
</code></pre> 
  <p><strong>Using Maven:</strong></p> 
  <div class="highlight highlight-text-xml">
   <pre>&lt;<span class="pl-ent">dependency</span>&gt;
    &lt;<span class="pl-ent">groupId</span>&gt;com.databricks&lt;/<span class="pl-ent">groupId</span>&gt;
    &lt;<span class="pl-ent">artifactId</span>&gt;spark-avro_2.10&lt;/<span class="pl-ent">artifactId</span>&gt;
    &lt;<span class="pl-ent">version</span>&gt;3.2.0&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre>
  </div> 
  <h3><a href="https://github.com/databricks/spark-avro#with-spark-shell-or-spark-submit" aria-hidden="true" class="anchor" id="user-content-with-spark-shell-or-spark-submit" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>With <code>spark-shell</code> or <code>spark-submit</code></h3> 
  <p>This library can also be added to Spark jobs launched through <code>spark-shell</code> or <code>spark-submit</code> by using the <code>--packages</code> command line option. For example, to include it when starting the spark shell:</p> 
  <pre><code>$ bin/spark-shell --packages com.databricks:spark-avro_2.11:3.2.0
</code></pre> 
  <p>Unlike using <code>--jars</code>, using <code>--packages</code> ensures that this library and its dependencies will be added to the classpath. The <code>--packages</code> argument can also be used with <code>bin/spark-submit</code>.</p> 
  <h2><a href="https://github.com/databricks/spark-avro#features" aria-hidden="true" class="anchor" id="user-content-features" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Features</h2> 
  <p>Avro Data Source for Spark supports reading and writing of Avro data from Spark SQL.</p> 
  <ul> 
   <li><strong>Automatic schema conversion:</strong> It supports most conversions between Spark SQL and Avro records, making Avro a first-class citizen in Spark.</li> 
   <li><strong>Partitioning:</strong> This library allows developers to easily read and write partitioned data witout any extra configuration. Just pass the columns you want to partition on, just like you would for Parquet.</li> 
   <li><strong>Compression:</strong> You can specify the type of compression to use when writing Avro out to disk. The supported types are <code>uncompressed</code>, <code>snappy</code>, and <code>deflate</code>. You can also specify the deflate level.</li> 
   <li><strong>Specifying record names:</strong> You can specify the record name and namespace to use by passing a map of parameters with <code>recordName</code> and <code>recordNamespace</code>.</li> 
  </ul> 
  <h2><a href="https://github.com/databricks/spark-avro#supported-types-for-avro---spark-sql-conversion" aria-hidden="true" class="anchor" id="user-content-supported-types-for-avro---spark-sql-conversion" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Supported types for Avro -&gt; Spark SQL conversion</h2> 
  <p>This library supports reading all Avro types. It uses the following mapping from Avro types to Spark SQL types:</p> 
  <table> 
   <thead> 
    <tr> 
     <th>Avro type</th> 
     <th>Spark SQL type</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>boolean</td> 
     <td>BooleanType</td> 
    </tr> 
    <tr> 
     <td>int</td> 
     <td>IntegerType</td> 
    </tr> 
    <tr> 
     <td>long</td> 
     <td>LongType</td> 
    </tr> 
    <tr> 
     <td>float</td> 
     <td>FloatType</td> 
    </tr> 
    <tr> 
     <td>double</td> 
     <td>DoubleType</td> 
    </tr> 
    <tr> 
     <td>bytes</td> 
     <td>BinaryType</td> 
    </tr> 
    <tr> 
     <td>string</td> 
     <td>StringType</td> 
    </tr> 
    <tr> 
     <td>record</td> 
     <td>StructType</td> 
    </tr> 
    <tr> 
     <td>enum</td> 
     <td>StringType</td> 
    </tr> 
    <tr> 
     <td>array</td> 
     <td>ArrayType</td> 
    </tr> 
    <tr> 
     <td>map</td> 
     <td>MapType</td> 
    </tr> 
    <tr> 
     <td>fixed</td> 
     <td>BinaryType</td> 
    </tr> 
    <tr> 
     <td>union</td> 
     <td>See below</td> 
    </tr>
   </tbody>
  </table> 
  <p>In addition to the types listed above, it supports reading <code>union</code> types. The following three types are considered basic <code>union</code> types:</p> 
  <ol> 
   <li><code>union(int, long)</code> will be mapped to <code>LongType</code>.</li> 
   <li><code>union(float, double)</code> will be mapped to <code>DoubleType</code>.</li> 
   <li><code>union(something, null)</code>, where <code>something</code> is any supported Avro type. This will be mapped to the same Spark SQL type as that of <code>something</code>, with <code>nullable</code> set to <code>true</code>.</li> 
  </ol> 
  <p>All other <code>union</code> types are considered complex. They will be mapped to <code>StructType</code> where field names are <code>member0</code>, <code>member1</code>, etc., in accordance with members of the <code>union</code>. This is consistent with the behavior when converting between Avro and Parquet.</p> 
  <p>At the moment, it ignores docs, aliases and other properties present in the Avro file.</p> 
  <h2><a href="https://github.com/databricks/spark-avro#supported-types-for-spark-sql---avro-conversion" aria-hidden="true" class="anchor" id="user-content-supported-types-for-spark-sql---avro-conversion" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Supported types for Spark SQL -&gt; Avro conversion</h2> 
  <p>This library supports writing of all Spark SQL types into Avro. For most types, the mapping from Spark types to Avro types is straightforward (e.g. IntegerType gets converted to int); however, there are a few special cases which are listed below:</p> 
  <table> 
   <thead> 
    <tr> 
     <th>Spark SQL type</th> 
     <th>Avro type</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>ByteType</td> 
     <td>int</td> 
    </tr> 
    <tr> 
     <td>ShortType</td> 
     <td>int</td> 
    </tr> 
    <tr> 
     <td>DecimalType</td> 
     <td>string</td> 
    </tr> 
    <tr> 
     <td>BinaryType</td> 
     <td>bytes</td> 
    </tr> 
    <tr> 
     <td>TimestampType</td> 
     <td>long</td> 
    </tr> 
    <tr> 
     <td>StructType</td> 
     <td>record</td> 
    </tr>
   </tbody>
  </table> 
  <h2><a href="https://github.com/databricks/spark-avro#examples" aria-hidden="true" class="anchor" id="user-content-examples" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Examples</h2> 
  <p>The recommended way to read or write Avro data from Spark SQL is by using Spark's DataFrame APIs, which are available in Scala, Java, Python, and R.</p> 
  <p>These examples use an Avro file available for download <a href="https://github.com/databricks/spark-avro/raw/master/src/test/resources/episodes.avro" target="_blank">here</a>:</p> 
  <h3><a href="https://github.com/databricks/spark-avro#scala-api" aria-hidden="true" class="anchor" id="user-content-scala-api" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Scala API</h3> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-c"><span class="pl-c">//</span> import needed for the .avro method to be added</span>
<span class="pl-k">import</span> <span class="pl-v">com.databricks.spark.avro.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>).getOrCreate()

<span class="pl-c"><span class="pl-c">//</span> The Avro records get converted to Spark types, filtered, and</span>
<span class="pl-c"><span class="pl-c">//</span> then written back out as Avro records</span>
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> spark.read.avro(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>)
df.filter(<span class="pl-s"><span class="pl-pds">"</span>doctor &gt; 5<span class="pl-pds">"</span></span>).write.avro(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>Alternatively you can specify the format to use instead:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>).getOrCreate()
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> spark.read
    .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>)
    .load(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>)

df.filter(<span class="pl-s"><span class="pl-pds">"</span>doctor &gt; 5<span class="pl-pds">"</span></span>).write.format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>).save(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>You can specify a custom Avro schema:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">org.apache.avro.</span><span class="pl-v">Schema</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">schema</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">Schema.Parser</span>().parse(<span class="pl-k">new</span> <span class="pl-en">File</span>(<span class="pl-s"><span class="pl-pds">"</span>user.avsc<span class="pl-pds">"</span></span>))
<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>).getOrCreate()
spark
  .read
  .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>)
  .option(<span class="pl-s"><span class="pl-pds">"</span>avroSchema<span class="pl-pds">"</span></span>, schema.toString)
  .load(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>).show()</pre>
  </div> 
  <p>You can also specify Avro compression options:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.databricks.spark.avro.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>).getOrCreate()

<span class="pl-c"><span class="pl-c">//</span> configuration to use deflate compression</span>
spark.conf.set(<span class="pl-s"><span class="pl-pds">"</span>spark.sql.avro.compression.codec<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>deflate<span class="pl-pds">"</span></span>)
spark.conf.set(<span class="pl-s"><span class="pl-pds">"</span>spark.sql.avro.deflate.level<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>5<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> spark.read.avro(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">//</span> writes out compressed Avro records</span>
df.write.avro(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>You can write partitioned Avro records like this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.databricks.spark.avro.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>).getOrCreate()

<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> spark.createDataFrame(
  <span class="pl-en">Seq</span>(
    (<span class="pl-c1">2012</span>, <span class="pl-c1">8</span>, <span class="pl-s"><span class="pl-pds">"</span>Batman<span class="pl-pds">"</span></span>, <span class="pl-c1">9.8</span>),
    (<span class="pl-c1">2012</span>, <span class="pl-c1">8</span>, <span class="pl-s"><span class="pl-pds">"</span>Hero<span class="pl-pds">"</span></span>, <span class="pl-c1">8.7</span>),
    (<span class="pl-c1">2012</span>, <span class="pl-c1">7</span>, <span class="pl-s"><span class="pl-pds">"</span>Robot<span class="pl-pds">"</span></span>, <span class="pl-c1">5.5</span>),
    (<span class="pl-c1">2011</span>, <span class="pl-c1">7</span>, <span class="pl-s"><span class="pl-pds">"</span>Git<span class="pl-pds">"</span></span>, <span class="pl-c1">2.0</span>))
  ).toDF(<span class="pl-s"><span class="pl-pds">"</span>year<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>month<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>title<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>rating<span class="pl-pds">"</span></span>)

df.toDF.write.partitionBy(<span class="pl-s"><span class="pl-pds">"</span>year<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>month<span class="pl-pds">"</span></span>).avro(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <p>You can specify the record name and namespace like this:</p> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.databricks.spark.avro.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">org.apache.spark.sql.</span><span class="pl-v">SparkSession</span>

<span class="pl-k">val</span> <span class="pl-en">spark</span> <span class="pl-k">=</span> <span class="pl-en">SparkSession</span>.builder().master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>).getOrCreate()
<span class="pl-k">val</span> <span class="pl-en">df</span> <span class="pl-k">=</span> spark.read.avro(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>)

<span class="pl-k">val</span> <span class="pl-en">name</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>AvroTest<span class="pl-pds">"</span></span>
<span class="pl-k">val</span> <span class="pl-en">namespace</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>
<span class="pl-k">val</span> <span class="pl-en">parameters</span> <span class="pl-k">=</span> <span class="pl-en">Map</span>(<span class="pl-s"><span class="pl-pds">"</span>recordName<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> name, <span class="pl-s"><span class="pl-pds">"</span>recordNamespace<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> namespace)

df.write.options(parameters).avro(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a href="https://github.com/databricks/spark-avro#java-api" aria-hidden="true" class="anchor" id="user-content-java-api" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Java API</h3> 
  <div class="highlight highlight-source-java">
   <pre><span class="pl-k">import</span> <span class="pl-smi">org.apache.spark.sql.*</span>;
<span class="pl-k">import</span> <span class="pl-smi">org.apache.spark.sql.functions</span>;

<span class="pl-smi">SparkSession</span> spark <span class="pl-k">=</span> <span class="pl-smi">SparkSession</span><span class="pl-k">.</span>builder()<span class="pl-k">.</span>master(<span class="pl-s"><span class="pl-pds">"</span>local<span class="pl-pds">"</span></span>)<span class="pl-k">.</span>getOrCreate();

<span class="pl-c"><span class="pl-c">//</span> Creates a DataFrame from a specified file</span>
<span class="pl-k">Dataset&lt;<span class="pl-smi">Row</span>&gt;</span> df <span class="pl-k">=</span> spark<span class="pl-k">.</span>read()<span class="pl-k">.</span>format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>)
  .load(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>);

<span class="pl-c"><span class="pl-c">//</span> Saves the subset of the Avro records read in</span>
df<span class="pl-k">.</span>filter(functions<span class="pl-k">.</span>expr(<span class="pl-s"><span class="pl-pds">"</span>doctor &gt; 5<span class="pl-pds">"</span></span>))<span class="pl-k">.</span>write()
  .format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>)
  .save(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>);</pre>
  </div> 
  <h3><a href="https://github.com/databricks/spark-avro#python-api" aria-hidden="true" class="anchor" id="user-content-python-api" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Python API</h3> 
  <div class="highlight highlight-source-python">
   <pre><span class="pl-c"><span class="pl-c">#</span> Creates a DataFrame from a specified directory</span>
df <span class="pl-k">=</span> spark.read.format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>).load(<span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span>  Saves the subset of the Avro records read in</span>
subset <span class="pl-k">=</span> df.where(<span class="pl-s"><span class="pl-pds">"</span>doctor &gt; 5<span class="pl-pds">"</span></span>)
subset.write.format(<span class="pl-s"><span class="pl-pds">"</span>com.databricks.spark.avro<span class="pl-pds">"</span></span>).save(<span class="pl-s"><span class="pl-pds">"</span>/tmp/output<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h3><a href="https://github.com/databricks/spark-avro#sql-api" aria-hidden="true" class="anchor" id="user-content-sql-api" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>SQL API</h3> 
  <p>Avro data can be queried in pure SQL by registering the data as a temporary table.</p> 
  <div class="highlight highlight-source-sql">
   <pre>CREATE TEMPORARY TABLE episodes
USING <span class="pl-c1">com</span>.<span class="pl-c1">databricks</span>.<span class="pl-c1">spark</span>.<span class="pl-c1">avro</span>
OPTIONS (<span class="pl-k">path</span> <span class="pl-s"><span class="pl-pds">"</span>src/test/resources/episodes.avro<span class="pl-pds">"</span></span>)</pre>
  </div> 
  <h2><a href="https://github.com/databricks/spark-avro#building-from-source" aria-hidden="true" class="anchor" id="user-content-building-from-source" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Building From Source</h2> 
  <p>This library is built with <a href="http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html" target="_blank">SBT</a>, which is automatically downloaded by the included shell script. To build a JAR file simply run <code>build/sbt package</code> from the project root.</p> 
  <h2><a href="https://github.com/databricks/spark-avro#testing" aria-hidden="true" class="anchor" id="user-content-testing" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Testing</h2> 
  <p>To run the tests, you should run <code>build/sbt test</code>. In case you are doing improvements that target speed, you can generate a sample Avro file and check how long it takes to read that Avro file using the following commands:</p> 
  <pre><code>build/sbt "test:run-main com.databricks.spark.avro.AvroFileGenerator NUMBER_OF_RECORDS NUMBER_OF_FILES"
</code></pre> 
  <p>will create sample avro files in <code>target/avroForBenchmark/</code>. You can specify the number of records for each file, as well as the overall number of files.</p> 
  <pre><code>build/sbt "test:run-main com.databricks.spark.avro.AvroReadBenchmark"
</code></pre> 
  <p>runs <code>count()</code> on the data inside <code>target/avroForBenchmark/</code> and tells you how the operation took.</p> 
  <p>Similarly, you can do benchmarks on how long it takes to write DataFrame as Avro file with</p> 
  <pre><code>build/sbt "test:run-main com.databricks.spark.avro.AvroWriteBenchmark NUMBER_OF_ROWS"
</code></pre> 
  <p>where <code>NUMBER_OF_ROWS</code> is an optional parameter that allows you to specify the number of rows in DataFrame that we will be writing.</p> 
 </article>
</div>