<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-bus-floating-data" class="anchor" href="https://github.com/anierbeck/busfloatingdata#bus-floating-data" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Bus Floating Data</h1> 
  <p>this project is a simple show case which shows how to create a streaming akka actor to ingest data from the http source to Kafka And consume those messages either via akka or spark to push them to a cassandra table. </p> 
  <p>The project is a multi project with cross compilation due to incompatabilities with kafka spark and akka when using the latest scala akka versions. </p> 
  <h3><a id="user-content-for-a-clean-build-from-commandline" class="anchor" href="https://github.com/anierbeck/busfloatingdata#for-a-clean-build-from-commandline" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>for a clean build from commandline:</h3> 
  <p><code>sbt ";clean ;test; publishLocal"</code></p> 
  <p>or simply call </p> 
  <p><code>sbt create</code></p> 
  <h3><a id="user-content-to-run-the-applications" class="anchor" href="https://github.com/anierbeck/busfloatingdata#to-run-the-applications" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>To run the applications</h3> 
  <p><strong>ingest:</strong> </p> 
  <p><code>sbt ingest/run</code></p> 
  <p>select the 1st entry as the simple does all steps at once or better </p> 
  <p><code>sbt runIngest</code></p> 
  <p>to prepare the ingest container call: </p> 
  <p><code>sbt createIngestContainer</code></p> 
  <p><strong>akka frontend:</strong> <code>so service/run</code> <code>sbt runService</code></p> 
  <p>to prepare the service container: </p> 
  <p><code>sbt createServerContainer</code></p> 
  <p><strong>spark digest:</strong> The spark digest starts a local spark master. As a spark job requires a fat jar first create that one: </p> 
  <p><code>sbt createDigestUberJar</code></p> 
  <p>to run the kafka-&gt;cassandra spark job: <code>sbt submitKafkaCassandra</code></p> 
  <p>to run the cluster spark job: <code>sbt submitClusterSpark</code></p> 
  <h2><a id="user-content-pre-conditions-to-run-those-have" class="anchor" href="https://github.com/anierbeck/busfloatingdata#pre-conditions-to-run-those-have" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Pre-conditions to run those, have</h2> 
  <p>a) a cassandra running b) a zookeeper running c) a kafka server running</p> 
  <p>for b) and c) you can use brew to install those. e.g. brew cask install zookeeper and brew install kafka</p> 
  <p>to run zookeper (not as a service)</p> 
  <p>zkServer start</p> 
  <p>to run kafka (not as a service)</p> 
  <p>kafka-server-start /usr/local/etc/kafka/server.properties</p> 
  <h3><a id="user-content-operating-kafka" class="anchor" href="https://github.com/anierbeck/busfloatingdata#operating-kafka" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Operating Kafka</h3> 
  <p>to clean old data from kafka turn down the retention time in kafka for the topic</p> 
  <p><code>kafka-configs --zookeeper localhost:2181 --entity-type topics --alter --add-config retention.ms=1000 --entity-name METRO-Vehicles</code></p> 
  <p>this will result in a clean up afte about a minute</p> 
  <p>after that you can configure back the time to about 2 days (more isn't worth when testing)</p> 
  <p><code>kafka-configs --zookeeper localhost:2181 --entity-type topics --alter --add-config retention.ms=172800000 --entity-name METRO-Vehicles</code></p> 
  <p>to check the current setting do the following: </p> 
  <p><code>kafka-configs --zookeeper localhost:2181 --entity-type topics --describe --entity-name METRO-Vehicles</code></p> 
  <h3><a id="user-content-preparations-for-running-on-aws" class="anchor" href="https://github.com/anierbeck/busfloatingdata#preparations-for-running-on-aws" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Preparations for running on AWS</h3> 
  <p>See also the terraform folder on how to run this on AWS. But prior to using it on AWS with a DC/OS cluster, make sure to create all required Docker images and deploy them to a place you have access to. To create those artefacts just call</p> 
  <p><code>sbt createAWS</code></p> 
  <p>this will create all uber-jars and runnable Docker images. </p> 
 </article>
</div>