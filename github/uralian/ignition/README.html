<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-ignition" class="anchor" href="https://github.com/uralian/ignition#ignition" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Ignition</h1> 
  <p><a href="https://camo.githubusercontent.com/7ceac9389d25a2d82295465f036cad71b10037af/68747470733a2f2f7472617669732d63692e6f72672f7572616c69616e2f69676e6974696f6e2e7376673f6272616e63683d6d6173746572" target="_blank"><img src="https://camo.githubusercontent.com/7ceac9389d25a2d82295465f036cad71b10037af/68747470733a2f2f7472617669732d63692e6f72672f7572616c69616e2f69676e6974696f6e2e7376673f6272616e63683d6d6173746572" alt="alt tag" data-canonical-src="https://travis-ci.org/uralian/ignition.svg?branch=master" style="max-width:100%;"></a> <a href="https://camo.githubusercontent.com/0685218728831eb3ab6c6d84ae7f19ba9d7f9328/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f7572616c69616e2f69676e6974696f6e2f62616467652e737667" target="_blank"><img src="https://camo.githubusercontent.com/0685218728831eb3ab6c6d84ae7f19ba9d7f9328/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f7572616c69616e2f69676e6974696f6e2f62616467652e737667" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/uralian/ignition/badge.svg" style="max-width:100%;"></a></p> 
  <p>Ignition is a tool for creating reusable workflows for Apache Spark. Inspired by <a href="http://community.pentaho.com/projects/data-integration/" target="_blank">Pentaho Kettle</a>, it is an attempt to provide data analysts with means to create Spark-based analytical workflows without the need to learn Scala, Java, or Python.</p> 
  <p>A workflow is a directed graph where data travels the edges connecting various input and transformation nodes called <code>steps</code>. Each step can have an arbitrary number of input and output ports that are used to connect it to other steps. Data traveling between steps is represented by Spark DataFrames, i.e. represents a regular grid with a fixed number of columns, each having a name and a type (similar to a relational DB table).</p> 
  <h2><a id="user-content-features" class="anchor" href="https://github.com/uralian/ignition#features" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Features</h2> 
  <ul> 
   <li>The followig data types are supported: 
    <ul> 
     <li><code>binary</code> (array of bytes)</li> 
     <li><code>boolean</code></li> 
     <li><code>string</code></li> 
     <li><code>byte</code></li> 
     <li><code>short</code></li> 
     <li><code>integer</code></li> 
     <li><code>long</code></li> 
     <li><code>float</code></li> 
     <li><code>double</code></li> 
     <li><code>decimal</code></li> 
     <li><code>date</code></li> 
     <li><code>timestamp</code></li> 
    </ul></li> 
   <li>The following pre-defined step types are available: 
    <ul> 
     <li>Producer (0 inputs, 1 output)</li> 
     <li>Transformer (1 input, 1 output)</li> 
     <li>Splitter (1 input, &gt;1 outputs)</li> 
     <li>Merger (&gt;1 inputs, 1 output)</li> 
     <li>Module (N inputs, M outputs) </li> 
     <li>SubFlow (N input, M outputs â€“ contains a a subgraph of steps)</li> 
    </ul></li> 
   <li>Supports named Spark broadcasts and accumulators</li> 
   <li>Uses native Spark map-reduce engine for data processing</li> 
   <li>Supports SQL queries through SparkSQL</li> 
   <li>Contains more than 30 steps out of the box</li> 
   <li>Provides an intuitive DSL for building and connecting the steps</li> 
   <li>Supports JSON and XML serialization of workflows (under development)</li> 
  </ul> 
  <h2><a id="user-content-download" class="anchor" href="https://github.com/uralian/ignition#download" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Download</h2> 
  <p>This project has been published to the Maven Central Repository. For SBT to download the connector binaries, sources and javadoc, put this in your project SBT config:</p> 
  <pre><code>libraryDependencies += "com.uralian" %% "ignition" % "0.2.0"
</code></pre> 
  <p>Ignition allows to run spark in embedded mode by using the special <code>local[*]</code> Spark master URL. For deployment on a spark cluster, refer to Apache Spark application <a href="https://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">deployment guide</a>.</p> 
  <h2><a id="user-content-5-minute-start-guide" class="anchor" href="https://github.com/uralian/ignition#5-minute-start-guide" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>5-minute start guide</h2> 
  <div class="highlight highlight-source-scala">
   <pre><span class="pl-k">import</span> <span class="pl-v">com.ignition.</span><span class="pl-v">SparkPlug</span>
<span class="pl-k">import</span> <span class="pl-v">com.ignition.frame.</span><span class="pl-v">_</span>
<span class="pl-k">import</span> <span class="pl-v">com.ignition.types.</span><span class="pl-v">_</span>

<span class="pl-k">import</span> <span class="pl-v">BasicAggregator.</span><span class="pl-v">_</span>

<span class="pl-k">val</span> <span class="pl-en">flow</span> <span class="pl-k">=</span> <span class="pl-en">DataFlow</span> {

  <span class="pl-c"><span class="pl-c">//</span> create an input data grid with 4 columns of the specified types and fill it with test data</span>
  <span class="pl-c"><span class="pl-c">//</span> in real life, one will probably use CassandraInput or TextFileInput etc.</span>
  <span class="pl-k">val</span> <span class="pl-en">grid1</span> <span class="pl-k">=</span> <span class="pl-en">DataGrid</span>(string(<span class="pl-s"><span class="pl-pds">"</span>id<span class="pl-pds">"</span></span>) <span class="pl-k">~</span> string(<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>) <span class="pl-k">~</span> int(<span class="pl-s"><span class="pl-pds">"</span>weight<span class="pl-pds">"</span></span>) <span class="pl-k">~</span> date(<span class="pl-s"><span class="pl-pds">"</span>dob<span class="pl-pds">"</span></span>)) rows (
    (<span class="pl-s"><span class="pl-pds">"</span>j1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>john<span class="pl-pds">"</span></span>, <span class="pl-c1">155</span>, date(<span class="pl-c1">1980</span>, <span class="pl-c1">5</span>, <span class="pl-c1">2</span>)),
    (<span class="pl-s"><span class="pl-pds">"</span>j2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>jane<span class="pl-pds">"</span></span>, <span class="pl-c1">190</span>, date(<span class="pl-c1">1982</span>, <span class="pl-c1">4</span>, <span class="pl-c1">25</span>)),
    (<span class="pl-s"><span class="pl-pds">"</span>j3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>jake<span class="pl-pds">"</span></span>, <span class="pl-c1">160</span>, date(<span class="pl-c1">1974</span>, <span class="pl-c1">11</span>, <span class="pl-c1">3</span>)),
    (<span class="pl-s"><span class="pl-pds">"</span>j4<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>josh<span class="pl-pds">"</span></span>, <span class="pl-c1">120</span>, date(<span class="pl-c1">1995</span>, <span class="pl-c1">1</span>, <span class="pl-c1">10</span>))
  )

  <span class="pl-c"><span class="pl-c">//</span> create another input data grid with 1 column and two rows.</span>
  <span class="pl-c"><span class="pl-c">//</span> when a row contains only one element, you can skip ()</span>
  <span class="pl-k">val</span> <span class="pl-en">grid2</span> <span class="pl-k">=</span> <span class="pl-en">DataGrid</span>(string(<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>)) rows (<span class="pl-s"><span class="pl-pds">"</span>jane<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>josh<span class="pl-pds">"</span></span>)

  <span class="pl-c"><span class="pl-c">//</span> our first pipeline will use SQL join to merge the input grids by 'name' field.</span>
  <span class="pl-c"><span class="pl-c">//</span> SQLQuery step is a merger, i.e. it can have any number of inputs and 1 output</span>
  <span class="pl-c"><span class="pl-c">//</span> in SQL, you can refer to the inputs using names input0, input1, etc.</span>
  <span class="pl-k">val</span> <span class="pl-en">queryA</span> <span class="pl-k">=</span> <span class="pl-en">SQLQuery</span>(<span class="pl-s"><span class="pl-pds">"""</span></span>
<span class="pl-s">    SELECT SUM(weight) AS total, AVG(weight) AS mean, MIN(weight) AS low</span>
<span class="pl-s">    FROM input0 JOIN input1 ON input0.name = input1.name</span>
<span class="pl-s">    WHERE input0.name LIKE 'j%'<span class="pl-pds">"""</span></span>)

  <span class="pl-c"><span class="pl-c">//</span> SelectValues step is a transformer, which allows renaming the data columns,</span>
  <span class="pl-c"><span class="pl-c">//</span> change data types, retain or delete columns</span>
  <span class="pl-k">val</span> <span class="pl-en">selectA</span> <span class="pl-k">=</span> <span class="pl-en">SelectValues</span>() rename (<span class="pl-s"><span class="pl-pds">"</span>mean<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>average<span class="pl-pds">"</span></span>) retype (<span class="pl-s"><span class="pl-pds">"</span>average<span class="pl-pds">"</span></span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>int<span class="pl-pds">"</span></span>)

  <span class="pl-c"><span class="pl-c">//</span> DebugOutput() simply prints the data and passes it through to the next node, if connected</span>
  <span class="pl-k">val</span> <span class="pl-en">debugA</span> <span class="pl-k">=</span> <span class="pl-en">DebugOutput</span>()

  <span class="pl-c"><span class="pl-c">//</span> this notation means, that the outputs of steps grid1 and grid2 need to be</span>
  <span class="pl-c"><span class="pl-c">//</span> connected to the inputs 0 and 1 of step queryA, the output of queryA &amp;ndash;</span>
  <span class="pl-c"><span class="pl-c">//</span> to the input of step selectA, and finally the output of selectA &amp;ndash;</span>
  <span class="pl-c"><span class="pl-c">//</span> to the input of debugA </span>
  (grid1, grid2) <span class="pl-k">--</span><span class="pl-k">&gt;</span> queryA <span class="pl-k">--</span><span class="pl-k">&gt;</span> selectA <span class="pl-k">--</span><span class="pl-k">&gt;</span> debugA

  <span class="pl-c"><span class="pl-c">//</span> the second pipeline also uses SQLQuery to process the data from one input</span>
  <span class="pl-k">val</span> <span class="pl-en">queryB</span> <span class="pl-k">=</span> <span class="pl-en">SQLQuery</span>(<span class="pl-s"><span class="pl-pds">"</span>SELECT SUBSTR(name, 1, 2) AS name, weight FROM input0<span class="pl-pds">"</span></span>)

  <span class="pl-c"><span class="pl-c">//</span> BasicStats() provides aggregation functions and grouping</span>
  <span class="pl-k">val</span> <span class="pl-en">statsB</span> <span class="pl-k">=</span> <span class="pl-en">BasicStats</span>() groupBy (<span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>) aggr (<span class="pl-s"><span class="pl-pds">"</span>weight<span class="pl-pds">"</span></span>, <span class="pl-en">AVG</span>, <span class="pl-en">MAX</span>, <span class="pl-en">COUNT_DISTINCT</span>)

  <span class="pl-c"><span class="pl-c">//</span> another debug output</span>
  <span class="pl-k">val</span> <span class="pl-en">debugB</span> <span class="pl-k">=</span> <span class="pl-en">DebugOutput</span>()

  <span class="pl-c"><span class="pl-c">//</span> a simple pipeline of steps</span>
  grid1 <span class="pl-k">--</span><span class="pl-k">&gt;</span> queryB <span class="pl-k">--</span><span class="pl-k">&gt;</span> statsB <span class="pl-k">--</span><span class="pl-k">&gt;</span> debugB

  <span class="pl-c"><span class="pl-c">//</span> Ignition uses a lazy evaluation model, i.e. only those steps that contribute</span>
  <span class="pl-c"><span class="pl-c">//</span> to the final result will be evaluated. The last line of the DataFlow definition</span>
  <span class="pl-c"><span class="pl-c">//</span> must contain a list of terminal nodes that need to be evaluated.</span>
  (debugA, debugB)
}

<span class="pl-c"><span class="pl-c">//</span> SparkPlug helper object connects to the Spark runtime and runs the data flow.</span>
<span class="pl-en">SparkPlug</span>.runDataFlow(flow)</pre>
  </div> 
  <p>The code above should produce these results: </p> 
  <pre><code>+----------+----------+----------+
|     total|   average|       low|
+----------+----------+----------+
|       310|       155|       120|
+----------+----------+----------+
+----------+----------+----------+---------------+
|      name|weight_avg|weight_max|weight_cnt_dist|
+----------+----------+----------+---------------+
|        ja|175.000000|       190|              2|
|        jo|137.500000|       155|              2|
+----------+----------+----------+---------------+
</code></pre> 
 </article>
</div>